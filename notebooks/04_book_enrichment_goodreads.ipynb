{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "610bb5d3-5748-45f7-a905-d382746a41b3",
   "metadata": {},
   "source": [
    "## Step 1 ‚Äî Load Configuration & Base Dataset\n",
    "\n",
    "In this step, we load the main configuration file (`config.yaml`) to access all project paths, and then import the base dataset `books_clustered_final.csv` from the `data/clean` directory.\n",
    "\n",
    "This dataset contains the books used for clustering in the previous step. It will serve as the foundation for enriching missing information such as ratings and genres using external data sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e3f9a2-a5fc-4753-82a8-f5e5ebad92a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 1 ‚Äî Load Configuration & Base Dataset\n",
    "# ============================================================\n",
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from functions import load_config, ensure_directories\n",
    "\n",
    "# --- Load configuration from project root ---\n",
    "config_path = Path(\"..\") / \"config.yaml\"\n",
    "config = load_config(config_path)\n",
    "\n",
    "# --- Ensure all folders exist ---\n",
    "ensure_directories(config[\"paths\"])\n",
    "\n",
    "# --- Load base dataset ---\n",
    "data_clean_path = Path(\"..\") / config[\"paths\"][\"data_clean\"]\n",
    "input_file = data_clean_path / \"books_clustered_final.csv\"\n",
    "\n",
    "df_main = pd.read_csv(input_file)\n",
    "\n",
    "print(f\"Dataset loaded successfully: {df_main.shape}\")\n",
    "df_main.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13d23cd-1f4d-4569-a8fe-c66fb4a38ab9",
   "metadata": {},
   "source": [
    "## Step 2 ‚Äî Load Goodreads Dataset (Kaggle goodbooks-10k)\n",
    "\n",
    "In this step, we load the `books.csv` file from the Kaggle dataset ‚Äúgoodbooks-10k‚Äù.  \n",
    "This dataset contains around 10 000 books with standardized metadata such as title, author, average rating, number of ratings, and publication year.  \n",
    "It is a lighter and cleaner dataset than the previous BrightData version and aligns well with our book titles.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87a5b40-47e4-4478-9532-06582f261226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 2 ‚Äî Load Goodreads Dataset (Kaggle goodbooks-10k)\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define path (using config paths)\n",
    "data_raw_path = Path(\"..\") / \"data\" / \"raw\"\n",
    "goodreads_file = data_raw_path / \"books.csv\"  # rename your downloaded books.csv to this\n",
    "\n",
    "# Load dataset\n",
    "df_goodreads = pd.read_csv(goodreads_file)\n",
    "print(f\"Kaggle Goodreads dataset loaded: {df_goodreads.shape}\")\n",
    "\n",
    "# Display available columns\n",
    "print(\"Columns:\", df_goodreads.columns.tolist())\n",
    "\n",
    "# Preview\n",
    "df_goodreads.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6fc00c-df21-4fed-b973-ee43c2d9dc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_goodreads.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a97aec-d935-4e36-80a8-073efc2e4326",
   "metadata": {},
   "source": [
    "## Step 3 ‚Äî Preprocess Titles & Authors for Merging\n",
    "\n",
    "Before merging both datasets, we standardize and align the column names used as matching keys.\n",
    "\n",
    "In the Kaggle dataset, the relevant columns are:\n",
    "- `title` ‚Üí book title  \n",
    "- `authors` ‚Üí author name(s)  \n",
    "- `average_rating` ‚Üí Goodreads average rating  \n",
    "- `ratings_count` ‚Üí total number of ratings  \n",
    "- `original_publication_year` ‚Üí publication year  \n",
    "- `image_url` ‚Üí cover image\n",
    "\n",
    "We will:\n",
    "1. Keep only these relevant columns.  \n",
    "2. Rename them for consistency.  \n",
    "3. Normalize `title` and `author` text to lowercase for reliable matching.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed80f06-33c7-4450-830e-2a344f783b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 3 ‚Äî Preprocess Titles & Authors for Merging\n",
    "# ============================================================\n",
    "\n",
    "# Select and rename relevant columns\n",
    "cols_to_keep = [\n",
    "    \"title\",\n",
    "    \"authors\",\n",
    "    \"average_rating\",\n",
    "    \"ratings_count\",\n",
    "    \"original_publication_year\",\n",
    "    \"image_url\"\n",
    "]\n",
    "\n",
    "df_goodreads = df_goodreads[cols_to_keep].rename(columns={\n",
    "    \"authors\": \"author\",\n",
    "    \"average_rating\": \"avg_rating_goodreads\",\n",
    "    \"ratings_count\": \"ratings_count_goodreads\",\n",
    "    \"original_publication_year\": \"published_year_goodreads\",\n",
    "    \"image_url\": \"cover_url_goodreads\"\n",
    "})\n",
    "\n",
    "# Normalize titles and authors in both datasets\n",
    "df_main[\"title_clean\"] = df_main[\"title\"].str.lower().str.strip()\n",
    "df_main[\"author_clean\"] = df_main[\"author\"].str.lower().str.strip()\n",
    "\n",
    "df_goodreads[\"title_clean\"] = df_goodreads[\"title\"].str.lower().str.strip()\n",
    "df_goodreads[\"author_clean\"] = df_goodreads[\"author\"].str.lower().str.strip()\n",
    "\n",
    "print(\"Columns prepared for merging:\")\n",
    "print(df_goodreads.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea56c7b0-e0b9-40ee-b26b-586891a1fd68",
   "metadata": {},
   "source": [
    "## Step 4 ‚Äî Merge Datasets (Left Join by Title & Author)\n",
    "\n",
    "In this step, we merge our main dataset (`books_clustered_final.csv`) with the Kaggle Goodreads dataset (`books.csv`)\n",
    "using the normalized columns `title_clean` and `author_clean` as join keys.\n",
    "\n",
    "This allows us to enrich our dataset with:\n",
    "- More accurate average ratings from Goodreads\n",
    "- Total number of ratings (`ratings_count_goodreads`)\n",
    "- Publication year\n",
    "- Cover image URL\n",
    "\n",
    "We use a **left join** to keep all entries from our main dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3792c533-3d3f-4f02-9e49-378f5843c75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 4 ‚Äî Merge Datasets (Left Join by Title & Author)\n",
    "# ============================================================\n",
    "\n",
    "# Perform left join\n",
    "df_merged = pd.merge(\n",
    "    df_main,\n",
    "    df_goodreads[\n",
    "        [\n",
    "            \"title_clean\",\n",
    "            \"author_clean\",\n",
    "            \"avg_rating_goodreads\",\n",
    "            \"ratings_count_goodreads\",\n",
    "            \"published_year_goodreads\",\n",
    "            \"cover_url_goodreads\"\n",
    "        ]\n",
    "    ],\n",
    "    on=[\"title_clean\", \"author_clean\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(f\"Merge completed: {df_merged.shape}\")\n",
    "\n",
    "# Display sample of enriched data\n",
    "df_merged[\n",
    "    [\"title\", \"author\", \"avg_rating\", \"avg_rating_goodreads\", \"ratings_count_goodreads\"]\n",
    "].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d53456-6576-48c3-9e8b-a00836a5e1ba",
   "metadata": {},
   "source": [
    "## Step 5 ‚Äî Replace Imputed Ratings and Save Enriched Dataset\n",
    "\n",
    "In this step, we replace the imputed values from our main dataset\n",
    "with the real Goodreads data obtained from the merge.\n",
    "\n",
    "Specifically:\n",
    "- Replace `avg_rating` values equal to 4.11 with the Goodreads rating when available.\n",
    "- Add the Goodreads `ratings_count` as a new feature.\n",
    "- Save the enriched dataset as `books_final_enriched.csv` in the `data/clean` folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b62ad99-d70f-49b9-8c8d-67f482e74cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 5 ‚Äî Safely Replace Imputed Ratings and Save Enriched Dataset\n",
    "# ============================================================\n",
    "\n",
    "from functions import save_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Create a copy to be safe ---\n",
    "df_enriched = df_merged.copy()\n",
    "\n",
    "# Replace only imputed ratings (4.11) with Goodreads ratings when available\n",
    "mask_replace = (\n",
    "    df_enriched[\"avg_rating\"].round(2) == 4.11\n",
    ") & (df_enriched[\"avg_rating_goodreads\"].notna())\n",
    "\n",
    "df_enriched.loc[mask_replace, \"avg_rating\"] = df_enriched.loc[\n",
    "    mask_replace, \"avg_rating_goodreads\"\n",
    "]\n",
    "\n",
    "# Keep Goodreads ratings_count as a new column (optional feature)\n",
    "df_enriched[\"ratings_count\"] = df_enriched[\"ratings_count_goodreads\"]\n",
    "\n",
    "# Remove helper columns but keep your core structure intact\n",
    "df_enriched = df_enriched.drop(columns=[\"avg_rating_goodreads\", \"ratings_count_goodreads\"])\n",
    "\n",
    "# Save the enriched dataset\n",
    "output_path = Path(\"..\") / \"data\" / \"clean\" / \"books_final_enriched.csv\"\n",
    "save_dataset(df_enriched, output_path)\n",
    "\n",
    "# --- Summary ---\n",
    "print(\"‚úÖ Enriched dataset saved safely ‚Üí books_final_enriched.csv\")\n",
    "print(f\"Ratings replaced (4.11 ‚Üí Goodreads): {mask_replace.sum()}\")\n",
    "print(df_enriched[[\"title\", \"author\", \"avg_rating\", \"ratings_count\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48336482-de13-4804-a011-31826e1e846e",
   "metadata": {},
   "source": [
    "## Step 6 ‚Äî Summary & Quality Check\n",
    "\n",
    "In this final step, we evaluate how much the dataset improved after enrichment.\n",
    "\n",
    "We will:\n",
    "- Count how many books had their imputed `avg_rating` (4.11) replaced with real Goodreads values.\n",
    "- Compare the average rating before and after enrichment.\n",
    "- Show basic statistics for the new `ratings_count` feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5e4469-3476-4b7c-a0a3-7cd14295f450",
   "metadata": {},
   "source": [
    "## Step 7 ‚Äî Clean Final Dataset for Re-Training (Overwrite Existing File)\n",
    "\n",
    "Before re-running the Machine Learning pipeline (PCA, Elbow, K-Means),\n",
    "we clean the enriched dataset to remove columns that are no longer needed.\n",
    "\n",
    "This step:\n",
    "- Removes outdated columns from the previous clustering (`cluster`, `pca_1`, `pca_2`).\n",
    "- Drops helper columns created during the enrichment (`title_clean`, `author_clean`, `cover_url_goodreads`).\n",
    "- Keeps relevant features for the next model training:\n",
    "  - **avg_rating** (quality)\n",
    "  - **ratings_count** (popularity)\n",
    "  - **price**, **genre**, **published_year**, etc.\n",
    "- Overwrites the file `books_final_enriched.csv` in the `data/clean` folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b67e6c-c1e5-4c47-ad69-cacba88ee257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 7 ‚Äî Clean Final Dataset for Re-Training (Overwrite File)\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load enriched dataset\n",
    "path_enriched = Path(\"..\") / \"data\" / \"clean\" / \"books_final_enriched.csv\"\n",
    "df = pd.read_csv(path_enriched)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "cols_to_drop = [\n",
    "    \"cluster\",\n",
    "    \"pca_1\",\n",
    "    \"pca_2\",\n",
    "    \"title_clean\",\n",
    "    \"author_clean\",\n",
    "    \"cover_url_goodreads\"\n",
    "]\n",
    "df = df.drop(columns=[col for col in cols_to_drop if col in df.columns])\n",
    "\n",
    "# Ensure ratings_count is numeric\n",
    "df[\"ratings_count\"] = pd.to_numeric(df[\"ratings_count\"], errors=\"coerce\")\n",
    "\n",
    "# Overwrite the same file\n",
    "df.to_csv(path_enriched, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"‚úÖ Cleaned and overwritten successfully ‚Üí books_final_enriched.csv\")\n",
    "print(f\"Final shape: {df.shape}\")\n",
    "print(\"Columns ready for re-training:\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd723272-1199-42ba-ae9f-6af50fbdb48c",
   "metadata": {},
   "source": [
    "## Step 8 ‚Äî Data Health Check (Missing Values & Completeness)\n",
    "\n",
    "Before deciding which features to include in the clustering model,\n",
    "we examine the completeness of the key numeric and categorical columns.\n",
    "\n",
    "This helps ensure that we don't include variables with too many missing values,\n",
    "which could distort scaling, PCA, or clustering results.\n",
    "\n",
    "We will check:\n",
    "- `avg_rating`\n",
    "- `price`\n",
    "- `ratings_count`\n",
    "- `genre`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab30857-7071-4765-a9ed-a1a8029e2af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 8 ‚Äî Data Health Check (Missing Values & Completeness)\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the cleaned enriched dataset\n",
    "path_data = Path(\"..\") / \"data\" / \"clean\" / \"books_final_enriched.csv\"\n",
    "df = pd.read_csv(path_data)\n",
    "\n",
    "# Select relevant columns to inspect\n",
    "cols_to_check = [\n",
    "    \"avg_rating\",\n",
    "    \"price\",\n",
    "    \"ratings_count\",\n",
    "    \"genre\"    \n",
    "]\n",
    "\n",
    "# Calculate missing counts and percentages\n",
    "missing_counts = df[cols_to_check].isna().sum()\n",
    "missing_pct = (missing_counts / len(df)) * 100\n",
    "\n",
    "# Combine into summary DataFrame\n",
    "missing_summary = pd.DataFrame({\n",
    "    \"Missing Values\": missing_counts,\n",
    "    \"Missing %\": missing_pct.round(2)\n",
    "}).sort_values(\"Missing %\", ascending=False)\n",
    "\n",
    "print(\"Missing Value Summary:\")\n",
    "display(missing_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c527f5f6-e476-491a-9f81-b15e7d22197e",
   "metadata": {},
   "source": [
    "## Step 9 ‚Äî Feature Preparation (Final Set for Clustering)\n",
    "\n",
    "Based on the data health check, we will only use columns that are fully complete.\n",
    "\n",
    "Selected features for clustering:\n",
    "- **avg_rating** ‚Üí reader-perceived quality\n",
    "- **price** ‚Üí economic value\n",
    "- **genre** ‚Üí categorical diversity\n",
    "\n",
    "The column **published_year_goodreads** will be kept in the dataset for visualization in the Streamlit app, but it won‚Äôt be used in the clustering model because it has too many missing values (~49%).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3d38a6-4e02-4580-964a-e2bb5c1531bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 9 ‚Äî Feature Preparation (Price & Rating Only)\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Select relevant numeric features ---\n",
    "features = [\"avg_rating\", \"price\"]\n",
    "df_features = df[features].copy()\n",
    "\n",
    "# --- Handle missing prices ---\n",
    "median_price = df_features[\"price\"].median()\n",
    "df_features[\"price\"] = df_features[\"price\"].fillna(median_price)\n",
    "df[\"price\"] = df[\"price\"].fillna(median_price)\n",
    "print(f\"Filled missing 'price' values with median: {median_price:.2f}\")\n",
    "\n",
    "# --- Standardize numeric features ---\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(df_features),\n",
    "    columns=features\n",
    ")\n",
    "print(\"üìè Numeric columns standardized (avg_rating, price).\")\n",
    "\n",
    "# --- Check for missing values ---\n",
    "missing_check = df_scaled.isna().sum().sum()\n",
    "if missing_check == 0:\n",
    "    print(\"‚úÖ No missing values remain in scaled feature matrix.\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è {missing_check} missing values still present ‚Äî check source data.\")\n",
    "\n",
    "# --- Create feature matrix for clustering ---\n",
    "X = df_scaled.values\n",
    "print(f\"\\n‚úÖ Feature matrix ready for clustering. Shape: {df_scaled.shape}\")\n",
    "\n",
    "# --- Quick preview ---\n",
    "display(df_scaled.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef2d79c-d4b4-4404-b5cc-9aca4ebb9644",
   "metadata": {},
   "source": [
    "## Step 10 ‚Äî K-Means Clustering (Elbow & Silhouette Analysis)\n",
    "\n",
    "We now test multiple K-Means clustering configurations (k = 2 to 10)\n",
    "to determine the optimal number of clusters.\n",
    "\n",
    "Steps:\n",
    "1. Run K-Means for different values of *k*.\n",
    "2. Compute the **inertia** (Elbow Method) and **silhouette score** for each model.\n",
    "3. Plot both metrics side by side.\n",
    "4. Identify the best *k* value according to the silhouette score.\n",
    "5. Save plots and metrics for later use in the Streamlit dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9782d94c-82e4-4ae3-a838-3a0efbc8d65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 10 ‚Äî K-Means Clustering (Elbow & Silhouette Method) ‚úÖ\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Feature matrix (scaled numeric data) ---\n",
    "X = df_scaled.copy()\n",
    "\n",
    "# --- Initialize lists ---\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "print(\"Running K-Means for k = 2 to 10...\\n\")\n",
    "\n",
    "# --- Run K-Means across different k values ---\n",
    "for k in K_range:\n",
    "    try:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(X)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "        score = silhouette_score(X, kmeans.labels_)\n",
    "        silhouette_scores.append(score)\n",
    "        print(f\"k={k} ‚Äî Inertia={kmeans.inertia_:.2f}, Silhouette={score:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error for k={k}: {e}\")\n",
    "        inertias.append(np.nan)\n",
    "        silhouette_scores.append(np.nan)\n",
    "\n",
    "# --- Determine best k by silhouette score ---\n",
    "valid_scores = [s for s in silhouette_scores if not np.isnan(s)]\n",
    "best_k = list(K_range)[silhouette_scores.index(max(valid_scores))]\n",
    "print(f\"\\n‚úÖ Best k by silhouette score: {best_k}\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# üìà Visualization ‚Äî Elbow & Silhouette\n",
    "# ============================================================\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 4))\n",
    "\n",
    "# --- Elbow Method ---\n",
    "ax1.plot(K_range, inertias, marker='o', color='steelblue')\n",
    "ax1.set_title(\"Elbow Method ‚Äî K-Means Inertia\", fontsize=11)\n",
    "ax1.set_xlabel(\"Number of Clusters (k)\")\n",
    "ax1.set_ylabel(\"Inertia\")\n",
    "ax1.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# --- Silhouette Score ---\n",
    "ax2.plot(K_range, silhouette_scores, marker='o', color='orange')\n",
    "ax2.set_title(\"Silhouette Scores by Number of Clusters\", fontsize=11)\n",
    "ax2.set_xlabel(\"Number of Clusters (k)\")\n",
    "ax2.set_ylabel(\"Silhouette Score\")\n",
    "ax2.set_ylim(-1, 1)  # ‚úÖ Correct axis range: Silhouette ‚àà [-1, 1]\n",
    "ax2.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# üíæ Save Results\n",
    "# ============================================================\n",
    "\n",
    "viz_path = Path(\"..\") / \"visualizations\"\n",
    "viz_path.mkdir(parents=True, exist_ok=True)\n",
    "fig.savefig(viz_path / \"kmeans_elbow_silhouette_combined.png\", dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"Plot saved ‚Üí {viz_path / 'kmeans_elbow_silhouette_combined.png'}\")\n",
    "\n",
    "metrics_path = Path(\"..\") / \"data\" / \"clean\"\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"k\": list(K_range),\n",
    "    \"inertia\": inertias,\n",
    "    \"silhouette\": silhouette_scores\n",
    "})\n",
    "metrics_df.to_csv(metrics_path / \"kmeans_metrics.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"Metrics saved ‚Üí {metrics_path / 'kmeans_metrics.csv'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da42290-ffd5-4111-a4c1-8787bc11baef",
   "metadata": {},
   "source": [
    "## Step 10.1 ‚Äî Re-run K-Means with k=3 (Manual Selection for Interpretability)\n",
    "\n",
    "Although k=2 gave the highest Silhouette Score (‚âà0.87), \n",
    "the clusters were highly imbalanced ‚Äî one cluster contained almost all books, \n",
    "reducing interpretability.\n",
    "\n",
    "To gain richer insights, we manually re-run K-Means with k=3, \n",
    "balancing statistical validity and business relevance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e936afc-b52e-4370-bb61-3a330695f472",
   "metadata": {},
   "source": [
    "## Step 11 ‚Äî Train Final K-Means Model & Visualize Clusters (PCA 2D)\n",
    "\n",
    "Using the optimal *k* value obtained from the Elbow & Silhouette analysis,  \n",
    "we train the final K-Means model and project the results in 2D using PCA for visualization.\n",
    "\n",
    "Steps:\n",
    "1. Scale the feature matrix (`df_encoded`).  \n",
    "2. Train K-Means with *k = best_k*.  \n",
    "3. Apply PCA to obtain two principal components.  \n",
    "4. Visualize the clusters in 2D.  \n",
    "5. Save the updated dataset and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1585f83d-a67e-4bf7-bda1-81a9a766b685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 11 ‚Äî Train K-Means (k=3) & Visualize Clusters (PCA 2D)\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Set final k manually ---\n",
    "k_final = 3\n",
    "print(f\"Applying final K-Means model with k = {k_final}...\\n\")\n",
    "\n",
    "# --- Use scaled numeric features (avg_rating, price) ---\n",
    "X_scaled = df_scaled.copy()\n",
    "\n",
    "# --- Train K-Means ---\n",
    "kmeans_final = KMeans(n_clusters=k_final, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans_final.fit_predict(X_scaled)\n",
    "\n",
    "# --- Assign clusters ---\n",
    "df = df.copy()\n",
    "df[\"cluster\"] = cluster_labels\n",
    "\n",
    "# --- PCA for visualization ---\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "pca_components = pca.fit_transform(X_scaled)\n",
    "df[\"pca_1\"] = pca_components[:, 0]\n",
    "df[\"pca_2\"] = pca_components[:, 1]\n",
    "\n",
    "# ============================================================\n",
    "# üìä PCA 2D Visualization\n",
    "# ============================================================\n",
    "\n",
    "fig_pca, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=df,\n",
    "    x=\"pca_1\", y=\"pca_2\",\n",
    "    hue=\"cluster\",\n",
    "    palette=\"Set2\",\n",
    "    s=65,\n",
    "    alpha=0.85,\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=0.7,\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_title(f\"Book Clusters ‚Äî PCA 2D Projection (k = {k_final})\", fontsize=13, pad=10)\n",
    "ax.set_xlabel(\"Principal Component 1\")\n",
    "ax.set_ylabel(\"Principal Component 2\")\n",
    "ax.legend(title=\"Cluster\", loc=\"best\", fontsize=9)\n",
    "ax.grid(alpha=0.25, linestyle=\"--\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# üìã Cluster Summary ‚Äî Detailed Genre Composition\n",
    "# ============================================================\n",
    "\n",
    "cluster_summary = (\n",
    "    df.groupby([\"cluster\", \"genre\"])\n",
    "    .agg(\n",
    "        Count=(\"title\", \"count\"),\n",
    "        Avg_Rating=(\"avg_rating\", \"mean\"),\n",
    "        Avg_Price_EUR=(\"price\", \"mean\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Proportion within each cluster\n",
    "cluster_summary[\"cluster\"] = cluster_summary[\"cluster\"].astype(int)\n",
    "cluster_totals = cluster_summary.groupby(\"cluster\")[\"Count\"].transform(\"sum\")\n",
    "cluster_summary[\"Proportion (%)\"] = (cluster_summary[\"Count\"] / cluster_totals * 100).round(2)\n",
    "\n",
    "# Order and round values\n",
    "cluster_summary = cluster_summary.sort_values([\"cluster\", \"Count\"], ascending=[True, False])\n",
    "cluster_summary[\"Avg_Rating\"] = cluster_summary[\"Avg_Rating\"].round(2)\n",
    "cluster_summary[\"Avg_Price_EUR\"] = cluster_summary[\"Avg_Price_EUR\"].round(2)\n",
    "\n",
    "print(f\"üìö Cluster Summary ‚Äî Detailed Genre Composition (k = {k_final})\")\n",
    "display(cluster_summary)\n",
    "\n",
    "# ============================================================\n",
    "# üíæ Save Outputs (clean version ‚Äî overwrite previous files)\n",
    "# ============================================================\n",
    "\n",
    "viz_path = Path(\"..\") / \"visualizations\"\n",
    "viz_path.mkdir(parents=True, exist_ok=True)\n",
    "fig_pca.savefig(viz_path / \"pca_clusters_final.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "# Save dataset & cluster summary (overwrite existing)\n",
    "output_path = Path(\"..\") / \"data\" / \"clean\"\n",
    "\n",
    "df.to_csv(output_path / \"books_clustered_final_enriched.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "cluster_summary.to_csv(output_path / \"cluster_summary.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"‚úÖ Clustering completed ‚Äî {df['cluster'].nunique()} clusters created.\")\n",
    "print(f\"üíæ PCA plot saved ‚Üí {viz_path / 'pca_clusters_final.png'}\")\n",
    "print(f\"üíæ Dataset saved ‚Üí {output_path / 'books_clustered_final_enriched.csv'}\")\n",
    "print(f\"üíæ Summary saved ‚Üí {output_path / 'cluster_summary.csv'}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603889b5-1a72-40bd-be53-2c89215470fb",
   "metadata": {},
   "source": [
    "## Step 12 ‚Äî Cluster Profiling and Centroid Analysis\n",
    "\n",
    "To interpret the K-Means results, we summarize the characteristics of each cluster.\n",
    "This step focuses on numeric features only ‚Äî average rating and average price ‚Äî which were used to train the clustering model.\n",
    "Specifically, we:\n",
    "- Calculate the **mean rating** and **mean price** for each cluster. \n",
    "- Adds the total **number of books** in each cluster.  \n",
    "\n",
    "These summaries help us understand the general profile of each group ‚Äî  \n",
    "for example, whether a cluster represents ‚Äúaffordable popular titles‚Äù or ‚Äúhigh-priced premium books.‚Äù\n",
    "\n",
    "Genre information can still be referenced descriptively, but it was not used as part of the model‚Äôs features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fffc0c-fb00-4c9e-9f4c-058271519f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 12 ‚Äî Cluster Profiling and Genre Composition (Descriptive Only)\n",
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "üéØ Step 12 ‚Äî Cluster Profiling and Genre Composition (Descriptive Only)\n",
    "Although the clustering was trained only on numeric features (avg_rating, price),\n",
    "we include genre information here to interpret and describe each group.\n",
    "\n",
    "The final model uses k = 3 clusters for better interpretability.\n",
    "\"\"\"\n",
    "\n",
    "# --- Genre distribution within clusters ---\n",
    "genre_summary = (\n",
    "    df.groupby([\"cluster\", \"genre\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "# --- Add proportion (%) within each cluster ---\n",
    "cluster_sizes = df[\"cluster\"].value_counts().to_dict()\n",
    "genre_summary[\"proportion_%\"] = genre_summary.apply(\n",
    "    lambda row: round((row[\"count\"] / cluster_sizes[row[\"cluster\"]]) * 100, 2),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# --- Add numeric averages per cluster ---\n",
    "cluster_means = (\n",
    "    df.groupby(\"cluster\")[[\"avg_rating\", \"price\"]]\n",
    "    .mean()\n",
    "    .round(2)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# --- Merge to get complete profile ---\n",
    "cluster_profile = genre_summary.merge(cluster_means, on=\"cluster\", how=\"left\")\n",
    "cluster_profile = cluster_profile.sort_values([\"cluster\", \"count\"], ascending=[True, False])\n",
    "\n",
    "# --- Display top genres per cluster ---\n",
    "print(\"üìä Cluster Composition Summary (Genres used for interpretation only):\\n\")\n",
    "display(cluster_profile.groupby(\"cluster\").head(6))\n",
    "\n",
    "print(\"\\nüß≠ Interpretation Guide:\")\n",
    "print(\"- avg_rating ‚Üí Average rating in the cluster\")\n",
    "print(\"- price ‚Üí Average price in the cluster\")\n",
    "print(\"- genre ‚Üí Genre distribution (not used in clustering)\")\n",
    "print(\"- count ‚Üí Number of books per genre\")\n",
    "print(\"- proportion_% ‚Üí Share of that genre within its cluster\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc818a97-7e8a-428e-8016-77c3b6021dab",
   "metadata": {},
   "source": [
    "## Step 13 ‚Äî Export Final Clustered Dataset (Enriched Version)\n",
    "\n",
    "We now export the final clustered dataset and summary table generated from the enriched data.\n",
    "This version will be saved under a different name to allow comparison with the results from Notebook 03.\n",
    "\n",
    "Outputs:\n",
    "- `books_clustered_final_enriched.csv` ‚Üí detailed dataset with PCA & clusters  \n",
    "- `cluster_summary_enriched.csv` ‚Üí summarized cluster characteristics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774821b8-1e21-4f53-a65a-e6d8102b701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 13 ‚Äî Export Final Clustered Dataset + Detailed Genre Summary (k = 3)\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# --- Define export paths ---\n",
    "data_clean_path = Path(\"..\") / \"data\" / \"clean\"\n",
    "viz_path = Path(\"..\") / \"visualizations\"\n",
    "data_clean_path.mkdir(parents=True, exist_ok=True)\n",
    "viz_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Export enriched clustered dataset ---\n",
    "final_cluster_path = data_clean_path / \"books_clustered_final_enriched.csv\"\n",
    "\n",
    "export_cols = [\n",
    "    \"title\", \"author\", \"avg_rating\", \"genre\", \"price\", \"currency\",\n",
    "    \"cover_url\", \"link\", \"cluster\", \"pca_1\", \"pca_2\"\n",
    "]\n",
    "\n",
    "df[export_cols].to_csv(final_cluster_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"üíæ Enriched clustered dataset saved successfully ‚Üí {final_cluster_path.resolve()}\")\n",
    "\n",
    "# ============================================================\n",
    "# üìä Detailed Cluster Composition Summary (with genre proportions) ‚Äî k = 3\n",
    "# ============================================================\n",
    "\n",
    "# --- Count genres per cluster ---\n",
    "genre_summary = (\n",
    "    df.groupby([\"cluster\", \"genre\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "# --- Add proportion (%) within each cluster ---\n",
    "cluster_sizes = df[\"cluster\"].value_counts().to_dict()\n",
    "genre_summary[\"proportion_%\"] = genre_summary.apply(\n",
    "    lambda row: round((row[\"count\"] / cluster_sizes[row[\"cluster\"]]) * 100, 2),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# --- Add numeric averages per cluster ---\n",
    "cluster_means = (\n",
    "    df.groupby(\"cluster\")[[\"avg_rating\", \"price\"]]\n",
    "    .mean()\n",
    "    .round(2)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# --- Merge into one summary table ---\n",
    "cluster_profile = genre_summary.merge(cluster_means, on=\"cluster\", how=\"left\")\n",
    "cluster_profile = cluster_profile.sort_values([\"cluster\", \"count\"], ascending=[True, False])\n",
    "\n",
    "print(\"\\nüìó Cluster Composition Summary (Top Genres per Cluster) ‚Äî k = 3\")\n",
    "display(cluster_profile.groupby(\"cluster\").head(8))\n",
    "\n",
    "# --- Save detailed summary ---\n",
    "summary_path = data_clean_path / \"cluster_summary_detailed.csv\"\n",
    "cluster_profile.to_csv(summary_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"\\nüíæ Detailed cluster summary saved ‚Üí {summary_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85e8786-abb9-4f17-ad2c-4af59f17ff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üìä Cluster Summary Table ‚Äî Detailed Genre Breakdown (k = 3)\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Usa la tabla real generada en el notebook (cluster_profile) ---\n",
    "# Si ya la tienes en memoria, puedes usar directamente `cluster_profile`\n",
    "# Si no, carga desde CSV:\n",
    "# cluster_profile = pd.read_csv(\"../data/clean/cluster_summary_detailed.csv\")\n",
    "\n",
    "# --- Top 6 g√©neros por cluster ---\n",
    "cluster_top_genres = (\n",
    "    cluster_profile.groupby(\"cluster\")\n",
    "    .head(6)\n",
    "    .reset_index(drop=True)\n",
    "    .rename(columns={\n",
    "        \"cluster\": \"Cluster\",\n",
    "        \"genre\": \"Genre\",\n",
    "        \"count\": \"Count\",\n",
    "        \"proportion_%\": \"Proportion (%)\",\n",
    "        \"avg_rating\": \"Avg Rating\",\n",
    "        \"price\": \"Avg Price (EUR)\"\n",
    "    })\n",
    ")\n",
    "\n",
    "# --- Estilo visual igual al de tu tabla de presentaci√≥n ---\n",
    "styled_genres = (\n",
    "    cluster_top_genres.style\n",
    "    .set_caption(\"üìö Cluster Summary ‚Äî Detailed Genre Composition (k = 3)\")\n",
    "    .set_table_styles([\n",
    "        {\"selector\": \"caption\", \n",
    "         \"props\": [(\"text-align\", \"left\"), (\"font-size\", \"16px\"), \n",
    "                   (\"font-weight\", \"bold\"), (\"color\", \"#00c3ff\")]},\n",
    "        {\"selector\": \"table\", \n",
    "         \"props\": [(\"border\", \"2px solid #00c3ff\"), \n",
    "                   (\"border-radius\", \"8px\"),\n",
    "                   (\"border-collapse\", \"collapse\")]},\n",
    "        {\"selector\": \"th\", \n",
    "         \"props\": [(\"background-color\", \"#1c1c1c\"), (\"color\", \"white\"), \n",
    "                   (\"text-align\", \"center\"), (\"font-size\", \"14px\")]},\n",
    "        {\"selector\": \"td\", \n",
    "         \"props\": [(\"background-color\", \"#505050\"), (\"color\", \"#f2f2f2\"), \n",
    "                   (\"font-size\", \"13px\"), (\"text-align\", \"center\")]}\n",
    "    ])\n",
    "    .hide(axis=\"index\")\n",
    "    .format({\n",
    "        \"Avg Rating\": \"{:.2f}\",\n",
    "        \"Avg Price (EUR)\": \"{:.2f}\",\n",
    "        \"Proportion (%)\": \"{:.2f}\"\n",
    "    })\n",
    ")\n",
    "\n",
    "# --- Mostrar tabla estilizada ---\n",
    "display(styled_genres)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book_recommender (uv)",
   "language": "python",
   "name": "book-recommendation-system"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
