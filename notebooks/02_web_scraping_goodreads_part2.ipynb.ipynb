{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "772c0d20-f4f9-4002-b805-ccfa8fb0fe15",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Step 1 ‚Äî Imports & Setup\n",
    "\n",
    "In this step, we import our custom functions and configuration file to ensure the environment is ready.  \n",
    "We will use the same utility functions (`fetch_html`, `parse_html`, `extract_books_from_soup`, etc.)  \n",
    "from the `functions.py` file located in the `notebooks` folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0c95ff-335e-48c6-a5fa-d6c3bd33f2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 1 ‚Äî Imports & Setup\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "\n",
    "# --- Add notebooks path and import functions ---\n",
    "sys.path.append(\"notebooks\")\n",
    "\n",
    "import functions\n",
    "from functions import load_config, ensure_directories\n",
    "\n",
    "# --- Reload in case of changes ---\n",
    "importlib.reload(functions)\n",
    "\n",
    "# --- Load config from project root ---\n",
    "config_path = Path(\"../config.yaml\")\n",
    "config = load_config(config_path)\n",
    "\n",
    "# --- Ensure directories exist (defined in config.yaml) ---\n",
    "ensure_directories(config[\"paths\"])\n",
    "\n",
    "print(\"‚úÖ All functions and configuration loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91ddccc-cb8f-488a-875b-89656a61a78a",
   "metadata": {},
   "source": [
    "## Step 2 ‚Äî Scrape Goodreads Pages 6‚Äì12\n",
    "\n",
    "In this step, we will continue the web scraping process for the Goodreads list  \n",
    "**‚ÄúBest Books Ever‚Äù**, focusing on pages **6 to 12** to complete a total of ~1000 books.  \n",
    "\n",
    "We will use the same helper functions from `functions.py`:\n",
    "- `fetch_html()` to retrieve page content  \n",
    "- `parse_html()` to parse it with BeautifulSoup  \n",
    "- `extract_books_from_soup()` to extract titles, authors, and metadata  \n",
    "\n",
    "Each page will be scraped with a small random delay between requests (ethical scraping).  \n",
    "The final result will be saved as `data/raw/goodreads_books_6_to_10.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c902731e-56ee-45e6-a8f1-c30d94b2ad07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 2 ‚Äî Scrape Goodreads Pages 6‚Äì12 (Extended Range)\n",
    "# ============================================================\n",
    "\n",
    "from functions import fetch_html, parse_html, extract_books_from_soup, save_dataset\n",
    "from time import sleep\n",
    "import random\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üîπ Starting scraping for pages 6‚Äì12...\")\n",
    "\n",
    "base_url = \"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=\"\n",
    "all_books_part2 = []\n",
    "\n",
    "# --- Loop through pages 6 to 12 ---\n",
    "for page in range(6, 13):\n",
    "    url = base_url + str(page)\n",
    "    print(f\"\\nüåç Fetching page {page}: {url}\")\n",
    "\n",
    "    html = fetch_html(url)\n",
    "    if html is None:\n",
    "        print(f\"‚ö†Ô∏è Skipping page {page} due to empty response.\")\n",
    "        continue\n",
    "\n",
    "    soup = parse_html(html)\n",
    "    books_page = extract_books_from_soup(soup)\n",
    "    all_books_part2.append(books_page)\n",
    "\n",
    "    # --- Ethical delay ---\n",
    "    sleep(random.uniform(1, 2))\n",
    "\n",
    "print(f\"\\n‚úÖ Scraping completed for {len(all_books_part2)} pages.\")\n",
    "\n",
    "# --- Combine all new DataFrames ---\n",
    "df_part2 = pd.concat(all_books_part2, ignore_index=True)\n",
    "print(f\"‚úÖ Combined dataset shape (pages 6‚Äì12): {df_part2.shape}\")\n",
    "\n",
    "# --- Save dataset correctly in the root-level data/raw ---\n",
    "raw_output_path = Path(\"..\") / config[\"paths\"][\"data_raw\"] / \"goodreads_books_6_to_12.csv\"\n",
    "print(f\"üìÅ Saving file to: {raw_output_path.resolve()}\")\n",
    "\n",
    "save_dataset(df_part2, raw_output_path)\n",
    "\n",
    "# --- Preview ---\n",
    "df_part2.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563a020e-7353-4ef9-b196-89fd5305277d",
   "metadata": {},
   "source": [
    "## Step 3 ‚Äî Combine New Books (Pages 6‚Äì12) with Existing Clean Dataset\n",
    "\n",
    "In this step, we will combine the newly scraped books from pages **6‚Äì10**  \n",
    "with the previously cleaned dataset (`books_clean.csv`) generated in Notebook 01.  \n",
    "\n",
    "No raw CSV is saved ‚Äî we will only update the cleaned dataset structure,  \n",
    "ensuring consistent columns (`title`, `author`, `rating`, `genre`, `price`, etc.)  \n",
    "and removing any duplicates by title.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7571dba0-d6ab-4939-9f15-63100c2b4f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from functions import save_dataset\n",
    "\n",
    "# --- Load existing clean dataset ---\n",
    "clean_path_prev = Path(\"..\") / config[\"paths\"][\"data_clean\"] / \"books_clean.csv\"\n",
    "df_existing = pd.read_csv(clean_path_prev)\n",
    "print(f\"‚úÖ Loaded previous clean dataset: {df_existing.shape}\")\n",
    "\n",
    "# --- Define expected columns from the clean dataset ---\n",
    "expected_cols = df_existing.columns.tolist()\n",
    "\n",
    "# --- Create any missing columns in df_part2 ---\n",
    "for col in expected_cols:\n",
    "    if col not in df_part2.columns:\n",
    "        df_part2[col] = pd.NA\n",
    "\n",
    "# --- Ensure only expected columns are kept (ignore extras safely) ---\n",
    "df_part2 = df_part2[[col for col in expected_cols if col in df_part2.columns]]\n",
    "\n",
    "# --- Combine both datasets ---\n",
    "df_combined = pd.concat(\n",
    "    [df_existing, df_part2.dropna(how=\"all\")],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# --- Drop duplicates by title ---\n",
    "df_combined = df_combined.drop_duplicates(subset=[\"title\"]).reset_index(drop=True)\n",
    "\n",
    "print(f\"‚úÖ Combined dataset shape: {df_combined.shape}\")\n",
    "print(f\"Unique authors: {df_combined['author'].nunique()}\")\n",
    "\n",
    "# ============================================================\n",
    "# üßπ Handle Missing Values Before Saving\n",
    "# ============================================================\n",
    "\n",
    "# --- Fill missing genre and price before saving ---\n",
    "df_combined[\"genre\"] = df_combined[\"genre\"].fillna(\"Unknown\")\n",
    "\n",
    "# --- Fill missing price with median (robust against outliers) ---\n",
    "median_price = df_combined[\"price\"].median()\n",
    "df_combined[\"price\"] = df_combined[\"price\"].fillna(median_price)\n",
    "\n",
    "print(f\" Filled missing 'price' values with median: {median_price:.2f}\")\n",
    "print(f\" Filled missing 'genre' with 'Unknown'\")\n",
    "\n",
    "# ============================================================\n",
    "# üíæ Save the Updated Clean Dataset\n",
    "# ============================================================\n",
    "\n",
    "output_path = Path(\"..\") / config[\"paths\"][\"data_clean\"] / \"books_clean_1000.csv\"\n",
    "print(f\" Saving combined clean dataset to: {output_path.resolve()}\")\n",
    "\n",
    "save_dataset(df_combined, output_path)\n",
    "\n",
    "# --- Quick preview ---\n",
    "print(\"\\nüìò Preview of combined dataset:\")\n",
    "display(df_combined.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa53abb-61a6-4ad1-b113-1315ec0c248b",
   "metadata": {},
   "source": [
    "# Step 4 ‚Äî Quick Data Verification\n",
    "\n",
    "In this step, we will perform a quick verification of the combined dataset  \n",
    "(`books_clean_1000.csv`) to ensure data integrity after merging the two sources.  \n",
    "\n",
    "We will check:\n",
    "- Dataset dimensions and column names  \n",
    "- Missing values per column  \n",
    "- Unique authors  \n",
    "- Basic descriptive statistics (for numeric columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52355e19-21c3-4709-8a1f-d5f06a3b5b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 4 ‚Äî Quick Data Verification\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Load combined dataset ---\n",
    "combined_path = Path(\"..\") / config[\"paths\"][\"data_clean\"] / \"books_clean_1000.csv\"\n",
    "df = pd.read_csv(combined_path)\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded successfully: {combined_path}\")\n",
    "print(f\"Shape: {df.shape}\\n\")\n",
    "\n",
    "# --- Overview of columns ---\n",
    "print(\"üìä Columns:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# --- Quick info and missing values ---\n",
    "print(\"\\nüîç DataFrame Info:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nüîç Missing values per column:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "# --- Quick summary statistics (no datetime flag for older pandas) ---\n",
    "print(\"\\nüìà Descriptive Statistics:\")\n",
    "display(df.describe(include=\"all\"))\n",
    "\n",
    "# --- Check duplicates by title ---\n",
    "dup_titles = df[\"title\"].duplicated().sum()\n",
    "print(f\"\\n‚ö†Ô∏è Duplicate titles found: {dup_titles}\")\n",
    "\n",
    "# --- Unique authors ---\n",
    "unique_authors = df[\"author\"].nunique()\n",
    "print(f\"üë©‚Äçüíª Unique authors: {unique_authors}\")\n",
    "\n",
    "# --- Example preview ---\n",
    "print(\"\\nüìò Preview of the combined dataset:\")\n",
    "display(df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea7051b-def6-415e-b840-60d776a63a3d",
   "metadata": {},
   "source": [
    "# Step 5 ‚Äî Enrich New Books (Pages 6‚Äì10) with Google Books API\n",
    "\n",
    "In this step, we will enrich the newly added books (pages **6‚Äì10**)  \n",
    "with metadata from the **Google Books API**, following the same method used  \n",
    "in Notebook 01.  \n",
    "\n",
    "We will retrieve:\n",
    "- Published year  \n",
    "- Genre / categories  \n",
    "- Cover URL  \n",
    "- Price and currency (when available)  \n",
    "\n",
    "Only books missing `avg_rating`, `genre`, or `published_year` will be processed  \n",
    "to avoid duplicate API requests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ca463e-df4b-475d-a07e-4fde3471aacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 5 ‚Äî Reuse Google Books API functions\n",
    "# ============================================================\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def get_book_info_from_google(title, author):\n",
    "    \"\"\"Query Google Books API and return metadata for a given title + author.\"\"\"\n",
    "    query = f\"intitle:{title}+inauthor:{author}\"\n",
    "    url = f\"https://www.googleapis.com/books/v1/volumes?q={query}\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if \"items\" in data and len(data[\"items\"]) > 0:\n",
    "                info = data[\"items\"][0][\"volumeInfo\"]\n",
    "                return {\n",
    "                    \"published_year\": info.get(\"publishedDate\", None),\n",
    "                    \"genre\": \", \".join(info.get(\"categories\", [])) if info.get(\"categories\") else None,\n",
    "                    \"cover_url\": info.get(\"imageLinks\", {}).get(\"thumbnail\", None)\n",
    "                }\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error fetching '{title}': {e}\")\n",
    "    \n",
    "    return {\"published_year\": None, \"genre\": None, \"cover_url\": None}\n",
    "\n",
    "\n",
    "def get_price_from_google(title, author):\n",
    "    \"\"\"Query Google Books API for price info (listPrice or retailPrice).\"\"\"\n",
    "    query = f\"intitle:{title}+inauthor:{author}\"\n",
    "    url = f\"https://www.googleapis.com/books/v1/volumes?q={query}\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if \"items\" in data:\n",
    "                info = data[\"items\"][0].get(\"saleInfo\", {})\n",
    "                price_info = info.get(\"listPrice\", {}) or info.get(\"retailPrice\", {})\n",
    "                if price_info:\n",
    "                    return price_info.get(\"amount\"), price_info.get(\"currencyCode\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error fetching '{title}': {e}\")\n",
    "    return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e51816-527d-4b61-8e74-16d9ed902ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 5.1 ‚Äî Enrich only books missing metadata\n",
    "# ============================================================\n",
    "\n",
    "# --- Load combined dataset ---\n",
    "from pathlib import Path\n",
    "combined_path = Path(\"..\") / config[\"paths\"][\"data_clean\"] / \"books_clean_1000.csv\"\n",
    "df = pd.read_csv(combined_path)\n",
    "\n",
    "# --- Filter only the books that need enrichment ---\n",
    "df_to_enrich = df[df[\"avg_rating\"].isna()].copy()\n",
    "print(f\"üìö Books pending enrichment: {len(df_to_enrich)}\")\n",
    "\n",
    "# --- Apply Google Books API to retrieve metadata ---\n",
    "results = []\n",
    "for _, row in tqdm(df_to_enrich.iterrows(), total=len(df_to_enrich)):\n",
    "    meta = get_book_info_from_google(row[\"title\"], row[\"author\"])\n",
    "    results.append(meta)\n",
    "    time.sleep(0.5)  # ethical delay\n",
    "\n",
    "api_df = pd.DataFrame(results)\n",
    "df_enriched = pd.concat([df_to_enrich.reset_index(drop=True), api_df], axis=1)\n",
    "\n",
    "print(f\"‚úÖ Metadata enrichment completed for {len(df_enriched)} books.\")\n",
    "df_enriched.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e68de5-afb0-4b91-ba9f-3cd31a247588",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enriched.to_csv(\"../data/raw/temp_books_meta_backup.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"üíæ Backup saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f4fd3f-d70c-4482-9635-867055dc67f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enriched.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889e322f-5927-4f4a-8028-b905b7810732",
   "metadata": {},
   "source": [
    "## Step 5.2‚Äî Price Enrichment (Optimized Version)\n",
    "\n",
    "Retrieve book prices from Google Books API, using checkpointing to avoid\n",
    "losing progress if interrupted. Merge results with previously enriched metadata\n",
    "to build a fully enriched dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6484232-53da-48b8-a3f3-f2f3e99ee3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 5.2 ‚Äî Price Enrichment (Optimized & Re-startable)\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Load the intermediate enriched file (from Step 5.2) ---\n",
    "intermediate_path = Path(\"..\") / config[\"paths\"][\"data_raw\"] / \"temp_books_meta_backup.csv\"\n",
    "df_enriched = pd.read_csv(intermediate_path)\n",
    "\n",
    "print(f\"‚úÖ Loaded enriched dataset for price retrieval: {df_enriched.shape[0]} books\")\n",
    "\n",
    "# --- Define checkpoint path ---\n",
    "checkpoint_path = Path(\"..\") / config[\"paths\"][\"data_raw\"] / \"temp_prices_checkpoint.csv\"\n",
    "\n",
    "# --- If checkpoint exists, resume from there ---\n",
    "if checkpoint_path.exists():\n",
    "    df_checkpoint = pd.read_csv(checkpoint_path)\n",
    "    processed_titles = set(df_checkpoint[\"title\"].unique())\n",
    "    print(f\"‚è© Resuming from checkpoint ({len(df_checkpoint)} books already processed).\")\n",
    "else:\n",
    "    df_checkpoint = pd.DataFrame(columns=[\"title\", \"price\", \"currency\"])\n",
    "    processed_titles = set()\n",
    "    print(\"üÜï Starting fresh price enrichment.\")\n",
    "\n",
    "# --- Filter only books not yet processed ---\n",
    "df_to_process = df_enriched[~df_enriched[\"title\"].isin(processed_titles)].copy()\n",
    "print(f\"üìö Remaining books to process: {len(df_to_process)}\")\n",
    "\n",
    "# --- Apply Google Books API to get prices ---\n",
    "prices = []\n",
    "\n",
    "for i, (_, row) in enumerate(tqdm(df_to_process.iterrows(), total=len(df_to_process))):\n",
    "    price, currency = get_price_from_google(row[\"title\"], row[\"author\"])\n",
    "    prices.append({\"title\": row[\"title\"], \"price\": price, \"currency\": currency})\n",
    "    \n",
    "    # --- Save progress every 50 books ---\n",
    "    if (i + 1) % 50 == 0 or (i + 1) == len(df_to_process):\n",
    "        df_partial = pd.DataFrame(prices)\n",
    "        df_checkpoint = pd.concat([df_checkpoint, df_partial], ignore_index=True)\n",
    "        df_checkpoint.to_csv(checkpoint_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"üíæ Checkpoint saved ({len(df_checkpoint)} total so far)\")\n",
    "        prices = []  # reset buffer\n",
    "    \n",
    "    time.sleep(0.5)  # reduced ethical delay\n",
    "\n",
    "print(\"\\n‚úÖ Price enrichment completed!\")\n",
    "\n",
    "# --- Merge checkpoint results into main enriched dataset ---\n",
    "df_prices_final = pd.read_csv(checkpoint_path)\n",
    "\n",
    "# ‚úÖ Fix: ensure columns exist even if no new prices were processed\n",
    "if \"price\" not in df_prices_final.columns:\n",
    "    df_prices_final[\"price\"] = None\n",
    "if \"currency\" not in df_prices_final.columns:\n",
    "    df_prices_final[\"currency\"] = None\n",
    "\n",
    "df_final = pd.merge(df_enriched, df_prices_final, on=\"title\", how=\"left\")\n",
    "\n",
    "# --- Save final enriched dataset ---\n",
    "output_path = Path(\"..\") / config[\"paths\"][\"data_clean\"] / \"books_clean_enriched_1000.csv\"\n",
    "df_final.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"\\nüíæ Final enriched dataset saved successfully ‚Üí {output_path.resolve()}\")\n",
    "print(f\"Rows: {len(df_final)}, Columns: {len(df_final.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0080dd9f-e27d-4c14-8fbf-f683df3782aa",
   "metadata": {},
   "source": [
    "# Step 6 ‚Äî Clean Duplicated Columns from Enriched Dataset\n",
    "\n",
    "Before merging the old and new datasets, we will remove duplicate columns\n",
    "with `_x` and `_y` suffixes. The `_y` columns contain the correct enriched data\n",
    "(pulled from the Google Books API).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315db5fa-6444-4a7a-aae9-e7064ca53415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 6 ‚Äî Final Cleanup (Keep only API-enhanced columns)\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Load dataset (enriched) ---\n",
    "clean_path = Path(\"..\") / config[\"paths\"][\"data_clean\"] / \"books_clean_enriched_1000.csv\"\n",
    "df = pd.read_csv(clean_path)\n",
    "print(f\"‚úÖ Loaded dataset: {df.shape}\")\n",
    "\n",
    "# --- Columns to keep/replace manually based on enrichment ---\n",
    "# Keep only the API-enriched versions of key columns\n",
    "if \"genre.1\" in df.columns:\n",
    "    df[\"genre\"] = df[\"genre.1\"]\n",
    "\n",
    "if \"published_year.1\" in df.columns:\n",
    "    df[\"published_year\"] = df[\"published_year.1\"]\n",
    "\n",
    "if \"price_y\" in df.columns:\n",
    "    df[\"price\"] = df[\"price_y\"]\n",
    "\n",
    "if \"currency_y\" in df.columns:\n",
    "    df[\"currency\"] = df[\"currency_y\"]\n",
    "\n",
    "if \"cover_url.1\" in df.columns:\n",
    "    df[\"cover_url\"] = df[\"cover_url.1\"]\n",
    "\n",
    "# --- Drop unwanted duplicates ---\n",
    "cols_to_drop = [\n",
    "    \"genre.1\", \"published_year.1\",\n",
    "    \"price_x\", \"price_y\",\n",
    "    \"currency_x\", \"currency_y\",\n",
    "    \"cover_url\", \"cover_url.1\"\n",
    "]\n",
    "df = df.drop(columns=[c for c in cols_to_drop if c in df.columns], errors=\"ignore\")\n",
    "\n",
    "# --- Reorder columns (keep avg_rating) ---\n",
    "cols_final = [\n",
    "    \"title\", \"author\", \"avg_rating\", \"genre\",\n",
    "    \"published_year\", \"price\", \"currency\", \"cover_url\", \"link\"\n",
    "]\n",
    "df = df[[c for c in cols_final if c in df.columns]]\n",
    "\n",
    "# --- Save safely under new name ---\n",
    "final_path = Path(\"..\") / config[\"paths\"][\"data_clean\"] / \"books_clean_enriched_final.csv\"\n",
    "df.to_csv(final_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"üíæ Cleaned dataset saved safely ‚Üí {final_path.resolve()}\")\n",
    "print(f\"‚úÖ Final shape: {df.shape}\")\n",
    "display(df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd10f0f6-efaf-4b8c-8b6a-84cf1a484843",
   "metadata": {},
   "source": [
    "## Step 6.1 ‚Äî Recalculate and Populate `avg_rating`\n",
    "\n",
    "In this step, we ensure that the `avg_rating` column in the newly scraped dataset  \n",
    "(`books_clean_enriched_1000.csv`) is complete and consistent with the first dataset  \n",
    "(`books_clean.csv`).\n",
    "\n",
    "Since both datasets represent books from the same Goodreads list,  \n",
    "we use the ratings from the first dataset as a reference.  \n",
    "If a book title exists in both datasets, we copy its `avg_rating`.  \n",
    "If it doesn‚Äôt exist, we assign the global average rating (‚âà 4.1).\n",
    "\n",
    "This guarantees that all books have a valid numerical rating value  \n",
    "before merging both datasets in the next step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc5f123-1299-46f9-a936-314081554723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 6.1 ‚Äî Recalculate and Populate avg_rating\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Paths ---\n",
    "base_path = Path(\"..\") / \"data\" / \"clean\" / \"books_clean.csv\"\n",
    "enriched_path = Path(\"..\") / \"data\" / \"clean\" / \"books_clean_enriched_1000.csv\"\n",
    "\n",
    "# --- Load datasets ---\n",
    "df_base = pd.read_csv(base_path)\n",
    "df_enriched = pd.read_csv(enriched_path)\n",
    "\n",
    "print(f\"üìò Base dataset: {df_base.shape}\")\n",
    "print(f\"üíé Enriched dataset: {df_enriched.shape}\")\n",
    "\n",
    "# --- Compute global average rating for fallback ---\n",
    "global_avg = df_base[\"avg_rating\"].mean()\n",
    "print(f\"üåç Global average rating: {global_avg:.2f}\")\n",
    "\n",
    "# --- Merge ratings by title ---\n",
    "df_enriched = df_enriched.merge(\n",
    "    df_base[[\"title\", \"avg_rating\"]],\n",
    "    on=\"title\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_base\")\n",
    ")\n",
    "\n",
    "# --- Fill missing ratings ---\n",
    "df_enriched[\"avg_rating\"] = df_enriched[\"avg_rating\"].fillna(df_enriched[\"avg_rating_base\"])\n",
    "df_enriched[\"avg_rating\"] = df_enriched[\"avg_rating\"].fillna(global_avg)\n",
    "\n",
    "# --- Drop helper column ---\n",
    "df_enriched = df_enriched.drop(columns=[\"avg_rating_base\"], errors=\"ignore\")\n",
    "\n",
    "# --- Save intermediate result ---\n",
    "df_enriched.to_csv(enriched_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"‚úÖ Ratings populated and saved ‚Üí {enriched_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a674d8ad-761b-460b-b744-32be48d1e953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 6.2 ‚Äî Final Cleanup (Preserve all URLs)\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Load dataset (after ratings filled) ---\n",
    "clean_path = Path(\"..\") / \"data\" / \"clean\" / \"books_clean_enriched_1000.csv\"\n",
    "df = pd.read_csv(clean_path)\n",
    "print(f\"‚úÖ Loaded dataset: {df.shape}\")\n",
    "\n",
    "# --- Replace columns with API-enriched versions ---\n",
    "if \"genre.1\" in df.columns:\n",
    "    df[\"genre\"] = df[\"genre.1\"]\n",
    "\n",
    "if \"published_year.1\" in df.columns:\n",
    "    df[\"published_year\"] = df[\"published_year.1\"]\n",
    "\n",
    "if \"price_y\" in df.columns:\n",
    "    df[\"price\"] = df[\"price_y\"]\n",
    "\n",
    "if \"currency_y\" in df.columns:\n",
    "    df[\"currency\"] = df[\"currency_y\"]\n",
    "\n",
    "if \"cover_url.1\" in df.columns:\n",
    "    df[\"cover_url\"] = df[\"cover_url.1\"]\n",
    "\n",
    "# --- Drop only redundant duplicates ---\n",
    "cols_to_drop = [\n",
    "    \"genre.1\", \"published_year.1\",\n",
    "    \"price_x\", \"price_y\",\n",
    "    \"currency_x\", \"currency_y\",\n",
    "    \"cover_url.1\"\n",
    "]\n",
    "df = df.drop(columns=[c for c in cols_to_drop if c in df.columns], errors=\"ignore\")\n",
    "\n",
    "# --- Keep both 'url' and 'link' if exist ---\n",
    "url_cols = [c for c in [\"url\", \"link\"] if c in df.columns]\n",
    "\n",
    "# --- Reorder columns (preserving both URLs) ---\n",
    "cols_final = [\n",
    "    \"title\", \"author\", \"avg_rating\", \"genre\",\n",
    "    \"published_year\", \"price\", \"currency\",\n",
    "    \"cover_url\", *url_cols\n",
    "]\n",
    "df = df[[c for c in cols_final if c in df.columns]]\n",
    "\n",
    "# --- Save final clean dataset ---\n",
    "final_path = Path(\"..\") / \"data\" / \"clean\" / \"books_clean_enriched_1000.csv\"\n",
    "df.to_csv(final_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"üíæ Cleaned dataset saved successfully ‚Üí {final_path.resolve()}\")\n",
    "print(f\"‚úÖ Final shape: {df.shape}\")\n",
    "display(df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b68ea-62ba-4a57-8120-ae8b7d1029e0",
   "metadata": {},
   "source": [
    "# Step 7 ‚Äî Merge Final Dataset (1000 Books)\n",
    "\n",
    "Now that both datasets are fully cleaned and enriched,  \n",
    "we combine them into a single master dataset containing around 1000 books.\n",
    "\n",
    "This step:\n",
    "- Loads the first dataset (`books_clean.csv`) ‚Äî pages 1‚Äì5  \n",
    "- Loads the new dataset (`books_clean_enriched_1000.csv`) ‚Äî pages 6‚Äì10  \n",
    "- Merges both, removes duplicates by title, and ensures column consistency  \n",
    "- Saves the final version as `books_final_1000.csv` in the `/data/clean` folder\n",
    "\n",
    "The resulting dataset will be the input for the **Exploratory Data Analysis** phase in the next notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ed0aa6-5dd7-4cf1-9f60-6cba8dce6b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 7 ‚Äî Merge Final Dataset (1000 Books)\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Paths ---\n",
    "base_path = Path(\"..\") / config[\"paths\"][\"data_clean\"]\n",
    "path_part1 = base_path / \"books_clean.csv\"                 # Dataset from pages 1‚Äì5\n",
    "path_part2 = base_path / \"books_clean_enriched_1000.csv\"   # Dataset from pages 6‚Äì10\n",
    "path_final = base_path / \"books_final_1000.csv\"            # Output file\n",
    "\n",
    "# --- Load datasets ---\n",
    "df_part1 = pd.read_csv(path_part1)\n",
    "df_part2 = pd.read_csv(path_part2)\n",
    "\n",
    "print(f\"üìò First dataset: {df_part1.shape}\")\n",
    "print(f\"üìó Second dataset: {df_part2.shape}\")\n",
    "\n",
    "# --- Standardize columns ---\n",
    "common_cols = [c for c in df_part1.columns if c in df_part2.columns]\n",
    "df_part1 = df_part1[common_cols]\n",
    "df_part2 = df_part2[common_cols]\n",
    "\n",
    "# --- Combine and clean ---\n",
    "df_final = pd.concat([df_part1, df_part2], ignore_index=True)\n",
    "df_final.drop_duplicates(subset=[\"title\"], inplace=True)\n",
    "\n",
    "print(f\"‚úÖ Combined dataset shape: {df_final.shape}\")\n",
    "print(f\"Unique authors: {df_final['author'].nunique()}\")\n",
    "\n",
    "# --- Quick sanity check ---\n",
    "missing = df_final.isna().sum()\n",
    "print(\"\\nüîç Missing values summary:\")\n",
    "print(missing[missing > 0])\n",
    "\n",
    "# --- Save final dataset ---\n",
    "df_final.to_csv(path_final, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"\\nüíæ Final dataset saved successfully ‚Üí {path_final.resolve()}\")\n",
    "print(df_final.head(10))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80728129-7f2d-4ebf-b120-81c5e96f1509",
   "metadata": {},
   "source": [
    "## Step 7.1 ‚Äî Standardize and Format Final Dataset\n",
    "\n",
    "Before moving on to the modeling phase, we will perform a **final cleaning and standardization** step to ensure consistency across all fields.\n",
    "\n",
    "This includes:\n",
    "- Rounding all numerical values (e.g. `avg_rating`, `price`) to **two decimals**\n",
    "- Stripping extra spaces from text columns\n",
    "- Standardizing capitalization for `genre` and `currency`\n",
    "- Sorting data alphabetically by `title`\n",
    "- Improving **visual alignment** (text to the left, numbers centered) for readability in Jupyter\n",
    "\n",
    "The cleaned dataset will overwrite the existing `books_final_1000.csv` in `data/clean/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26d6c8c-ff65-4f37-aeff-5633f4bcadb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 7.1 ‚Äî Standardize and Format Final Dataset\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Load final dataset ---\n",
    "final_path = Path(\"..\") / config[\"paths\"][\"data_clean\"] / \"books_final_1000.csv\"\n",
    "df = pd.read_csv(final_path)\n",
    "print(f\"‚úÖ Loaded dataset: {df.shape}\")\n",
    "\n",
    "# --- Round numeric columns to 2 decimals ---\n",
    "for col in [\"avg_rating\", \"price\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\").round(2)\n",
    "\n",
    "# --- Enforce numeric display format (2 decimals in Jupyter) ---\n",
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "\n",
    "# --- Clean text columns ---\n",
    "for col in [\"title\", \"author\", \"genre\", \"currency\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "# --- Standardize capitalization ---\n",
    "if \"genre\" in df.columns:\n",
    "    df[\"genre\"] = df[\"genre\"].str.title()\n",
    "if \"currency\" in df.columns:\n",
    "    df[\"currency\"] = df[\"currency\"].str.upper()\n",
    "\n",
    "# --- Sort alphabetically by title ---\n",
    "df = df.sort_values(\"title\").reset_index(drop=True)\n",
    "\n",
    "# --- Save standardized dataset ---\n",
    "df.to_csv(final_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"üíæ Final standardized dataset saved ‚Üí {final_path.resolve()}\")\n",
    "\n",
    "# --- Load the final standardized dataset ---\n",
    "final_path = Path(\"..\") / config[\"paths\"][\"data_clean\"] / \"books_final_1000.csv\"\n",
    "df = pd.read_csv(final_path)\n",
    "print(f\"‚úÖ Loaded dataset for display: {df.shape}\")\n",
    "\n",
    "# --- Jupyter display settings ---\n",
    "pd.set_option(\"display.max_colwidth\", 120)\n",
    "pd.set_option(\"display.float_format\", \"{:.2f}\".format)\n",
    "\n",
    "# --- Define column alignment ---\n",
    "left_cols = [\"title\", \"author\", \"genre\", \"cover_url\", \"link\"]\n",
    "center_cols = [c for c in df.columns if c not in left_cols]\n",
    "\n",
    "# --- Apply notebook-only style ---\n",
    "styled_df = (\n",
    "    df.head(20)\n",
    "    .style\n",
    "    .set_properties(subset=left_cols, **{\"text-align\": \"left\"})\n",
    "    .set_properties(subset=center_cols, **{\"text-align\": \"center\"})\n",
    "    .set_table_styles(\n",
    "        [{\"selector\": \"th\", \"props\": [(\"text-align\", \"center\")]}]  # center headers\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nü™Ñ Preview ‚Äî Text aligned left, numbers centered:\")\n",
    "display(styled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01be33ca-2f72-442e-8431-4ee1224f1e3b",
   "metadata": {},
   "source": [
    "# Step 8 ‚Äî Final Summary and Transition\n",
    "\n",
    "Before moving on to feature extraction and clustering, let's summarize the full pipeline executed in this notebook.\n",
    "\n",
    "We have combined both enriched datasets (Part 1 and Part 2), standardized them, and validated the integrity of the final dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Summary of the Data Merge\n",
    "\n",
    "**First dataset:** (493, 9)  \n",
    "**Second dataset:** (697, 8)  \n",
    "‚úÖ **Combined dataset shape:** (1190, 8)  \n",
    "üë©‚Äçüíª **Unique authors:** 714  \n",
    "\n",
    "**Missing values summary:**\n",
    "\n",
    "| Column | Missing Values |\n",
    "|:--|--:|\n",
    "| genre | 149 |\n",
    "| price | 636 |\n",
    "| currency | 636 |\n",
    "| cover_url | 118 |\n",
    "\n",
    "---\n",
    "\n",
    "üíæ **Final dataset saved successfully ‚Üí**\n",
    "\n",
    "data/clean/books_final_1000.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2e170c-c166-4f75-98b9-a339bbdd1a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total unique books: {df_final['title'].nunique()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book_recommender (uv)",
   "language": "python",
   "name": "book-recommendation-system"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
