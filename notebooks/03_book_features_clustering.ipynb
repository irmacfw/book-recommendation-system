{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68329238-301b-4926-898b-23d8902626c5",
   "metadata": {},
   "source": [
    "### üìò Notebook 03 ‚Äî Feature Engineering & Clustering\n",
    "\n",
    "In this notebook, we move from **data preparation** to **unsupervised learning**.  \n",
    "Using the cleaned dataset (`books_final_1000.csv`), we‚Äôll extract key features such as ratings, price, and genre to group similar books with **K-Means clustering**.\n",
    "\n",
    "**Goals:**\n",
    "- Prepare numerical and categorical features  \n",
    "- Apply **K-Means** and evaluate with **Elbow Method** & **Silhouette Score**  \n",
    "- Visualize and interpret clusters for future recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436f659a-f4e5-4af3-a457-08c4452d07dd",
   "metadata": {},
   "source": [
    "### Step 1 ‚Äî Imports & Setup\n",
    "\n",
    "Import core libraries for clustering and visualization,  \n",
    "reload the shared `functions.py` module, and verify that all paths from `config.yaml` are available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f027a062-e911-468c-8c82-bcc84cd11f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 1 ‚Äî Imports & Setup \n",
    "# ============================================================\n",
    "\n",
    "# --- System and project setup ---\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add 'notebooks' folder to path (functions.py lives there)\n",
    "sys.path.append(\"notebooks\")\n",
    "\n",
    "# --- Load shared utilities ---\n",
    "from functions import load_config, ensure_directories\n",
    "\n",
    "# --- ML and visualization libraries ---\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# --- Visualization settings (como en clase) ---\n",
    "sns.set(style=\"whitegrid\", palette=\"muted\")\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 5)\n",
    "\n",
    "# --- Load configuration and verify folders ---\n",
    "config_path = Path(\"..\") / \"config.yaml\"\n",
    "config = load_config(config_path)\n",
    "ensure_directories(config[\"paths\"])\n",
    "\n",
    "print(\"‚úÖ Environment ready ‚Äî config loaded and directories verified.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf9a4e9-fb7b-4eff-a38a-6144010179be",
   "metadata": {},
   "source": [
    "### Step 2 ‚Äî Load Final Dataset  \n",
    "\n",
    "Load the cleaned and standardized dataset (`books_final_1000.csv`) generated in the previous notebook.  \n",
    "We‚Äôll inspect its structure, check column types, and verify that all key variables are ready for feature preparation.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cc82b0-b7dd-495a-99a2-9b71470a1098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 2 ‚Äî Load Final Dataset\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Load dataset from data/clean ---\n",
    "data_path = Path(\"..\") / config[\"paths\"][\"data_clean\"] / \"books_final_1000.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded successfully: {data_path}\")\n",
    "print(f\"Shape: {df.shape}\\n\")\n",
    "\n",
    "# --- Quick overview ---\n",
    "display(df.head(5))\n",
    "\n",
    "# --- Basic info and types ---\n",
    "print(\"\\nüîç DataFrame Info:\")\n",
    "print(df.info())\n",
    "\n",
    "# --- Missing values summary ---\n",
    "missing_summary = df.isna().sum()\n",
    "missing_summary = missing_summary[missing_summary > 0]\n",
    "\n",
    "if not missing_summary.empty:\n",
    "    print(\"\\n‚ö†Ô∏è Missing values summary:\")\n",
    "    print(missing_summary)\n",
    "else:\n",
    "    print(\"\\n‚úÖ No missing values detected.\")\n",
    "\n",
    "# --- Optional: Unique values check (for categorical columns) ---\n",
    "print(\"\\nüß© Unique values per column:\")\n",
    "print(df.nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfe587d-3a9c-4a0c-9397-9895e6c39cdf",
   "metadata": {},
   "source": [
    "### Step 2.1 ‚Äî Clean & Normalize Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53e586f-4ba3-455a-8892-2ce5ce3782ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 2.1 ‚Äî Clean & Normalize Genres\n",
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "üéØ Ensure genre names are consistent and meaningful.\n",
    "Preserve subcategories (e.g., Young Adult Fiction, Juvenile Fiction)\n",
    "and fix missing or inconsistent values.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# --- Clean genre text ---\n",
    "df[\"genre\"] = (\n",
    "    df[\"genre\"]\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .replace({\"nan\": np.nan, \"None\": np.nan})\n",
    ")\n",
    "\n",
    "# --- Replace NaN with \"Unknown\" ---\n",
    "df[\"genre\"] = df[\"genre\"].fillna(\"Unknown\")\n",
    "\n",
    "# --- Title case normalization ---\n",
    "df[\"genre\"] = df[\"genre\"].str.title()\n",
    "\n",
    "# --- Fix common variants ---\n",
    "genre_replacements = {\n",
    "    \"Juvenile Fiction \": \"Juvenile Fiction\",\n",
    "    \"Young Adult Fiction \": \"Young Adult Fiction\",\n",
    "    \"Biography & Autobiography \": \"Biography & Autobiography\",\n",
    "    \"Nan\": \"Unknown\"\n",
    "}\n",
    "df[\"genre\"] = df[\"genre\"].replace(genre_replacements)\n",
    "\n",
    "# --- Final check ---\n",
    "print(\"‚úÖ Genre normalization complete.\\n\")\n",
    "print(\"Top 10 genres after cleaning:\")\n",
    "display(df[\"genre\"].value_counts().head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdc3dd9-e70a-4a10-987b-c2785ad84da6",
   "metadata": {},
   "source": [
    "### Step 2.2 ‚Äî Consolidate Main Genres (for Descriptive Analysis)\n",
    "\n",
    "Before running clustering, we standardize and consolidate similar genre labels\n",
    "(e.g., Juvenile Fiction and Young Adult Fiction) under their main categories.\n",
    "\n",
    "This step does not affect the clustering model directly,\n",
    "but ensures cleaner genre information for later descriptive and visualization steps\n",
    "(e.g., identifying dominant genres within each cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08302ca-d70c-4446-8a6a-4d26b7c274d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 2.2 ‚Äî Clean and Normalize Genres Before Encoding\n",
    "# ============================================================\n",
    "\n",
    "def normalize_genre(value):\n",
    "    if pd.isna(value):\n",
    "        return \"Unknown\"\n",
    "    value = value.strip().title()\n",
    "    # Simplify subcategories\n",
    "    if \"Juvenile\" in value:\n",
    "        return \"Juvenile Fiction\"\n",
    "    if \"Young Adult\" in value:\n",
    "        return \"Young Adult Fiction\"\n",
    "    if \"Biography\" in value:\n",
    "        return \"Biography & Autobiography\"\n",
    "    if \"Comics\" in value or \"Graphic\" in value:\n",
    "        return \"Comics & Graphic Novels\"\n",
    "    if \"Poet\" in value:\n",
    "        return \"Poetry\"\n",
    "    if \"Drama\" in value:\n",
    "        return \"Drama\"\n",
    "    if \"Relig\" in value:\n",
    "        return \"Religion\"\n",
    "    if \"History\" in value:\n",
    "        return \"History\"\n",
    "    if \"Fiction\" in value:\n",
    "        return \"Fiction\"\n",
    "    return value\n",
    "\n",
    "# Apply normalization\n",
    "df[\"genre\"] = df[\"genre\"].apply(normalize_genre)\n",
    "\n",
    "print(\"‚úÖ Genres normalized successfully.\\n\")\n",
    "print(\"Top 10 genres after normalization:\")\n",
    "display(df[\"genre\"].value_counts().head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b3587b-bc60-444b-97b0-23dc52c64bb0",
   "metadata": {},
   "source": [
    "### Step 3 ‚Äî Feature Preparation\n",
    "\n",
    "We now prepare the numerical features for K-Means clustering.\n",
    "\n",
    "- Selected features: **avg_rating** and **price**\n",
    "- Missing prices are filled with the median value\n",
    "- Features are standardized using **StandardScaler**\n",
    "- The resulting matrix `X` will be used for clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91b342a-aaaa-48e7-a78c-9d6a78d61055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 3 ‚Äî Feature Preparation (Numeric Features Only)\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Select relevant numerical columns for clustering ---\n",
    "features = [\"avg_rating\", \"price\"]\n",
    "df_features = df[features].copy()\n",
    "\n",
    "# --- Handle missing prices ---\n",
    "median_price = df_features[\"price\"].median()\n",
    "df_features[\"price\"] = df_features[\"price\"].fillna(median_price)\n",
    "\n",
    "# --- Reflect the filled prices back into the main DataFrame ---\n",
    "df[\"price\"] = df[\"price\"].fillna(median_price)\n",
    "print(f\"Filled missing 'price' values with median: {median_price:.2f}\")\n",
    "\n",
    "# --- Standardize numeric columns ---\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df_features)\n",
    "\n",
    "# --- Convert back to DataFrame for easier handling ---\n",
    "df_encoded = pd.DataFrame(df_scaled, columns=features)\n",
    "\n",
    "# --- Sanity check ---\n",
    "missing_check = df_encoded.isna().sum().sum()\n",
    "if missing_check == 0:\n",
    "    print(\"‚úÖ No missing values remain in feature matrix.\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è {missing_check} missing values still present ‚Äî check source data.\")\n",
    "\n",
    "# --- Assign to feature matrix for clustering ---\n",
    "X = df_encoded.values\n",
    "\n",
    "print(f\"\\n‚úÖ Feature matrix ready for clustering. Shape: {df_encoded.shape}\")\n",
    "\n",
    "# --- Quick preview ---\n",
    "display(df_encoded.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610698bf-5f3d-4739-8d38-971a5d463a8c",
   "metadata": {},
   "source": [
    "## Step 4 ‚Äî K-Means Clustering (Elbow & Silhouette Method)\n",
    "\n",
    "In this step, we apply the K-Means algorithm to identify potential groups (clusters) among the books.\n",
    "\n",
    "We will:\n",
    "1. Run K-Means for different values of *k* (from 2 to 10)\n",
    "2. Record both the **Inertia** (Elbow Method) and the **Silhouette Score**\n",
    "3. Identify the optimal number of clusters (`best_k`) based on the highest silhouette score\n",
    "4. Visualize both metrics side by side for comparison\n",
    "\n",
    "The **Elbow Method** helps detect the point where adding more clusters no longer improves the model significantly,  \n",
    "while the **Silhouette Score** evaluates how well-separated the clusters are (higher values indicate better-defined clusters).\n",
    "\n",
    "Finally, both the visualizations and metrics are saved for later analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d629a38-68ab-4e1d-9912-a3def855c82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 4 ‚Äî K-Means Clustering (Elbow & Silhouette Method) ‚Äî Fixed Silhouette Range\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# --- Feature matrix (numeric only) ---\n",
    "X = df_encoded.values\n",
    "\n",
    "# --- Initialize lists ---\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "print(\"Running K-Means for k = 2 to 10...\\n\")\n",
    "\n",
    "# --- Run K-Means across different k values ---\n",
    "for k in K_range:\n",
    "    try:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(X)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "        score = silhouette_score(X, kmeans.labels_)\n",
    "        silhouette_scores.append(score)\n",
    "        print(f\"k={k} ‚Äî Inertia={kmeans.inertia_:.2f}, Silhouette={score:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error for k={k}: {e}\")\n",
    "        inertias.append(np.nan)\n",
    "        silhouette_scores.append(np.nan)\n",
    "\n",
    "# --- Determine best k by silhouette score ---\n",
    "valid_scores = [s for s in silhouette_scores if not np.isnan(s)]\n",
    "best_k = list(K_range)[silhouette_scores.index(max(valid_scores))]\n",
    "\n",
    "# --- Show top 3 silhouette values ---\n",
    "sorted_scores = sorted(zip(K_range, silhouette_scores), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nTop 3 silhouette scores:\")\n",
    "for i, (k_val, s_val) in enumerate(sorted_scores[:3], start=1):\n",
    "    print(f\"{i}. k={k_val} ‚Üí silhouette={s_val:.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Best k by silhouette score: {best_k}\")\n",
    "\n",
    "# ============================================================\n",
    "# Visualization ‚Äî Elbow & Silhouette (Range -1 to 1)\n",
    "# ============================================================\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 4))\n",
    "\n",
    "# --- Elbow Method ---\n",
    "ax1.plot(K_range, inertias, marker='o', color='steelblue')\n",
    "ax1.set_title(\"Elbow Method ‚Äî K-Means Inertia\", fontsize=11)\n",
    "ax1.set_xlabel(\"Number of Clusters (k)\")\n",
    "ax1.set_ylabel(\"Inertia\")\n",
    "ax1.grid(alpha=0.3, linestyle='--')\n",
    "\n",
    "# --- Silhouette Scores ---\n",
    "ax2.plot(K_range, silhouette_scores, marker='o', color='orange')\n",
    "ax2.set_title(\"Silhouette Scores (Range -1 to 1)\", fontsize=11)\n",
    "ax2.set_xlabel(\"Number of Clusters (k)\")\n",
    "ax2.set_ylabel(\"Silhouette Score\")\n",
    "ax2.set_ylim(-1, 1)  # <-- ‚úÖ Force the correct range\n",
    "ax2.axhline(0, color='gray', linestyle='--', linewidth=0.8)\n",
    "ax2.grid(alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# Save Results\n",
    "# ============================================================\n",
    "\n",
    "viz_path = Path(\"..\") / \"visualizations\"\n",
    "viz_path.mkdir(parents=True, exist_ok=True)\n",
    "fig.savefig(viz_path / \"kmeans_elbow_silhouette_fixed.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "metrics_path = Path(\"..\") / \"data\" / \"clean\"\n",
    "metrics_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"k\": list(K_range),\n",
    "    \"inertia\": inertias,\n",
    "    \"silhouette\": silhouette_scores\n",
    "})\n",
    "metrics_df.to_csv(metrics_path / \"kmeans_metrics_fixed.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"\\nüíæ Results saved ‚Üí {metrics_path / 'kmeans_metrics_fixed.csv'}\")\n",
    "print(f\"üè∑Ô∏è Best number of clusters: {best_k}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707b605c-79de-41e4-bea2-567c0891b1ec",
   "metadata": {},
   "source": [
    "\n",
    "###  Interpretation of Clustering Metrics\n",
    "\n",
    "\n",
    "The Elbow Method shows a strong drop in inertia from k = 2 to k = 3, after which the curve flattens ‚Äî suggesting diminishing returns in compactness beyond three clusters.\n",
    "\n",
    "The Silhouette Scores (ranging from ‚Äì1 to 1) indicate that k = 2 achieves the highest separation (‚âà 0.74), meaning clusters are more compact and distinct.\n",
    "However, this two-cluster model oversimplifies the data, grouping too many diverse books together.\n",
    "\n",
    "üìä Decision: Although the silhouette metric suggests k = 2, we selected k = 3 as the final configuration ‚Äî providing a more interpretable segmentation that better reflects real differences in rating and price (e.g., affordable, mid-range, and premium books).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3391d8-a49f-4822-8347-b5bde5ee39bb",
   "metadata": {},
   "source": [
    "## Step 5 ‚Äî Apply Final K-Means & Visualize Clusters (PCA 2D)\n",
    "\n",
    "After determining the optimal number of clusters (`k=2`), we apply the final K-Means model to assign each book to a specific cluster.\n",
    "\n",
    "Steps performed:\n",
    "1. Scale all numeric features for consistent distance calculation.\n",
    "2. Train the K-Means model using the selected value of *k*.\n",
    "3. Assign the resulting cluster labels to each book.\n",
    "4. Apply **Principal Component Analysis (PCA)** to reduce the feature space to two dimensions for visualization.\n",
    "5. Plot the resulting clusters using a 2D scatter plot, where each point represents a book and color indicates its cluster.\n",
    "\n",
    "The resulting visualization helps identify distinct groups of books based on their ratings, price levels, and genre characteristics.\n",
    "Both the model output and visualization are saved for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f01ebbe-e7e9-4fed-bbe0-6cc9c67b48ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 5 ‚Äî Apply Final K-Means & Visualize Clusters (PCA 2D, k = 3)\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# --- Final number of clusters ---\n",
    "k_final = 3\n",
    "print(f\"Applying final K-Means model with k = {k_final}...\\n\")\n",
    "\n",
    "# --- Use scaled numeric features from df_encoded (already standardized) ---\n",
    "X_scaled = df_encoded.values\n",
    "\n",
    "# --- Train K-Means ---\n",
    "kmeans_final = KMeans(n_clusters=k_final, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans_final.fit_predict(X_scaled)\n",
    "\n",
    "# --- Assign clusters safely ---\n",
    "df = df.copy()\n",
    "df[\"cluster\"] = cluster_labels\n",
    "\n",
    "# --- PCA for visualization ---\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "pca_components = pca.fit_transform(X_scaled)\n",
    "df[\"pca_1\"] = pca_components[:, 0]\n",
    "df[\"pca_2\"] = pca_components[:, 1]\n",
    "\n",
    "# ============================================================\n",
    "# Visualization ‚Äî PCA 2D Scatter Plot (k = 3)\n",
    "# ============================================================\n",
    "\n",
    "fig_pca, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=df,\n",
    "    x=\"pca_1\", y=\"pca_2\",\n",
    "    hue=\"cluster\",\n",
    "    palette=\"Set2\",\n",
    "    s=65,\n",
    "    alpha=0.85,\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=0.7,\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_title(f\"Book Clusters ‚Äî PCA 2D Projection (k = {k_final})\", fontsize=13, pad=10)\n",
    "ax.set_xlabel(\"Principal Component 1\")\n",
    "ax.set_ylabel(\"Principal Component 2\")\n",
    "ax.legend(title=\"Cluster\", loc=\"best\", fontsize=9)\n",
    "ax.grid(alpha=0.25, linestyle=\"--\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# Save Plot\n",
    "# ============================================================\n",
    "\n",
    "viz_path = Path(\"..\") / \"visualizations\"\n",
    "viz_path.mkdir(parents=True, exist_ok=True)\n",
    "fig_pca.savefig(viz_path / f\"pca_clusters_k{k_final}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "print(f\"Clustering completed. {df['cluster'].nunique()} clusters created.\")\n",
    "print(f\"PCA cluster visualization saved ‚Üí {viz_path / f'pca_clusters_k{k_final}.png'}\\n\")\n",
    "\n",
    "# --- Preview sample ---\n",
    "display(df[[\"title\", \"author\", \"avg_rating\", \"genre\", \"price\", \"cluster\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa62509-c60f-49fa-9330-f368d3f3c72f",
   "metadata": {},
   "source": [
    "### üìä Interpretation of Final Clusters (k = 3)\n",
    "\n",
    "The **3-cluster configuration** reveals three meaningful groups of books based primarily on their **average rating** and **price level**.  \n",
    "Although genre information wasn‚Äôt directly used in the clustering, it helps describe each group‚Äôs general tendencies.\n",
    "\n",
    "---\n",
    "\n",
    " **Cluster 0 (Green ‚Äî Affordable / Mainstream)**\n",
    "- Contains books with **lower prices** (mostly below ‚Ç¨10).  \n",
    "- ‚≠ê Average ratings remain strong (‚âà 4.0‚Äì4.2), suggesting consistent reader satisfaction despite lower cost.  \n",
    "- Genres are mainly **fiction and young adult**, representing **popular mainstream titles**.\n",
    "\n",
    "---\n",
    "\n",
    "**Cluster 1 (Blue ‚Äî Mid-Range / Balanced)**\n",
    "- Represents books with **moderate prices** and **balanced ratings** (‚âà 4.1).  \n",
    "- Combines both **fiction** and **non-fiction** categories, offering a diverse yet stable segment.  \n",
    "- This group likely includes **widely read and accessible titles** with broad appeal.\n",
    "\n",
    "---\n",
    "\n",
    "**Cluster 2 (Orange ‚Äî Premium / Niche)**\n",
    "- Features **higher-priced titles**, often above ‚Ç¨25.  \n",
    "- ‚≠ê Ratings remain solid (‚âà 4.0), but the group shows greater price variability.  \n",
    "- Genres tend to include **art, criticism, or specialized works**, indicating **premium or niche markets**.\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "The model now differentiates books by **price tiers** and **market positioning**:  \n",
    "\n",
    "- **Cluster 0 ‚Üí** Affordable, mainstream fiction ‚Äî broad audience appeal.  \n",
    "- **Cluster 1 ‚Üí** Balanced, mid-range titles ‚Äî mix of genres with stable quality.  \n",
    "- **Cluster 2 ‚Üí** Premium, niche books ‚Äî higher prices, specialized interest.  \n",
    "\n",
    "This 3-cluster segmentation provides richer insight for future **recommendation logic**,  \n",
    "allowing readers to explore books within their preferred **value, genre, or quality range**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7170f744-bce7-4ec8-938c-bf6905a6b626",
   "metadata": {},
   "source": [
    "### Step 5.1 ‚Äî Analyze Cluster Centroids\n",
    "\n",
    "Let‚Äôs inspect the numerical centroids of each cluster to understand\n",
    "how books are grouped ‚Äî for example, by their average rating or price.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8815b3b-c418-44d6-abe5-baee33207a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 5.2 ‚Äî Cluster Composition Summary (k = 3)\n",
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "üéØ Step 5.2 ‚Äî Analyze Cluster Composition by Genre (k = 3)\n",
    "This version summarizes how genres distribute within each cluster,\n",
    "while also showing the average rating and price per cluster.\n",
    "\"\"\"\n",
    "\n",
    "# --- Numeric centroids (average rating & price) ---\n",
    "cluster_centroids = (\n",
    "    df.groupby(\"cluster\", as_index=False)\n",
    "    .agg({\n",
    "        \"avg_rating\": \"mean\",\n",
    "        \"price\": \"mean\"\n",
    "    })\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "# --- Genre distribution within each cluster ---\n",
    "genre_distribution = (\n",
    "    df.groupby([\"cluster\", \"genre\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "# --- Calculate proportions per cluster ---\n",
    "cluster_sizes = df[\"cluster\"].value_counts().to_dict()\n",
    "genre_distribution[\"proportion_%\"] = genre_distribution.apply(\n",
    "    lambda row: round((row[\"count\"] / cluster_sizes[row[\"cluster\"]]) * 100, 2),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# --- Merge numeric averages with genre distribution ---\n",
    "cluster_summary = (\n",
    "    genre_distribution.merge(cluster_centroids, on=\"cluster\", how=\"left\")\n",
    "    .sort_values([\"cluster\", \"count\"], ascending=[True, False])\n",
    ")\n",
    "\n",
    "# --- Display top 5 genres per cluster ---\n",
    "print(\"üìä Cluster Composition Summary (Top 5 Genres per Cluster, k = 3):\\n\")\n",
    "display(cluster_summary.groupby(\"cluster\").head(5))\n",
    "\n",
    "print(\"\\nüß≠ Interpretation Guide:\")\n",
    "print(\"- avg_rating ‚Üí Average rating within the cluster.\")\n",
    "print(\"- price ‚Üí Mean price (EUR) within the cluster.\")\n",
    "print(\"- count ‚Üí Number of books belonging to that genre in the cluster.\")\n",
    "print(\"- proportion_% ‚Üí Relative share (%) of that genre within the cluster.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e422c0-6a71-4e22-8e49-fbd29c995ae5",
   "metadata": {},
   "source": [
    "### üìö Interpretation ‚Äî Cluster Composition by Genre (k = 3)\n",
    "\n",
    "The genre distribution confirms a clear segmentation of the book market into three tiers based on price and reader ratings:\n",
    "\n",
    "Cluster 0 represents the mainstream and affordable segment ‚Äî books priced around ‚Ç¨8.28, with strong average ratings (‚âà 3.87 ‚òÖ), mostly belonging to fiction and young adult genres.\n",
    "\n",
    "Cluster 1 captures the mid-range market, combining both fiction and non-fiction titles. These books maintain balanced ratings (‚âà 4.19) and moderate prices around ‚Ç¨8.89, reflecting broadly popular and accessible works.\n",
    "\n",
    "Cluster 2 consists of premium and niche titles, typically priced around ‚Ç¨43, including art, criticism, and specialized literature. Despite their higher price, these books sustain solid reader satisfaction, highlighting their collector or expert appeal (around 4.21‚òÖ).\n",
    "\n",
    "Overall, even though genre was not included as a clustering feature, its distribution reveals how the clusters naturally align with economic and thematic segmentation ‚Äî from accessible mainstream fiction to high-value specialized works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db1fc3c-0a01-4b19-9108-35d25c40e2c4",
   "metadata": {},
   "source": [
    "## Step 6 ‚Äî Export Final Clustered Dataset\n",
    "\n",
    "We‚Äôll now export the final dataset including the cluster labels (`cluster`)\n",
    "and PCA coordinates (`pca_1`, `pca_2`) for visualization and further analysis.  \n",
    "This dataset can be used in **Tableau**, **Power BI**, or in the next notebook\n",
    "for building the **Recommendation System**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf3f9d1-96b0-40c9-a3bd-e3dcfbc85d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 6 ‚Äî Export Final Clustered Dataset & Cluster Summary (Overwrite Only)\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# --- Define export paths (overwrite mode) ---\n",
    "data_clean_path = Path(\"..\") / \"data\" / \"clean\"\n",
    "viz_path = Path(\"..\") / \"visualizations\"\n",
    "data_clean_path.mkdir(parents=True, exist_ok=True)\n",
    "viz_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Export final dataset (overwrite previous file) ---\n",
    "final_cluster_path = data_clean_path / \"books_clustered_final.csv\"\n",
    "\n",
    "export_cols = [\n",
    "    \"title\", \"author\", \"avg_rating\", \"genre\", \"price\", \"currency\",\n",
    "    \"cover_url\", \"link\", \"cluster\", \"pca_1\", \"pca_2\"\n",
    "]\n",
    "\n",
    "df[export_cols].to_csv(final_cluster_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"üíæ Final clustered dataset (k = 3) overwritten successfully ‚Üí {final_cluster_path.resolve()}\")\n",
    "\n",
    "print(\"\\nüìò Sample of exported dataset:\")\n",
    "display(df[export_cols].head(10))\n",
    "\n",
    "# ============================================================\n",
    "# üìä Cluster Summary (genre proportions + numeric averages)\n",
    "# ============================================================\n",
    "\n",
    "# --- Numeric stats per cluster ---\n",
    "cluster_centroids = (\n",
    "    df.groupby(\"cluster\", as_index=False)\n",
    "    .agg({\n",
    "        \"avg_rating\": \"mean\",\n",
    "        \"price\": \"mean\"\n",
    "    })\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "# --- Genre distribution per cluster ---\n",
    "genre_distribution = (\n",
    "    df.groupby([\"cluster\", \"genre\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "# --- Add proportions per cluster ---\n",
    "cluster_sizes = df[\"cluster\"].value_counts().to_dict()\n",
    "genre_distribution[\"proportion_%\"] = genre_distribution.apply(\n",
    "    lambda row: round((row[\"count\"] / cluster_sizes[row[\"cluster\"]]) * 100, 2),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# --- Merge numeric stats with genre composition ---\n",
    "cluster_summary = genre_distribution.merge(cluster_centroids, on=\"cluster\", how=\"left\")\n",
    "cluster_summary = cluster_summary.sort_values([\"cluster\", \"count\"], ascending=[True, False])\n",
    "\n",
    "print(\"\\nüìó Cluster Summary (Top Genres per Cluster, k = 3):\")\n",
    "display(cluster_summary.groupby(\"cluster\").head(5))\n",
    "\n",
    "# ‚úÖ No new CSV created ‚Äî previous files remain updated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924d74d7-72be-4b44-ad4a-75af8e77cda0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book_recommender (uv)",
   "language": "python",
   "name": "book-recommendation-system"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
