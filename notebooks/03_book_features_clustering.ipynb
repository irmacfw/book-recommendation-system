{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68329238-301b-4926-898b-23d8902626c5",
   "metadata": {},
   "source": [
    "### üìò Notebook 03 ‚Äî Feature Engineering & Clustering\n",
    "\n",
    "In this notebook, we move from **data preparation** to **unsupervised learning**.  \n",
    "Using the cleaned dataset (`books_final_1000.csv`), we‚Äôll extract key features such as ratings, price, and genre to group similar books with **K-Means clustering**.\n",
    "\n",
    "**Goals:**\n",
    "- Prepare numerical and categorical features  \n",
    "- Apply **K-Means** and evaluate with **Elbow Method** & **Silhouette Score**  \n",
    "- Visualize and interpret clusters for future recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436f659a-f4e5-4af3-a457-08c4452d07dd",
   "metadata": {},
   "source": [
    "### Step 1 ‚Äî Imports & Setup\n",
    "\n",
    "Import core libraries for clustering and visualization,  \n",
    "reload the shared `functions.py` module, and verify that all paths from `config.yaml` are available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f027a062-e911-468c-8c82-bcc84cd11f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 1 ‚Äî Imports & Setup \n",
    "# ============================================================\n",
    "\n",
    "# --- System and project setup ---\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add 'notebooks' folder to path (functions.py lives there)\n",
    "sys.path.append(\"notebooks\")\n",
    "\n",
    "# --- Load shared utilities ---\n",
    "from functions import load_config, ensure_directories\n",
    "\n",
    "# --- ML and visualization libraries ---\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# --- Visualization settings (como en clase) ---\n",
    "sns.set(style=\"whitegrid\", palette=\"muted\")\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 5)\n",
    "\n",
    "# --- Load configuration and verify folders ---\n",
    "config_path = Path(\"..\") / \"config.yaml\"\n",
    "config = load_config(config_path)\n",
    "ensure_directories(config[\"paths\"])\n",
    "\n",
    "print(\"‚úÖ Environment ready ‚Äî config loaded and directories verified.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf9a4e9-fb7b-4eff-a38a-6144010179be",
   "metadata": {},
   "source": [
    "### Step 2 ‚Äî Load Final Dataset  \n",
    "\n",
    "Load the cleaned and standardized dataset (`books_final_1000.csv`) generated in the previous notebook.  \n",
    "We‚Äôll inspect its structure, check column types, and verify that all key variables are ready for feature preparation.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cc82b0-b7dd-495a-99a2-9b71470a1098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 2 ‚Äî Load Final Dataset\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Load dataset from data/clean ---\n",
    "data_path = Path(\"..\") / config[\"paths\"][\"data_clean\"] / \"books_final_1000.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded successfully: {data_path}\")\n",
    "print(f\"Shape: {df.shape}\\n\")\n",
    "\n",
    "# --- Quick overview ---\n",
    "display(df.head(5))\n",
    "\n",
    "# --- Basic info and types ---\n",
    "print(\"\\nüîç DataFrame Info:\")\n",
    "print(df.info())\n",
    "\n",
    "# --- Missing values summary ---\n",
    "missing_summary = df.isna().sum()\n",
    "missing_summary = missing_summary[missing_summary > 0]\n",
    "\n",
    "if not missing_summary.empty:\n",
    "    print(\"\\n‚ö†Ô∏è Missing values summary:\")\n",
    "    print(missing_summary)\n",
    "else:\n",
    "    print(\"\\n‚úÖ No missing values detected.\")\n",
    "\n",
    "# --- Optional: Unique values check (for categorical columns) ---\n",
    "print(\"\\nüß© Unique values per column:\")\n",
    "print(df.nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfe587d-3a9c-4a0c-9397-9895e6c39cdf",
   "metadata": {},
   "source": [
    "### Step 2.1 ‚Äî Clean & Normalize Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53e586f-4ba3-455a-8892-2ce5ce3782ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 2.1 ‚Äî Clean & Normalize Genres\n",
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "üéØ Ensure genre names are consistent and meaningful.\n",
    "Preserve subcategories (e.g., Young Adult Fiction, Juvenile Fiction)\n",
    "and fix missing or inconsistent values.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# --- Clean genre text ---\n",
    "df[\"genre\"] = (\n",
    "    df[\"genre\"]\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .replace({\"nan\": np.nan, \"None\": np.nan})\n",
    ")\n",
    "\n",
    "# --- Replace NaN with \"Unknown\" ---\n",
    "df[\"genre\"] = df[\"genre\"].fillna(\"Unknown\")\n",
    "\n",
    "# --- Title case normalization ---\n",
    "df[\"genre\"] = df[\"genre\"].str.title()\n",
    "\n",
    "# --- Fix common variants ---\n",
    "genre_replacements = {\n",
    "    \"Juvenile Fiction \": \"Juvenile Fiction\",\n",
    "    \"Young Adult Fiction \": \"Young Adult Fiction\",\n",
    "    \"Biography & Autobiography \": \"Biography & Autobiography\",\n",
    "    \"Nan\": \"Unknown\"\n",
    "}\n",
    "df[\"genre\"] = df[\"genre\"].replace(genre_replacements)\n",
    "\n",
    "# --- Final check ---\n",
    "print(\"‚úÖ Genre normalization complete.\\n\")\n",
    "print(\"Top 10 genres after cleaning:\")\n",
    "display(df[\"genre\"].value_counts().head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdc3dd9-e70a-4a10-987b-c2785ad84da6",
   "metadata": {},
   "source": [
    "### Step 2.2 ‚Äî Consolidate Main Genres (for Descriptive Analysis)\n",
    "\n",
    "Before running clustering, we standardize and consolidate similar genre labels\n",
    "(e.g., Juvenile Fiction and Young Adult Fiction) under their main categories.\n",
    "\n",
    "This step does not affect the clustering model directly,\n",
    "but ensures cleaner genre information for later descriptive and visualization steps\n",
    "(e.g., identifying dominant genres within each cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08302ca-d70c-4446-8a6a-4d26b7c274d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 2.2 ‚Äî Clean and Normalize Genres Before Encoding\n",
    "# ============================================================\n",
    "\n",
    "def normalize_genre(value):\n",
    "    if pd.isna(value):\n",
    "        return \"Unknown\"\n",
    "    value = value.strip().title()\n",
    "    # Simplify subcategories\n",
    "    if \"Juvenile\" in value:\n",
    "        return \"Juvenile Fiction\"\n",
    "    if \"Young Adult\" in value:\n",
    "        return \"Young Adult Fiction\"\n",
    "    if \"Biography\" in value:\n",
    "        return \"Biography & Autobiography\"\n",
    "    if \"Comics\" in value or \"Graphic\" in value:\n",
    "        return \"Comics & Graphic Novels\"\n",
    "    if \"Poet\" in value:\n",
    "        return \"Poetry\"\n",
    "    if \"Drama\" in value:\n",
    "        return \"Drama\"\n",
    "    if \"Relig\" in value:\n",
    "        return \"Religion\"\n",
    "    if \"History\" in value:\n",
    "        return \"History\"\n",
    "    if \"Fiction\" in value:\n",
    "        return \"Fiction\"\n",
    "    return value\n",
    "\n",
    "# Apply normalization\n",
    "df[\"genre\"] = df[\"genre\"].apply(normalize_genre)\n",
    "\n",
    "print(\"‚úÖ Genres normalized successfully.\\n\")\n",
    "print(\"Top 10 genres after normalization:\")\n",
    "display(df[\"genre\"].value_counts().head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b3587b-bc60-444b-97b0-23dc52c64bb0",
   "metadata": {},
   "source": [
    "### Step 3 ‚Äî Feature Preparation\n",
    "\n",
    "We now prepare the numerical features for K-Means clustering.\n",
    "\n",
    "- Selected features: **avg_rating** and **price**\n",
    "- Missing prices are filled with the median value\n",
    "- Features are standardized using **StandardScaler**\n",
    "- The resulting matrix `X` will be used for clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91b342a-aaaa-48e7-a78c-9d6a78d61055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 3 ‚Äî Feature Preparation (Numeric Features Only)\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Select relevant numerical columns for clustering ---\n",
    "features = [\"avg_rating\", \"price\"]\n",
    "df_features = df[features].copy()\n",
    "\n",
    "# --- Handle missing prices ---\n",
    "median_price = df_features[\"price\"].median()\n",
    "df_features[\"price\"] = df_features[\"price\"].fillna(median_price)\n",
    "\n",
    "# --- Reflect the filled prices back into the main DataFrame ---\n",
    "df[\"price\"] = df[\"price\"].fillna(median_price)\n",
    "print(f\"Filled missing 'price' values with median: {median_price:.2f}\")\n",
    "\n",
    "# --- Standardize numeric columns ---\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df_features)\n",
    "\n",
    "# --- Convert back to DataFrame for easier handling ---\n",
    "df_encoded = pd.DataFrame(df_scaled, columns=features)\n",
    "\n",
    "# --- Sanity check ---\n",
    "missing_check = df_encoded.isna().sum().sum()\n",
    "if missing_check == 0:\n",
    "    print(\"‚úÖ No missing values remain in feature matrix.\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è {missing_check} missing values still present ‚Äî check source data.\")\n",
    "\n",
    "# --- Assign to feature matrix for clustering ---\n",
    "X = df_encoded.values\n",
    "\n",
    "print(f\"\\n‚úÖ Feature matrix ready for clustering. Shape: {df_encoded.shape}\")\n",
    "\n",
    "# --- Quick preview ---\n",
    "display(df_encoded.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610698bf-5f3d-4739-8d38-971a5d463a8c",
   "metadata": {},
   "source": [
    "## Step 4 ‚Äî K-Means Clustering (Elbow & Silhouette Method)\n",
    "\n",
    "In this step, we apply the K-Means algorithm to identify potential groups (clusters) among the books.\n",
    "\n",
    "We will:\n",
    "1. Run K-Means for different values of *k* (from 2 to 10)\n",
    "2. Record both the **Inertia** (Elbow Method) and the **Silhouette Score**\n",
    "3. Identify the optimal number of clusters (`best_k`) based on the highest silhouette score\n",
    "4. Visualize both metrics side by side for comparison\n",
    "\n",
    "The **Elbow Method** helps detect the point where adding more clusters no longer improves the model significantly,  \n",
    "while the **Silhouette Score** evaluates how well-separated the clusters are (higher values indicate better-defined clusters).\n",
    "\n",
    "Finally, both the visualizations and metrics are saved for later analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd8ee63-9cdc-4a04-bafa-99f05a609747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 4 ‚Äî K-Means Clustering (Elbow & Silhouette Method)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# --- Feature matrix (numeric only) ---\n",
    "X = df_encoded.values\n",
    "\n",
    "# --- Initialize lists ---\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "print(\"Running K-Means for k = 2 to 10...\\n\")\n",
    "\n",
    "# --- Run K-Means across different k values ---\n",
    "for k in K_range:\n",
    "    try:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(X)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "        score = silhouette_score(X, kmeans.labels_)\n",
    "        silhouette_scores.append(score)\n",
    "        print(f\"k={k} ‚Äî Inertia={kmeans.inertia_:.2f}, Silhouette={score:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error for k={k}: {e}\")\n",
    "        inertias.append(np.nan)\n",
    "        silhouette_scores.append(np.nan)\n",
    "\n",
    "# --- Determine best k by silhouette score ---\n",
    "valid_scores = [s for s in silhouette_scores if not np.isnan(s)]\n",
    "best_k = K_range[silhouette_scores.index(max(valid_scores))]\n",
    "\n",
    "# --- Show top 3 silhouette values ---\n",
    "sorted_scores = sorted(zip(K_range, silhouette_scores), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nTop 3 silhouette scores:\")\n",
    "for i, (k_val, s_val) in enumerate(sorted_scores[:3], start=1):\n",
    "    print(f\"{i}. k={k_val} ‚Üí silhouette={s_val:.4f}\")\n",
    "\n",
    "print(f\"\\nBest k by silhouette score: {best_k}\")\n",
    "\n",
    "# ============================================================\n",
    "# Visualization ‚Äî Elbow & Silhouette\n",
    "# ============================================================\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 4))\n",
    "\n",
    "# Elbow Method\n",
    "ax1.plot(K_range, inertias, marker='o', color='steelblue')\n",
    "ax1.set_title(\"Elbow Method ‚Äî K-Means Inertia\", fontsize=11)\n",
    "ax1.set_xlabel(\"Number of Clusters (k)\")\n",
    "ax1.set_ylabel(\"Inertia\")\n",
    "\n",
    "# Silhouette Scores\n",
    "ax2.plot(K_range, silhouette_scores, marker='o', color='orange')\n",
    "ax2.set_title(\"Silhouette Scores by Number of Clusters\", fontsize=11)\n",
    "ax2.set_xlabel(\"Number of Clusters (k)\")\n",
    "ax2.set_ylabel(\"Silhouette Score\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# Save Results\n",
    "# ============================================================\n",
    "\n",
    "viz_path = Path(\"..\") / \"visualizations\"\n",
    "viz_path.mkdir(parents=True, exist_ok=True)\n",
    "fig.savefig(viz_path / \"kmeans_elbow_silhouette_combined.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "metrics_path = Path(\"..\") / \"data\" / \"clean\"\n",
    "metrics_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"k\": list(K_range),\n",
    "    \"inertia\": inertias,\n",
    "    \"silhouette\": silhouette_scores\n",
    "})\n",
    "metrics_df.to_csv(metrics_path / \"kmeans_metrics.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"\\nüíæ Results saved ‚Üí {metrics_path / 'kmeans_metrics.csv'}\")\n",
    "print(f\"üè∑Ô∏è Best number of clusters: {best_k}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707b605c-79de-41e4-bea2-567c0891b1ec",
   "metadata": {},
   "source": [
    "\n",
    "###  Interpretation of Clustering Metrics\n",
    "\n",
    "\n",
    "The Elbow Method shows a sharp drop in inertia from k = 2 to k = 3,\n",
    "after which the curve flattens ‚Äî suggesting that adding more clusters\n",
    "beyond k = 2 provides limited improvement in compactness.\n",
    "\n",
    "The Silhouette Scores confirm that:\n",
    "- k = 2 achieves the highest separation (‚âà 0.75),\n",
    "  meaning clusters are well-defined and internally cohesive.\n",
    "- Higher k values slightly decrease silhouette quality,\n",
    "  indicating overlapping or less distinct groups.\n",
    "\n",
    "üìä **Decision:** We‚Äôll proceed with **k = 2 clusters**, as it offers \n",
    "a strong balance between compactness and separation.\n",
    "\n",
    "üß† **Interpretation:**\n",
    "Each book is represented by numerical and categorical features such as\n",
    "*avg_rating*, *genre*, and *price*.  \n",
    "K-Means groups books with similar characteristics into two main clusters ‚Äî\n",
    "possibly reflecting broad patterns such as\n",
    "‚Äúhigh-rated / premium‚Äù vs. ‚Äúlower-rated / affordable‚Äù categories,\n",
    "which can later inform recommendation logic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3391d8-a49f-4822-8347-b5bde5ee39bb",
   "metadata": {},
   "source": [
    "## Step 5 ‚Äî Apply Final K-Means & Visualize Clusters (PCA 2D)\n",
    "\n",
    "After determining the optimal number of clusters (`k=2`), we apply the final K-Means model to assign each book to a specific cluster.\n",
    "\n",
    "Steps performed:\n",
    "1. Scale all numeric features for consistent distance calculation.\n",
    "2. Train the K-Means model using the selected value of *k*.\n",
    "3. Assign the resulting cluster labels to each book.\n",
    "4. Apply **Principal Component Analysis (PCA)** to reduce the feature space to two dimensions for visualization.\n",
    "5. Plot the resulting clusters using a 2D scatter plot, where each point represents a book and color indicates its cluster.\n",
    "\n",
    "The resulting visualization helps identify distinct groups of books based on their ratings, price levels, and genre characteristics.\n",
    "Both the model output and visualization are saved for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d062a982-1d97-4da2-90de-3fa5db94bc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 5 ‚Äî Apply Final K-Means & Visualize Clusters (PCA 2D)\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# --- Final number of clusters ---\n",
    "k_final = 2\n",
    "print(f\"Applying final K-Means model with k = {k_final}...\\n\")\n",
    "\n",
    "# --- Use scaled numeric features from df_encoded (already standardized) ---\n",
    "X_scaled = df_encoded.values\n",
    "\n",
    "# --- Train K-Means ---\n",
    "kmeans_final = KMeans(n_clusters=k_final, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans_final.fit_predict(X_scaled)\n",
    "\n",
    "# --- Assign clusters safely ---\n",
    "df = df.copy()\n",
    "df[\"cluster\"] = cluster_labels\n",
    "\n",
    "# --- PCA for visualization ---\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "pca_components = pca.fit_transform(X_scaled)\n",
    "df[\"pca_1\"] = pca_components[:, 0]\n",
    "df[\"pca_2\"] = pca_components[:, 1]\n",
    "\n",
    "# ============================================================\n",
    "# Visualization ‚Äî PCA 2D Scatter Plot\n",
    "# ============================================================\n",
    "\n",
    "fig_pca, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=df,\n",
    "    x=\"pca_1\", y=\"pca_2\",\n",
    "    hue=\"cluster\",\n",
    "    palette=\"Set2\",\n",
    "    s=65,\n",
    "    alpha=0.85,\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=0.7,\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_title(f\"Book Clusters ‚Äî PCA 2D Projection (k = {k_final})\", fontsize=13, pad=10)\n",
    "ax.set_xlabel(\"Principal Component 1\")\n",
    "ax.set_ylabel(\"Principal Component 2\")\n",
    "ax.legend(title=\"Cluster\", loc=\"best\", fontsize=9)\n",
    "ax.grid(alpha=0.25, linestyle=\"--\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# Save Plot\n",
    "# ============================================================\n",
    "\n",
    "viz_path = Path(\"..\") / \"visualizations\"\n",
    "viz_path.mkdir(parents=True, exist_ok=True)\n",
    "fig_pca.savefig(viz_path / f\"pca_clusters_k{k_final}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "print(f\"Clustering completed. {df['cluster'].nunique()} clusters created.\")\n",
    "print(f\"PCA cluster visualization saved ‚Üí {viz_path / f'pca_clusters_k{k_final}.png'}\\n\")\n",
    "\n",
    "# --- Preview sample ---\n",
    "display(df[[\"title\", \"author\", \"avg_rating\", \"genre\", \"price\", \"cluster\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa62509-c60f-49fa-9330-f368d3f3c72f",
   "metadata": {},
   "source": [
    "### Cluster Interpretation & Summary\n",
    "\n",
    "### üìä Interpretation of Final Clusters\n",
    "\n",
    "The 2-cluster configuration reveals two distinct groups of books based primarily on their **average rating** and **price level**.  \n",
    "Although genre information wasn‚Äôt directly used in the clustering, it helps describe each cluster‚Äôs general tendencies.\n",
    "\n",
    "---\n",
    "\n",
    "#### üìò **Cluster 0 (Green)**\n",
    "- Contains books with **higher average prices**, sometimes reaching premium levels.  \n",
    "- ‚≠ê Average ratings remain solid (‚âà 4.1), but these titles show greater **variability in pricing**, suggesting a mix of editions or market positions.  \n",
    "- The genre distribution is more diverse, often including **specialized or non-mainstream categories**.\n",
    "\n",
    "---\n",
    "\n",
    "#### üìô **Cluster 1 (Orange)**\n",
    "- Concentrates books with **moderate, consistent prices** (around the dataset‚Äôs median ‚âà 9 EUR).  \n",
    "- ‚≠ê Maintains strong average ratings (‚âà 4.1 ‚Äì 4.3), representing a stable quality baseline.  \n",
    "- This group likely includes **popular, widely accessible titles**, often within **fiction**.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† **Interpretation**\n",
    "\n",
    "The model separates books mainly by **price range** and **rating consistency**:\n",
    "\n",
    "- **Cluster 1** groups books that are **affordable and evenly rated**, suggesting **mainstream popularity**.  \n",
    "- **Cluster 0** includes **higher-priced or more varied titles**, pointing toward **specialized or premium segments**.\n",
    "\n",
    "This segmentation provides a solid foundation for future **recommendation logic** ‚Äî  \n",
    "books within the same cluster share similar *value* and *rating profiles*,  \n",
    "helping guide both **price-sensitive** and **quality-focused** suggestions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7170f744-bce7-4ec8-938c-bf6905a6b626",
   "metadata": {},
   "source": [
    "### Step 5.1 ‚Äî Analyze Cluster Centroids\n",
    "\n",
    "Let‚Äôs inspect the numerical centroids of each cluster to understand\n",
    "how books are grouped ‚Äî for example, by their average rating or price.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d9f7a4-4d57-453f-93aa-1985251ba360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 5.1 ‚Äî Cluster Composition Summary\n",
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "üéØ Step 5.1 ‚Äî Analyze Cluster Composition by Genre\n",
    "This version summarizes how genres distribute within each cluster,\n",
    "while also showing the average rating and price per cluster.\n",
    "\"\"\"\n",
    "\n",
    "# --- Numeric centroids (average rating & price) ---\n",
    "cluster_centroids = (\n",
    "    df.groupby(\"cluster\", as_index=False)\n",
    "    .agg({\n",
    "        \"avg_rating\": \"mean\",\n",
    "        \"price\": \"mean\"\n",
    "    })\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "# --- Genre distribution within each cluster ---\n",
    "genre_distribution = (\n",
    "    df.groupby([\"cluster\", \"genre\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "# --- Calculate proportions per cluster ---\n",
    "cluster_sizes = df[\"cluster\"].value_counts().to_dict()\n",
    "genre_distribution[\"proportion_%\"] = genre_distribution.apply(\n",
    "    lambda row: round((row[\"count\"] / cluster_sizes[row[\"cluster\"]]) * 100, 2),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# --- Merge numeric averages with genre distribution ---\n",
    "cluster_summary = (\n",
    "    genre_distribution.merge(cluster_centroids, on=\"cluster\", how=\"left\")\n",
    "    .sort_values([\"cluster\", \"count\"], ascending=[True, False])\n",
    ")\n",
    "\n",
    "# --- Display top 5 genres per cluster ---\n",
    "print(\"üìä Cluster Composition Summary (Top 5 Genres per Cluster):\\n\")\n",
    "display(cluster_summary.groupby(\"cluster\").head(5))\n",
    "\n",
    "print(\"\\nüß≠ Interpretation Guide:\")\n",
    "print(\"- avg_rating ‚Üí Average rating within the cluster.\")\n",
    "print(\"- price ‚Üí Mean price (EUR) within the cluster.\")\n",
    "print(\"- count ‚Üí Number of books belonging to that genre in the cluster.\")\n",
    "print(\"- proportion_% ‚Üí Relative share (%) of that genre within the cluster.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e422c0-6a71-4e22-8e49-fbd29c995ae5",
   "metadata": {},
   "source": [
    "### üìö Interpretation ‚Äî Cluster Composition by Genre\n",
    "\n",
    "The genre distribution confirms that **Cluster 0** represents the mainstream market:\n",
    "books with average prices around ‚Ç¨9 and high reader satisfaction (‚âà 4.1 ‚òÖ),\n",
    "mostly from **fiction-related categories**.\n",
    "\n",
    "**Cluster 1**, in contrast, groups a few high-priced titles (‚âà ‚Ç¨43),\n",
    "covering specialized or collector genres such as *Art* or *Literary Criticism*.\n",
    "These books maintain high ratings, suggesting that higher price is associated\n",
    "with niche appeal or premium editions rather than lower quality.\n",
    "\n",
    "Overall, while genre was not part of the clustering features,\n",
    "its distribution provides valuable context:\n",
    "the clusters reflect **economic segmentation** in the book market,\n",
    "with Fiction dominating the accessible range and rare genres defining the premium range.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db1fc3c-0a01-4b19-9108-35d25c40e2c4",
   "metadata": {},
   "source": [
    "## Step 6 ‚Äî Export Final Clustered Dataset\n",
    "\n",
    "We‚Äôll now export the final dataset including the cluster labels (`cluster`)\n",
    "and PCA coordinates (`pca_1`, `pca_2`) for visualization and further analysis.  \n",
    "This dataset can be used in **Tableau**, **Power BI**, or in the next notebook\n",
    "for building the **Recommendation System**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c46bb7-212b-40f0-8de4-899ddd78593a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 6 ‚Äî Export Final Clustered Dataset & Cluster Summary\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# --- Define export paths ---\n",
    "data_clean_path = Path(\"..\") / config[\"paths\"][\"data_clean\"]\n",
    "viz_path = Path(\"..\") / \"visualizations\"\n",
    "data_clean_path.mkdir(parents=True, exist_ok=True)\n",
    "viz_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Export final dataset ---\n",
    "final_cluster_path = data_clean_path / \"books_clustered_final.csv\"\n",
    "\n",
    "export_cols = [\n",
    "    \"title\", \"author\", \"avg_rating\", \"genre\", \"price\", \"currency\",\n",
    "    \"cover_url\", \"link\", \"cluster\", \"pca_1\", \"pca_2\"\n",
    "]\n",
    "\n",
    "df[export_cols].to_csv(final_cluster_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"üíæ Final clustered dataset saved successfully ‚Üí {final_cluster_path.resolve()}\")\n",
    "\n",
    "print(\"\\nüìò Sample of exported dataset:\")\n",
    "display(df[export_cols].head(10))\n",
    "\n",
    "# ============================================================\n",
    "# üìä Cluster Summary (improved: true genre distribution)\n",
    "# ============================================================\n",
    "\n",
    "# --- Numeric stats ---\n",
    "cluster_centroids = (\n",
    "    df.groupby(\"cluster\")\n",
    "    .agg({\n",
    "        \"avg_rating\": \"mean\",\n",
    "        \"price\": \"mean\"\n",
    "    })\n",
    "    .round(2)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# --- Genre distribution ---\n",
    "genre_distribution = (\n",
    "    df.groupby([\"cluster\", \"genre\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "# --- Add proportions per cluster ---\n",
    "cluster_sizes = df[\"cluster\"].value_counts().to_dict()\n",
    "genre_distribution[\"proportion_%\"] = genre_distribution.apply(\n",
    "    lambda row: round((row[\"count\"] / cluster_sizes[row[\"cluster\"]]) * 100, 2),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# --- Merge numeric stats with genre composition ---\n",
    "cluster_summary = genre_distribution.merge(cluster_centroids, on=\"cluster\", how=\"left\")\n",
    "\n",
    "# --- Sort by cluster and descending count ---\n",
    "cluster_summary = cluster_summary.sort_values([\"cluster\", \"count\"], ascending=[True, False])\n",
    "\n",
    "print(\"\\nüìó Cluster Summary (Top Genres per Cluster):\")\n",
    "display(cluster_summary.groupby(\"cluster\").head(5))\n",
    "\n",
    "print(f\"\\nüíæ Cluster summary saved ‚Üí {data_clean_path / 'cluster_summary.csv'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf3f9d1-96b0-40c9-a3bd-e3dcfbc85d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book_recommender (uv)",
   "language": "python",
   "name": "book-recommendation-system"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
