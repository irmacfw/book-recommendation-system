{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c56f74c-ca49-40ca-8e62-665d8bbf9095",
   "metadata": {},
   "source": [
    "# Notebook 01 â€” Web Scraping Goodreads\n",
    "\n",
    "In this notebook, we will collect data from a public Goodreads list using web scraping.  \n",
    "We will extract the **title**, **author**, **rating**, and **link** of each book from the list  \n",
    "*\"Best Books Ever\"* as a starting point for our dataset.\n",
    "\n",
    "Additionally, we will extend the scraping process to capture complementary information such as  \n",
    "**genre**, **price**, **number of reviews**, **published year**, and **book cover**, \n",
    "which will enrich the dataset for later analysis and recommendations.\n",
    "\n",
    "## Step 1 â€” Imports & Setup\n",
    "\n",
    "In this step, we import all the required libraries and project functions.\n",
    "We also load the `config.yaml` file from the root directory to manage all\n",
    "file paths dynamically (for raw, clean, and other data folders).\n",
    "\n",
    "The `functions.py` module, located inside the `notebooks` folder, contains\n",
    "reusable utilities for web scraping, API fetching, and dataset handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7793d5-f1c4-45c8-aa01-ec97b9e09018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 1 â€” Imports & Setup (safe simple version)\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "import random\n",
    "import importlib\n",
    "\n",
    "sys.path.append(\"notebooks\")\n",
    "\n",
    "from functions import (\n",
    "    load_config,\n",
    "    ensure_directories,\n",
    "    fetch_html,\n",
    "    save_html,\n",
    "    parse_html,\n",
    "    extract_books_from_soup,\n",
    "    save_dataset\n",
    ")\n",
    "import functions\n",
    "importlib.reload(functions)\n",
    "\n",
    "# --- Load config directly from root ---\n",
    "config_path = Path(\"../config.yaml\")\n",
    "config = load_config(config_path)\n",
    "\n",
    "# --- Ensure directories once ---\n",
    "ensure_directories(config[\"paths\"])\n",
    "\n",
    "print(\"âœ… All functions loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18832d29-82bc-4626-8eb7-156108008697",
   "metadata": {},
   "source": [
    "## Step 2 â€” Define URL & Scrape Goodreads List\n",
    "\n",
    "In this step, we will define the Goodreads list URL and send a request to fetch its HTML content.  \n",
    "We will then parse the page using BeautifulSoup to extract the book data (title, author, rating, link, and other details like genre, price, and published year).  \n",
    "Finally, we will save the raw HTML file in the `data/raw` folder for future reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625fd649-bad3-437d-916f-e07f38b99250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 2 â€” Define URL & Scrape Goodreads List\n",
    "# ============================================================\n",
    "\n",
    "from functions import fetch_html, save_html, parse_html\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Define URL ---\n",
    "url = \"https://www.goodreads.com/list/show/1.Best_Books_Ever\"\n",
    "\n",
    "# --- Fetch & parse HTML ---\n",
    "html_content = fetch_html(url)\n",
    "soup = parse_html(html_content)\n",
    "\n",
    "# --- Save raw HTML ---\n",
    "raw_html_path = Path(config[\"paths\"][\"data_raw\"]) / \"goodreads_best_books.html\"\n",
    "save_html(soup.prettify(), raw_html_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b3f302-2a1b-4593-b71d-0077dcb29061",
   "metadata": {},
   "source": [
    "## Step 3 â€” Parse HTML & Extract Book Information\n",
    "\n",
    "In this step, we will parse the raw HTML file previously saved from the Goodreads list and extract all relevant book information.  \n",
    "Using BeautifulSoup, we will identify and capture key fields such as:\n",
    "\n",
    "- **Title**  \n",
    "- **Author**  \n",
    "- **Rating**  \n",
    "- **Book link**  \n",
    "- **Cover image (URL)**  \n",
    "- **Genre**  \n",
    "- **Price**  \n",
    "- **Published year**\n",
    "\n",
    "The extracted data will be stored in a structured pandas DataFrame, which will later be cleaned and saved in the `data/processed/` folder for further analysis and recommendation modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be377aa3-76d1-4a7d-8d76-7c15775b65db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 3 â€” Parse HTML & Extract Book Information\n",
    "# ============================================================\n",
    "\n",
    "# Define the target URL for the first page\n",
    "url = \"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=1\"\n",
    "\n",
    "print(f\" Fetching URL: {url}\")\n",
    "html = fetch_html(url)\n",
    "\n",
    "if html is None:\n",
    "    print(\" Could not fetch HTML. Please check the URL or your connection.\")\n",
    "else:\n",
    "    print(\" HTML fetched successfully!\")\n",
    "\n",
    "# --- Parse HTML ---\n",
    "soup = parse_html(html)\n",
    "\n",
    "# --- Extract book information into a DataFrame ---\n",
    "df_page1 = extract_books_from_soup(soup)\n",
    "\n",
    "print(f\"âœ… Extracted {len(df_page1)} books into DataFrame\")\n",
    "df_page1.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca67007-163a-4fa2-af67-74503b8e724b",
   "metadata": {},
   "source": [
    "### Step 3.1 â€” Scrape Multiple Pages (~500 books)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c909048-63e5-4482-9340-6782d8ecfe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 3.1 â€” Scrape Multiple Pages from Goodreads\n",
    "# ============================================================\n",
    "\n",
    "print(\"ðŸ”¹ Starting multi-page scraping...\")\n",
    "\n",
    "all_books = []\n",
    "base_url = \"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=\"\n",
    "\n",
    "for page in range(1, 6):  # scrape first 5 pages â‰ˆ 500 books\n",
    "    print(f\"ðŸ”¹ Scraping page {page}...\")\n",
    "    url = base_url + str(page)\n",
    "    html = fetch_html(url)\n",
    "    \n",
    "    if html is None:\n",
    "        print(f\"âš ï¸ Skipping page {page}, empty response.\")\n",
    "        continue\n",
    "\n",
    "    soup = parse_html(html)\n",
    "    books_on_page = extract_books_from_soup(soup)\n",
    "\n",
    "    # â¬‡ï¸ CHANGED: append DataFrame instead of extend\n",
    "    all_books.append(books_on_page)\n",
    "\n",
    "    # ethical delay\n",
    "    sleep(random.uniform(1, 2))\n",
    "\n",
    "print(f\"âœ… Scraping completed for {len(all_books)} pages.\")\n",
    "\n",
    "# Combine all DataFrames\n",
    "import pandas as pd\n",
    "df = pd.concat(all_books, ignore_index=True)\n",
    "print(f\"âœ… Combined dataset shape: {df.shape}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74baba5d-619c-4060-a7e8-d2aae6caea9d",
   "metadata": {},
   "source": [
    "### Step 3.2 â€” Enrich Goodreads Data with Google Books API\n",
    "\n",
    "In this step, we will use the **Google Books API** to fill in missing metadata for our 500 scraped books.  \n",
    "For each book title and author, we will request additional information such as:\n",
    "- Published year  \n",
    "- Genre (category)  \n",
    "- Cover image URL  \n",
    "\n",
    "This will enrich our dataset before the cleaning and modeling steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d61ff4-d4ce-4013-851b-a9873924aefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 3.2 â€” Enrich Goodreads Data with Google Books API\n",
    "# ============================================================\n",
    "\n",
    "import requests\n",
    "from tqdm import tqdm  # progress bar\n",
    "\n",
    "def get_book_info_from_google(title, author):\n",
    "    \"\"\"Query Google Books API and return metadata for a given title + author.\"\"\"\n",
    "    query = f\"intitle:{title}+inauthor:{author}\"\n",
    "    url = f\"https://www.googleapis.com/books/v1/volumes?q={query}\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if \"items\" in data and len(data[\"items\"]) > 0:\n",
    "                info = data[\"items\"][0][\"volumeInfo\"]\n",
    "                return {\n",
    "                    \"published_year\": info.get(\"publishedDate\", None),\n",
    "                    \"genre\": \", \".join(info.get(\"categories\", [])) if info.get(\"categories\") else None,\n",
    "                    \"cover_url\": info.get(\"imageLinks\", {}).get(\"thumbnail\", None)\n",
    "                }\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error fetching '{title}': {e}\")\n",
    "    \n",
    "    return {\"published_year\": None, \"genre\": None, \"cover_url\": None}\n",
    "\n",
    "\n",
    "# --- Apply API to all rows ---\n",
    "print(\"ðŸ”¹ Enriching dataset with Google Books API...\")\n",
    "\n",
    "results = []\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    meta = get_book_info_from_google(row[\"title\"], row[\"author\"])\n",
    "    results.append(meta)\n",
    "\n",
    "# Convert to DataFrame and merge\n",
    "api_df = pd.DataFrame(results)\n",
    "df_enriched = pd.concat([df.reset_index(drop=True), api_df], axis=1)\n",
    "\n",
    "print(f\"âœ… Enrichment completed! Final shape: {df_enriched.shape}\")\n",
    "df_enriched.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50deb51-d10e-42ff-a13c-16cdb2354383",
   "metadata": {},
   "source": [
    "## Step 3.3 â€” Enrich Goodreads Data with Price Information (Google Books API)\n",
    "\n",
    "In this step, we will use the Google Books API again to retrieve price information \n",
    "(`listPrice` or `retailPrice`) for each book when available.  \n",
    "This field is not available on Goodreads, so this enrichment completes our dataset \n",
    "with commercial metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4cc1f0-5d33-48cb-89a0-dd810fc68653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 3.3 â€” Enrich Goodreads Data with Price Information (Google Books API)\n",
    "# ============================================================\n",
    "\n",
    "import time\n",
    "def get_price_from_google(title, author):\n",
    "    \"\"\"Query Google Books API for price info (listPrice or retailPrice).\"\"\"\n",
    "    query = f\"intitle:{title}+inauthor:{author}\"\n",
    "    url = f\"https://www.googleapis.com/books/v1/volumes?q={query}\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if \"items\" in data:\n",
    "                info = data[\"items\"][0].get(\"saleInfo\", {})\n",
    "                price_info = info.get(\"listPrice\", {}) or info.get(\"retailPrice\", {})\n",
    "                if price_info:\n",
    "                    return price_info.get(\"amount\"), price_info.get(\"currencyCode\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error fetching '{title}': {e}\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# --- Apply to dataset ---\n",
    "prices = []\n",
    "for _, row in tqdm(df_enriched.iterrows(), total=len(df_enriched)):\n",
    "    price, currency = get_price_from_google(row[\"title\"], row[\"author\"])\n",
    "    prices.append({\"price\": price, \"currency\": currency})\n",
    "    time.sleep(1.5)  # ethical delay\n",
    "\n",
    "# --- Combine with original DataFrame ---\n",
    "df_prices = pd.DataFrame(prices)\n",
    "df_enriched_prices = pd.concat([df_enriched.reset_index(drop=True), df_prices], axis=1)\n",
    "\n",
    "print(f\"âœ… Price enrichment completed: {len(df_enriched_prices)} books processed.\")\n",
    "df_enriched_prices.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc31682-2835-40cf-a449-ca1ce69d8f6d",
   "metadata": {},
   "source": [
    "# Step 4 â€” Final Cleaning & Save Dataset (with prices)\n",
    "\n",
    "In this step, we perform the final data cleaning and consolidation of all book information\n",
    "gathered from both sources:\n",
    "\n",
    "- **Step 3.2:** Google Books API enrichment (adds `genre`, `cover_url`, `published_year`)\n",
    "- **Step 3.3:** Price API enrichment (adds `price`, `currency`)\n",
    "\n",
    "The goal is to standardize all fields, remove duplicates, and ensure the dataset\n",
    "is ready for exploratory analysis and model training.\n",
    "\n",
    "We will:\n",
    "- Normalize text columns (`title`, `author`, `genre`, `currency`)\n",
    "- Extract only the numeric rating values\n",
    "- Parse and extract the publication year from full dates\n",
    "- Convert `price` to numeric values\n",
    "- Remove duplicates and missing titles/authors\n",
    "- Keep only the relevant columns for analysis\n",
    "- Save the final dataset as `books_clean.csv` inside the `data/clean/` folder\n",
    "\n",
    "This cleaned dataset will be used for EDA (Step 5) and later integrated with additional\n",
    "data from the Open Library API to build a complete Book Recommendation System.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5216fe4-f5c0-4d19-aa97-603e704c1f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 4 â€” Final Cleaning & Save Dataset (with prices)\n",
    "# ============================================================\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# --- Ensure the right DataFrame is loaded ---\n",
    "# If the kernel was restarted, reload from backup or enriched variable\n",
    "if \"df_enriched_prices\" not in locals():\n",
    "    clean_folder = Path(config[\"paths\"][\"data_clean\"])\n",
    "    enriched_path = clean_folder / \"books_enriched_with_prices.csv\"\n",
    "\n",
    "    if enriched_path.exists():\n",
    "        df_enriched_prices = pd.read_csv(enriched_path)\n",
    "        print(f\"ðŸ”„ Reloaded enriched data from: {enriched_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"âŒ Missing df_enriched_prices in memory and CSV not found. Please re-run Step 3.3 once.\")\n",
    "\n",
    "# --- Load or copy from the enriched dataset (3.2 + 3.3 combined) ---\n",
    "df_clean = df_enriched_prices.copy()\n",
    "df_clean = df_clean.loc[:, ~df_clean.columns.duplicated(keep=\"last\")]\n",
    "\n",
    "# --- Clean text fields safely ---\n",
    "for col in [\"title\", \"author\", \"genre\", \"currency\"]:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = df_clean[col].astype(str).apply(\n",
    "            lambda x: x.strip() if isinstance(x, str) else x\n",
    "        )\n",
    "\n",
    "# --- Extract avg_rating as numeric ---\n",
    "if \"rating\" in df_clean.columns:\n",
    "    df_clean[\"avg_rating\"] = (\n",
    "        df_clean[\"rating\"].astype(str).str.extract(r\"(\\d+\\.\\d+)\")[0].astype(float)\n",
    "    )\n",
    "\n",
    "# --- Extract only year from published_year ---\n",
    "def extract_year(date_str):\n",
    "    if pd.isna(date_str):\n",
    "        return None\n",
    "    match = re.match(r\"(\\d{4})\", str(date_str))\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "if \"published_year\" in df_clean.columns:\n",
    "    df_clean[\"published_year\"] = df_clean[\"published_year\"].apply(extract_year)\n",
    "\n",
    "# --- Convert price column to numeric ---\n",
    "if \"price\" in df_clean.columns:\n",
    "    df_clean[\"price\"] = pd.to_numeric(df_clean[\"price\"], errors=\"coerce\")\n",
    "\n",
    "# --- Drop duplicates and missing titles/authors ---\n",
    "df_clean = df_clean.drop_duplicates(subset=[\"title\"]).dropna(subset=[\"title\", \"author\"])\n",
    "\n",
    "# --- Keep only relevant columns ---\n",
    "keep_cols = [\n",
    "    \"title\",\n",
    "    \"author\",\n",
    "    \"avg_rating\",\n",
    "    \"genre\",\n",
    "    \"published_year\",\n",
    "    \"price\",\n",
    "    \"currency\",\n",
    "    \"cover_url\",\n",
    "    \"link\",\n",
    "]\n",
    "df_clean = df_clean[[col for col in keep_cols if col in df_clean.columns]]\n",
    "\n",
    "# --- Save cleaned dataset ---\n",
    "output_path = Path(config[\"paths\"][\"data_clean\"]) / \"books_clean.csv\"\n",
    "save_dataset(df_clean, output_path)\n",
    "\n",
    "# âœ… Also save a backup\n",
    "backup_path = Path(config[\"paths\"][\"data_clean\"]) / \"books_clean_backup.csv\"\n",
    "df_clean.to_csv(backup_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"âœ… Final cleaned dataset saved successfully: {output_path}\")\n",
    "print(f\"ðŸ’¾ Backup saved at: {backup_path}\")\n",
    "print(f\"Rows: {len(df_clean)}, Columns: {len(df_clean.columns)}\")\n",
    "\n",
    "# --- Preview ---\n",
    "df_clean.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328e35de-b2c4-4721-8ffd-f3cf0388c4e5",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Step 5 â€” Exploratory Data Analysis (EDA)\n",
    "\n",
    "In this step, we performed an initial exploratory analysis of the cleaned dataset to understand its structure, completeness, and key characteristics.  \n",
    "We examined rating distributions, author frequency, and potential missing values, which will guide the next steps in data enrichment and modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf28b0a-9e14-4d7d-9498-81083d9ba65f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 5 â€” Exploratory Data Analysis (EDA)\n",
    "# ============================================================\n",
    "\n",
    "import importlib, functions\n",
    "importlib.reload(functions)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Load cleaned dataset ---\n",
    "clean_path = Path(config[\"paths\"][\"data_clean\"]) / \"books_clean.csv\"\n",
    "df = pd.read_csv(clean_path)\n",
    "print(f\"âœ… Dataset loaded: {clean_path}\")\n",
    "print(df.shape)\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# --- Quick overview ---\n",
    "display(df.head())\n",
    "display(df.describe(include='all'))\n",
    "\n",
    "# --- Check for missing values ---\n",
    "print(\"\\nðŸ” Missing values per column:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”Ž Quick Summary before Visual Analysis\n",
    "# ============================================================\n",
    "\n",
    "total_books = len(df)\n",
    "unique_authors = df[\"author\"].nunique()\n",
    "avg_rating_mean = df[\"avg_rating\"].mean()\n",
    "top_genre = df[\"genre\"].mode()[0] if not df[\"genre\"].mode().empty else \"N/A\"\n",
    "most_common_year = int(df[\"published_year\"].mode()[0]) if not df[\"published_year\"].mode().empty else \"N/A\"\n",
    "\n",
    "# --- Price metrics (new) ---\n",
    "books_with_price = df[\"price\"].notna().sum() if \"price\" in df.columns else 0\n",
    "avg_price = df[\"price\"].mean() if \"price\" in df.columns else None\n",
    "currency_mode = df[\"currency\"].mode()[0] if \"currency\" in df.columns and not df[\"currency\"].mode().empty else \"N/A\"\n",
    "\n",
    "print(\"\\nðŸ“˜ --- Quick Summary ---\")\n",
    "print(f\"Total books in dataset: {total_books}\")\n",
    "print(f\"Unique authors: {unique_authors}\")\n",
    "print(f\"Average book rating: {avg_rating_mean:.2f}\")\n",
    "print(f\"Most common genre: {top_genre}\")\n",
    "print(f\"Most common publication year: {most_common_year}\")\n",
    "print(f\"Books with price info: {books_with_price} ({books_with_price/total_books*100:.1f}%)\")\n",
    "if avg_price:\n",
    "    print(f\"Average price: {avg_price:.2f} {currency_mode}\")\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ“Š EDA Visualizations\n",
    "# ============================================================\n",
    "\n",
    "sns.set(style=\"whitegrid\", palette=\"crest\")\n",
    "\n",
    "# --- Plot 1: Distribution of average ratings ---\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(df[\"avg_rating\"], bins=15, kde=True)\n",
    "plt.title(\"Distribution of Average Book Ratings\", fontsize=13)\n",
    "plt.xlabel(\"Average Rating\")\n",
    "plt.ylabel(\"Number of Books\")\n",
    "plt.show()\n",
    "\n",
    "# --- Plot 2: Top 10 Authors by Number of Books ---\n",
    "top_authors = (\n",
    "    df[\"author\"]\n",
    "    .value_counts()\n",
    "    .head(10)\n",
    "    .sort_values(ascending=True)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "top_authors.plot(kind=\"barh\", color=\"mediumseagreen\")\n",
    "plt.title(\"Top 10 Authors by Number of Books\", fontsize=13)\n",
    "plt.xlabel(\"Number of Books\")\n",
    "plt.ylabel(\"Author\")\n",
    "plt.show()\n",
    "\n",
    "# --- Plot 3: Books by Published Year ---\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(\n",
    "    y=\"published_year\",\n",
    "    data=df,\n",
    "    hue=\"published_year\",\n",
    "    legend=False,\n",
    "    dodge=False,\n",
    "    order=df[\"published_year\"].value_counts().index[:15],\n",
    "    palette=\"viridis\"\n",
    ")\n",
    "plt.title(\"Books by Published Year\", fontsize=13)\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Year\")\n",
    "plt.show()\n",
    "\n",
    "# --- Plot 4: Top 10 Genres ---\n",
    "top_genres = (\n",
    "    df[\"genre\"]\n",
    "    .value_counts()\n",
    "    .head(10)\n",
    "    .sort_values(ascending=True)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "top_genres.plot(kind=\"barh\", color=\"cornflowerblue\")\n",
    "plt.title(\"Top 10 Genres\", fontsize=13)\n",
    "plt.xlabel(\"Number of Books\")\n",
    "plt.ylabel(\"Genre\")\n",
    "plt.show()\n",
    "\n",
    "# --- Plot 5: Price Distribution (new) ---\n",
    "if \"price\" in df.columns and df[\"price\"].notna().sum() > 0:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.histplot(df[\"price\"], bins=20, kde=True, color=\"darkorange\")\n",
    "    plt.title(\"Distribution of Book Prices\", fontsize=13)\n",
    "    plt.xlabel(\"Price\")\n",
    "    plt.ylabel(\"Number of Books\")\n",
    "    plt.show()\n",
    "\n",
    "# --- Plot 6: Average Price by Genre (new) ---\n",
    "if \"price\" in df.columns and \"genre\" in df.columns:\n",
    "    genre_prices = df.groupby(\"genre\")[\"price\"].mean().sort_values(ascending=True).head(10)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    genre_prices.plot(kind=\"barh\", color=\"goldenrod\")\n",
    "    plt.title(\"Average Price by Genre\", fontsize=13)\n",
    "    plt.xlabel(\"Average Price\")\n",
    "    plt.ylabel(\"Genre\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b3c554-3f4b-4e72-af91-f934a4674e82",
   "metadata": {},
   "source": [
    "### âœ… Step 5 Summary\n",
    "\n",
    "The dataset contains **493 books** from **330 unique authors**, primarily in the *Fiction* genre.  \n",
    "Average ratings hover around **4.1**, indicating that most books are highly rated and well-reviewed by readers.\n",
    "\n",
    "Roughly **46% of the books include price information**, with an average price of **9.55 EUR** â€”  \n",
    "a typical range for e-books or paperback editions. Prices are right-skewed, meaning that  \n",
    "most books are affordable, while only a few premium titles are priced above 40 EUR.\n",
    "\n",
    "Only a few missing values remain in non-critical columns (`genre`, `cover_url`, `price`),  \n",
    "which will not significantly impact the recommendation model.\n",
    "\n",
    "This cleaned and analyzed dataset is now ready to be used in the next phase:  \n",
    "building a **content-based recommendation system** using **TF-IDF vectorization** and **cosine similarity**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429959b6-ae6b-4e3a-b19a-7cac8804945d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book_recommender (uv)",
   "language": "python",
   "name": "book-recommendation-system"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
