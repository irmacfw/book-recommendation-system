





# ============================================================
# Step 1 ‚Äî Imports & Setup 
# ============================================================

import sys
from pathlib import Path
import importlib

# --- Access shared functions ---
sys.path.append("notebooks")
from functions import load_config, ensure_directories
import functions
importlib.reload(functions)

# --- Additional libraries for ML and visualization ---
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# --- Load configuration from root ---
config_path = Path("..") / "config.yaml"
config = load_config(config_path)

# --- Verify folders ---
ensure_directories(config["paths"])

print("‚úÖ Environment ready ‚Äî config loaded and directories verified.")






# ============================================================
# Step 2 ‚Äî Load Final Dataset
# ============================================================

import pandas as pd
from pathlib import Path

# --- Load dataset from data/clean ---
data_path = Path("..") / config["paths"]["data_clean"] / "books_final_1000.csv"
df = pd.read_csv(data_path)

print(f"‚úÖ Dataset loaded successfully: {data_path}")
print(f"Shape: {df.shape}\n")

# --- Quick overview ---
display(df.head(5))

# --- Basic info and types ---
print("\nüîç DataFrame Info:")
print(df.info())

# --- Missing values summary ---
missing_summary = df.isna().sum()
missing_summary = missing_summary[missing_summary > 0]

if not missing_summary.empty:
    print("\n‚ö†Ô∏è Missing values summary:")
    print(missing_summary)
else:
    print("\n‚úÖ No missing values detected.")






# ============================================================
# Step 3 ‚Äî Feature Preparation
# ============================================================

from sklearn.preprocessing import StandardScaler

# --- Select relevant columns ---
features = ["avg_rating", "price", "genre"]
df_features = df[features].copy()

# --- Handle missing values ---
# Fill missing prices with median (robust against outliers)
median_price = df_features["price"].median()
df_features["price"] = df_features["price"].fillna(median_price)

print(f"üí∞ Filled missing 'price' values with median: {median_price:.2f}")

# --- One-Hot Encode 'genre' ---
df_encoded = pd.get_dummies(df_features, columns=["genre"], drop_first=True)

# --- Standardize numeric columns ---
scaler = StandardScaler()
numeric_cols = ["avg_rating", "price"]
df_encoded[numeric_cols] = scaler.fit_transform(df_encoded[numeric_cols])

# Ensure missing prices are filled before clustering
median_price = df["price"].median()
df["price"] = df["price"].fillna(median_price)

print(f"‚úÖ Missing prices filled with median: {median_price}")


print(f"‚úÖ Feature matrix ready for clustering. Shape: {df_encoded.shape}")

# --- Quick preview ---
display(df_encoded.head(5))






# ============================================================
# Step 4 ‚Äî K-Means Clustering (Elbow & Silhouette Method)
# ============================================================

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

X = df_encoded.copy()

inertias = []
silhouette_scores = []
K_range = range(2, 11)  # test k between 2 and 10

print("üîπ Running K-Means for k = 2 to 10...\n")

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X)
    inertias.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(X, kmeans.labels_))

print("‚úÖ K-Means training completed.")

# --- Plot Elbow Method ---
plt.figure(figsize=(7,4))
plt.plot(K_range, inertias, marker='o')
plt.title("Elbow Method ‚Äî K-Means Inertia")
plt.xlabel("Number of Clusters (k)")
plt.ylabel("Inertia")
plt.grid(True)
plt.show()

# --- Plot Silhouette Scores ---
plt.figure(figsize=(7,4))
plt.plot(K_range, silhouette_scores, marker='o', color='orange')
plt.title("Silhouette Scores by Number of Clusters")
plt.xlabel("Number of Clusters (k)")
plt.ylabel("Silhouette Score")
plt.grid(True)
plt.show()









# ============================================================
# Step 5 ‚Äî Apply Final K-Means & Visualize Clusters (Enhanced)
# ============================================================

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

# --- Final number of clusters ---
k_final = 3
print(f"üè∑Ô∏è Applying final K-Means model with k = {k_final}...\n")

# --- Scale features ---
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_encoded)

# --- Train K-Means ---
kmeans_final = KMeans(n_clusters=k_final, random_state=42, n_init=10)
df["cluster"] = kmeans_final.fit_predict(X_scaled)

# --- PCA for visualization ---
pca = PCA(n_components=2, random_state=42)
pca_components = pca.fit_transform(X_scaled)
df["pca_1"] = pca_components[:, 0]
df["pca_2"] = pca_components[:, 1]

# --- Enhanced Visualization ---
plt.figure(figsize=(8,6))
sns.scatterplot(
    data=df,
    x="pca_1", y="pca_2",
    hue="cluster",
    palette="Set2",
    s=60,
    alpha=0.8,
    edgecolor="white",
    linewidth=0.7
)
plt.title("Book Clusters ‚Äî PCA 2D Projection (k = 3)", fontsize=13, pad=10)
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.legend(title="Cluster", loc="upper right")
plt.grid(alpha=0.3, linestyle="--")
plt.show()

print(f"‚úÖ Clustering completed. {df['cluster'].nunique()} clusters created.\n")
display(df[["title", "author", "avg_rating", "genre", "price", "cluster"]].head(10))






# ============================================================
# Step 5.1 ‚Äî Analyze Cluster Centroids
# ============================================================

cluster_centroids = (
    df.groupby("cluster")
    .agg({
        "avg_rating": "mean",
        "price": "mean",
        "genre": lambda x: x.mode().iloc[0] if not x.mode().empty else "Unknown"
    })
    .round(2)
)

print(" Cluster Centroids Summary:\n")
display(cluster_centroids)

print("\n Interpretation Guide:")
print("- avg_rating ‚Üí Average book rating per cluster.")
print("- price ‚Üí Mean price, useful to detect premium vs. budget titles.")
print("- genre ‚Üí Most common genre in each cluster.")









# ============================================================
# Step 7 ‚Äî Export Final Clustered Dataset
# ============================================================

from pathlib import Path

# --- Define export path ---
final_cluster_path = Path("..") / config["paths"]["data_clean"] / "books_clustered_final.csv"

# --- Select relevant columns ---
export_cols = [
    "title",
    "author",
    "avg_rating",
    "genre",
    "price",
    "currency",
    "cover_url",
    "link",
    "cluster",
    "pca_1",
    "pca_2"
]

# --- Save dataset ---
df[export_cols].to_csv(final_cluster_path, index=False, encoding="utf-8-sig")
print(f"üíæ Final clustered dataset saved successfully ‚Üí {final_cluster_path.resolve()}")

# --- Quick preview ---
print("\nüìò Sample of exported dataset:")
display(df[export_cols].head(10))




