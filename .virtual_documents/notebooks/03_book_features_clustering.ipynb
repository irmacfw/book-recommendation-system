





# ============================================================
# Step 1 â€” Imports & Setup 
# ============================================================

# --- System and project setup ---
import sys
from pathlib import Path

# Add 'notebooks' folder to path (functions.py lives there)
sys.path.append("notebooks")

# --- Load shared utilities ---
from functions import load_config, ensure_directories

# --- ML and visualization libraries ---
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# --- Visualization settings (como en clase) ---
sns.set(style="whitegrid", palette="muted")
plt.rcParams["figure.figsize"] = (8, 5)

# --- Load configuration and verify folders ---
config_path = Path("..") / "config.yaml"
config = load_config(config_path)
ensure_directories(config["paths"])

print("âœ… Environment ready â€” config loaded and directories verified.")






# ============================================================
# Step 2 â€” Load Final Dataset
# ============================================================

import pandas as pd
from pathlib import Path

# --- Load dataset from data/clean ---
data_path = Path("..") / config["paths"]["data_clean"] / "books_final_1000.csv"
df = pd.read_csv(data_path)

print(f"âœ… Dataset loaded successfully: {data_path}")
print(f"Shape: {df.shape}\n")

# --- Quick overview ---
display(df.head(5))

# --- Basic info and types ---
print("\nğŸ” DataFrame Info:")
print(df.info())

# --- Missing values summary ---
missing_summary = df.isna().sum()
missing_summary = missing_summary[missing_summary > 0]

if not missing_summary.empty:
    print("\nâš ï¸ Missing values summary:")
    print(missing_summary)
else:
    print("\nâœ… No missing values detected.")

# --- Optional: Unique values check (for categorical columns) ---
print("\nğŸ§© Unique values per column:")
print(df.nunique())






# ============================================================
# Step 3 â€” Feature Preparation (Fix Missing Price Fill)
# ============================================================

from sklearn.preprocessing import StandardScaler
import pandas as pd

# --- Select relevant columns ---
features = ["avg_rating", "price", "genre"]
df_features = df[features].copy()

# --- Handle missing prices ---
median_price = df_features["price"].median()
df_features["price"] = df_features["price"].fillna(median_price)

# --- Reflect the filled prices back into df ---
df["price"] = df["price"].fillna(median_price)
print(f"Filled missing 'price' values with median: {median_price:.2f}")

# --- One-Hot Encode 'genre' ---
df_encoded = pd.get_dummies(df_features, columns=["genre"], drop_first=True)
print(f"Encoded dataset has {df_encoded.shape[1]} columns after one-hot encoding.")

# --- Standardize numeric columns ---
scaler = StandardScaler()
numeric_cols = ["avg_rating", "price"]
df_encoded[numeric_cols] = scaler.fit_transform(df_encoded[numeric_cols])
print("ğŸ“ Numeric columns standardized (avg_rating, price).")

# --- Optional sanity check ---
missing_check = df_encoded.isna().sum().sum()
if missing_check == 0:
    print("âœ… No missing values remain in feature matrix.")
else:
    print(f"âš ï¸ {missing_check} missing values still present â€” check source data.")

# --- Assign to feature matrix for clustering ---
X = df_encoded.values

print(f"\nâœ… Feature matrix ready for clustering. Shape: {df_encoded.shape}")

# --- Quick preview ---
display(df_encoded.head(5))






# ============================================================
# Step 4 â€” K-Means Clustering (Elbow & Silhouette Method)
# ============================================================

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
from pathlib import Path
import pandas as pd
import numpy as np

# --- Feature matrix ---
X = df_encoded.copy()

# --- Initialize lists ---
inertias = []
silhouette_scores = []
K_range = range(2, 11)

print("ğŸ”¹ Running K-Means for k = 2 to 10...\n")

# --- Run K-Means across different k values ---
for k in K_range:
    try:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(X)
        inertias.append(kmeans.inertia_)
        score = silhouette_score(X, kmeans.labels_)
        silhouette_scores.append(score)
        print(f"âœ… k={k} â€” Inertia={kmeans.inertia_:.2f}, Silhouette={score:.4f}")
    except Exception as e:
        print(f"âš ï¸ Error for k={k}: {e}")
        inertias.append(np.nan)
        silhouette_scores.append(np.nan)

# --- Determine best k by silhouette score ---
valid_scores = [s for s in silhouette_scores if not np.isnan(s)]
best_k = K_range[silhouette_scores.index(max(valid_scores))]
print(f"\nğŸŒŸ Best k by silhouette score: {best_k}\n")

# ============================================================
# ğŸ“ˆ Visualization â€” Elbow & Silhouette
# ============================================================

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 4))

# --- Elbow Method ---
ax1.plot(K_range, inertias, marker='o', color='steelblue')
ax1.set_title("Elbow Method â€” K-Means Inertia", fontsize=11)
ax1.set_xlabel("Number of Clusters (k)")
ax1.set_ylabel("Inertia")

# --- Silhouette Scores ---
ax2.plot(K_range, silhouette_scores, marker='o', color='orange')
ax2.set_title("Silhouette Scores by Number of Clusters", fontsize=11)
ax2.set_xlabel("Number of Clusters (k)")
ax2.set_ylabel("Silhouette Score")

plt.tight_layout()
plt.show()

# ============================================================
# ğŸ’¾ Save Results
# ============================================================

viz_path = Path("..") / "visualizations"
viz_path.mkdir(parents=True, exist_ok=True)

fig.savefig(viz_path / "kmeans_elbow_silhouette_combined.png", dpi=300, bbox_inches="tight")
print(f"ğŸ’¾ Combined plot saved â†’ {viz_path / 'kmeans_elbow_silhouette_combined.png'}")

metrics_path = Path("..") / "data" / "clean"
metrics_path.mkdir(parents=True, exist_ok=True)

metrics_df = pd.DataFrame({
    "k": list(K_range),
    "inertia": inertias,
    "silhouette": silhouette_scores
})
metrics_df.to_csv(metrics_path / "kmeans_metrics.csv", index=False, encoding="utf-8-sig")

print(f"ğŸ’¾ Metrics saved â†’ {metrics_path / 'kmeans_metrics.csv'}")









# ============================================================
# Step 5 â€” Apply Final K-Means & Visualize Clusters (PCA 2D)
# ============================================================

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import numpy as np

# --- Final number of clusters ---
k_final = 2
print(f"ğŸ·ï¸ Applying final K-Means model with k = {k_final}...\n")

# --- Scale features ---
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_encoded)

# --- Train K-Means ---
kmeans_final = KMeans(n_clusters=k_final, random_state=42, n_init=10)
cluster_labels = kmeans_final.fit_predict(X_scaled)

# --- Assign clusters safely ---
df = df.copy()
df["cluster"] = cluster_labels

# --- PCA for visualization ---
pca = PCA(n_components=2, random_state=42)
pca_components = pca.fit_transform(X_scaled)
df["pca_1"] = pca_components[:, 0]
df["pca_2"] = pca_components[:, 1]

# ============================================================
# ğŸ¨ Enhanced Visualization â€” PCA 2D Scatter Plot
# ============================================================

fig_pca, ax = plt.subplots(figsize=(8, 6))
sns.scatterplot(
    data=df,
    x="pca_1", y="pca_2",
    hue="cluster",
    palette="Set2",
    s=65,
    alpha=0.85,
    edgecolor="white",
    linewidth=0.7,
    ax=ax
)
ax.set_title(f"Book Clusters â€” PCA 2D Projection (k = {k_final})", fontsize=13, pad=10)
ax.set_xlabel("Principal Component 1")
ax.set_ylabel("Principal Component 2")
ax.legend(title="Cluster", loc="best", fontsize=9)
ax.grid(alpha=0.25, linestyle="--")
plt.tight_layout()
plt.show()

# ============================================================
# ğŸ’¾ Save Plot
# ============================================================

viz_path = Path("..") / "visualizations"
fig_pca.savefig(viz_path / f"pca_clusters_k{k_final}.png", dpi=300, bbox_inches="tight")

print(f"Clustering completed. {df['cluster'].nunique()} clusters created.")
print(f"PCA cluster visualization saved â†’ {viz_path / f'pca_clusters_k{k_final}.png'}\n")

# --- Preview sample ---
display(df[["title", "author", "avg_rating", "genre", "price", "cluster"]].head(10))









# ============================================================
# Step 5.1 â€” Analyze Cluster Centroids
# ============================================================

"""
ğŸ¯ Step 5.1 â€” Analyze Cluster Centroids
Resumir los valores promedio de rating, precio y el gÃ©nero predominante
para entender las caracterÃ­sticas principales de cada cluster.
"""

cluster_centroids = (
    df.groupby("cluster")
    .agg({
        "avg_rating": "mean",
        "price": "mean",
        "genre": lambda x: x.mode().iloc[0] if not x.mode().empty else "Unknown"
    })
    .round(2)
)

# --- Add cluster sizes ---
cluster_counts = df["cluster"].value_counts().sort_index()
cluster_centroids["count"] = cluster_counts.values

print("ğŸ“Š Cluster Centroids Summary:\n")
display(cluster_centroids)

print("\nğŸ§­ Interpretation Guide:")
print("- avg_rating â†’ Average book rating per cluster.")
print("- price â†’ Mean price, useful to detect premium vs. budget titles.")
print("- genre â†’ Most common genre in each cluster.")
print("- count â†’ Number of books in each cluster.")









# ============================================================
# Step 6 â€” Export Final Clustered Dataset & Cluster Summary
# ============================================================

from pathlib import Path
import pandas as pd

# --- Define export paths ---
data_clean_path = Path("..") / config["paths"]["data_clean"]
viz_path = Path("..") / "visualizations"
data_clean_path.mkdir(parents=True, exist_ok=True)
viz_path.mkdir(parents=True, exist_ok=True)

# --- Export final dataset ---
final_cluster_path = data_clean_path / "books_clustered_final.csv"

export_cols = [
    "title", "author", "avg_rating", "genre", "price", "currency",
    "cover_url", "link", "cluster", "pca_1", "pca_2"
]

df[export_cols].to_csv(final_cluster_path, index=False, encoding="utf-8-sig")
print(f"ğŸ’¾ Final clustered dataset saved successfully â†’ {final_cluster_path.resolve()}")

# --- Quick preview ---
print("\nğŸ“˜ Sample of exported dataset:")
display(df[export_cols].head(10))

# ============================================================
# ğŸ“Š Cluster Summary (for visualization or presentation)
# ============================================================

cluster_summary = (
    df.groupby("cluster")
    .agg({
        "avg_rating": "mean",
        "price": "mean",
        "genre": lambda x: x.mode().iloc[0] if not x.mode().empty else "Unknown"
    })
    .round(2)
    .reset_index()
)

# --- Add cluster sizes ---
cluster_summary["count"] = df["cluster"].value_counts().sort_index().values

# --- Save summary ---
summary_path = data_clean_path / "cluster_summary.csv"
cluster_summary.to_csv(summary_path, index=False, encoding="utf-8-sig")

print("\nğŸ“— Cluster Summary:")
display(cluster_summary)

print(f"\nğŸ’¾ Cluster summary saved â†’ {summary_path.resolve()}")



# ============================================================
# ğŸ“Š Cluster Summary Table â€” Final Styled Version (Updated for k = 2)
# ============================================================

import pandas as pd
from IPython.display import display, HTML

# --- Use real summary from notebook ---
cluster_summary = pd.DataFrame({
    "Cluster": [0, 1],
    "Dominant Genre": ["Unknown", "Fiction"],
    "Avg Rating": [4.05, 4.21],
    "Avg Price (EUR)": [7.89, 9.15]
})

# --- Styling (same as before) ---
styled_summary = (
    cluster_summary.style
    .set_caption("ğŸ“š Cluster Summary â€” Average Features per Group (k = 2)")
    .set_table_styles([
        {"selector": "caption", 
         "props": [("text-align", "left"), ("font-size", "16px"), 
                   ("font-weight", "bold"), ("color", "#00c3ff")]},
        {"selector": "table", 
         "props": [("border", "2px solid #00c3ff"), ("border-radius", "8px"), 
                   ("border-collapse", "collapse")]},
        {"selector": "th", 
         "props": [("background-color", "#1c1c1c"), ("color", "white"), 
                   ("text-align", "center"), ("font-size", "14px")]},
        {"selector": "td", 
         "props": [("background-color", "#505050"), ("color", "#f2f2f2"), 
                   ("font-size", "13px"), ("text-align", "center")]}
    ])
    .hide(axis="index")
    .format({
        "Avg Rating": "{:.2f}",
        "Avg Price (EUR)": "{:.2f}"
    })
)

# --- Display ---
display(styled_summary)

