





# ============================================================
# Step 1 ‚Äî Imports & Setup 
# ============================================================

# --- System and project setup ---
import sys
from pathlib import Path

# Add 'notebooks' folder to path (functions.py lives there)
sys.path.append("notebooks")

# --- Load shared utilities ---
from functions import load_config, ensure_directories

# --- ML and visualization libraries ---
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# --- Visualization settings (como en clase) ---
sns.set(style="whitegrid", palette="muted")
plt.rcParams["figure.figsize"] = (8, 5)

# --- Load configuration and verify folders ---
config_path = Path("..") / "config.yaml"
config = load_config(config_path)
ensure_directories(config["paths"])

print("‚úÖ Environment ready ‚Äî config loaded and directories verified.")






# ============================================================
# Step 2 ‚Äî Load Final Dataset
# ============================================================

import pandas as pd
from pathlib import Path

# --- Load dataset from data/clean ---
data_path = Path("..") / config["paths"]["data_clean"] / "books_final_1000.csv"
df = pd.read_csv(data_path)

print(f"‚úÖ Dataset loaded successfully: {data_path}")
print(f"Shape: {df.shape}\n")

# --- Quick overview ---
display(df.head(5))

# --- Basic info and types ---
print("\nüîç DataFrame Info:")
print(df.info())

# --- Missing values summary ---
missing_summary = df.isna().sum()
missing_summary = missing_summary[missing_summary > 0]

if not missing_summary.empty:
    print("\n‚ö†Ô∏è Missing values summary:")
    print(missing_summary)
else:
    print("\n‚úÖ No missing values detected.")

# --- Optional: Unique values check (for categorical columns) ---
print("\nüß© Unique values per column:")
print(df.nunique())






# ============================================================
# Step 2.1 ‚Äî Clean & Normalize Genres
# ============================================================

"""
üéØ Ensure genre names are consistent and meaningful.
Preserve subcategories (e.g., Young Adult Fiction, Juvenile Fiction)
and fix missing or inconsistent values.
"""

import numpy as np

# --- Clean genre text ---
df["genre"] = (
    df["genre"]
    .astype(str)
    .str.strip()
    .replace({"nan": np.nan, "None": np.nan})
)

# --- Replace NaN with "Unknown" ---
df["genre"] = df["genre"].fillna("Unknown")

# --- Title case normalization ---
df["genre"] = df["genre"].str.title()

# --- Fix common variants ---
genre_replacements = {
    "Juvenile Fiction ": "Juvenile Fiction",
    "Young Adult Fiction ": "Young Adult Fiction",
    "Biography & Autobiography ": "Biography & Autobiography",
    "Nan": "Unknown"
}
df["genre"] = df["genre"].replace(genre_replacements)

# --- Final check ---
print("‚úÖ Genre normalization complete.\n")
print("Top 10 genres after cleaning:")
display(df["genre"].value_counts().head(10))






# ============================================================
# Step 2.2 ‚Äî Clean and Normalize Genres Before Encoding
# ============================================================

def normalize_genre(value):
    if pd.isna(value):
        return "Unknown"
    value = value.strip().title()
    # Simplify subcategories
    if "Juvenile" in value:
        return "Juvenile Fiction"
    if "Young Adult" in value:
        return "Young Adult Fiction"
    if "Biography" in value:
        return "Biography & Autobiography"
    if "Comics" in value or "Graphic" in value:
        return "Comics & Graphic Novels"
    if "Poet" in value:
        return "Poetry"
    if "Drama" in value:
        return "Drama"
    if "Relig" in value:
        return "Religion"
    if "History" in value:
        return "History"
    if "Fiction" in value:
        return "Fiction"
    return value

# Apply normalization
df["genre"] = df["genre"].apply(normalize_genre)

print("‚úÖ Genres normalized successfully.\n")
print("Top 10 genres after normalization:")
display(df["genre"].value_counts().head(10))






# ============================================================
# Step 3 ‚Äî Feature Preparation (Numeric Features Only)
# ============================================================

from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np

# --- Select relevant numerical columns for clustering ---
features = ["avg_rating", "price"]
df_features = df[features].copy()

# --- Handle missing prices ---
median_price = df_features["price"].median()
df_features["price"] = df_features["price"].fillna(median_price)

# --- Reflect the filled prices back into the main DataFrame ---
df["price"] = df["price"].fillna(median_price)
print(f"Filled missing 'price' values with median: {median_price:.2f}")

# --- Standardize numeric columns ---
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df_features)

# --- Convert back to DataFrame for easier handling ---
df_encoded = pd.DataFrame(df_scaled, columns=features)

# --- Sanity check ---
missing_check = df_encoded.isna().sum().sum()
if missing_check == 0:
    print("‚úÖ No missing values remain in feature matrix.")
else:
    print(f"‚ö†Ô∏è {missing_check} missing values still present ‚Äî check source data.")

# --- Assign to feature matrix for clustering ---
X = df_encoded.values

print(f"\n‚úÖ Feature matrix ready for clustering. Shape: {df_encoded.shape}")

# --- Quick preview ---
display(df_encoded.head(5))






# ============================================================
# Step 4 ‚Äî K-Means Clustering (Elbow & Silhouette Method)
# ============================================================

import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
from pathlib import Path
import pandas as pd

# --- Feature matrix (numeric only) ---
X = df_encoded.values

# --- Initialize lists ---
inertias = []
silhouette_scores = []
K_range = range(2, 11)

print("Running K-Means for k = 2 to 10...\n")

# --- Run K-Means across different k values ---
for k in K_range:
    try:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(X)
        inertias.append(kmeans.inertia_)
        score = silhouette_score(X, kmeans.labels_)
        silhouette_scores.append(score)
        print(f"k={k} ‚Äî Inertia={kmeans.inertia_:.2f}, Silhouette={score:.4f}")
    except Exception as e:
        print(f"‚ö†Ô∏è Error for k={k}: {e}")
        inertias.append(np.nan)
        silhouette_scores.append(np.nan)

# --- Determine best k by silhouette score ---
valid_scores = [s for s in silhouette_scores if not np.isnan(s)]
best_k = K_range[silhouette_scores.index(max(valid_scores))]

# --- Show top 3 silhouette values ---
sorted_scores = sorted(zip(K_range, silhouette_scores), key=lambda x: x[1], reverse=True)
print("\nTop 3 silhouette scores:")
for i, (k_val, s_val) in enumerate(sorted_scores[:3], start=1):
    print(f"{i}. k={k_val} ‚Üí silhouette={s_val:.4f}")

print(f"\nBest k by silhouette score: {best_k}")

# ============================================================
# Visualization ‚Äî Elbow & Silhouette
# ============================================================

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 4))

# Elbow Method
ax1.plot(K_range, inertias, marker='o', color='steelblue')
ax1.set_title("Elbow Method ‚Äî K-Means Inertia", fontsize=11)
ax1.set_xlabel("Number of Clusters (k)")
ax1.set_ylabel("Inertia")

# Silhouette Scores
ax2.plot(K_range, silhouette_scores, marker='o', color='orange')
ax2.set_title("Silhouette Scores by Number of Clusters", fontsize=11)
ax2.set_xlabel("Number of Clusters (k)")
ax2.set_ylabel("Silhouette Score")

plt.tight_layout()
plt.show()

# ============================================================
# Save Results
# ============================================================

viz_path = Path("..") / "visualizations"
viz_path.mkdir(parents=True, exist_ok=True)
fig.savefig(viz_path / "kmeans_elbow_silhouette_combined.png", dpi=300, bbox_inches="tight")

metrics_path = Path("..") / "data" / "clean"
metrics_path.mkdir(parents=True, exist_ok=True)

metrics_df = pd.DataFrame({
    "k": list(K_range),
    "inertia": inertias,
    "silhouette": silhouette_scores
})
metrics_df.to_csv(metrics_path / "kmeans_metrics.csv", index=False, encoding="utf-8-sig")

print(f"\nüíæ Results saved ‚Üí {metrics_path / 'kmeans_metrics.csv'}")
print(f"üè∑Ô∏è Best number of clusters: {best_k}")









# ============================================================
# Step 5 ‚Äî Apply Final K-Means & Visualize Clusters (PCA 2D)
# ============================================================

from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import numpy as np

# --- Final number of clusters ---
k_final = 2
print(f"Applying final K-Means model with k = {k_final}...\n")

# --- Use scaled numeric features from df_encoded (already standardized) ---
X_scaled = df_encoded.values

# --- Train K-Means ---
kmeans_final = KMeans(n_clusters=k_final, random_state=42, n_init=10)
cluster_labels = kmeans_final.fit_predict(X_scaled)

# --- Assign clusters safely ---
df = df.copy()
df["cluster"] = cluster_labels

# --- PCA for visualization ---
pca = PCA(n_components=2, random_state=42)
pca_components = pca.fit_transform(X_scaled)
df["pca_1"] = pca_components[:, 0]
df["pca_2"] = pca_components[:, 1]

# ============================================================
# Visualization ‚Äî PCA 2D Scatter Plot
# ============================================================

fig_pca, ax = plt.subplots(figsize=(8, 6))
sns.scatterplot(
    data=df,
    x="pca_1", y="pca_2",
    hue="cluster",
    palette="Set2",
    s=65,
    alpha=0.85,
    edgecolor="white",
    linewidth=0.7,
    ax=ax
)
ax.set_title(f"Book Clusters ‚Äî PCA 2D Projection (k = {k_final})", fontsize=13, pad=10)
ax.set_xlabel("Principal Component 1")
ax.set_ylabel("Principal Component 2")
ax.legend(title="Cluster", loc="best", fontsize=9)
ax.grid(alpha=0.25, linestyle="--")
plt.tight_layout()
plt.show()

# ============================================================
# Save Plot
# ============================================================

viz_path = Path("..") / "visualizations"
viz_path.mkdir(parents=True, exist_ok=True)
fig_pca.savefig(viz_path / f"pca_clusters_k{k_final}.png", dpi=300, bbox_inches="tight")

print(f"Clustering completed. {df['cluster'].nunique()} clusters created.")
print(f"PCA cluster visualization saved ‚Üí {viz_path / f'pca_clusters_k{k_final}.png'}\n")

# --- Preview sample ---
display(df[["title", "author", "avg_rating", "genre", "price", "cluster"]].head(10))









# ============================================================
# Step 5.1 ‚Äî Cluster Composition Summary
# ============================================================

"""
üéØ Step 5.1 ‚Äî Analyze Cluster Composition by Genre
This version summarizes how genres distribute within each cluster,
while also showing the average rating and price per cluster.
"""

# --- Numeric centroids (average rating & price) ---
cluster_centroids = (
    df.groupby("cluster", as_index=False)
    .agg({
        "avg_rating": "mean",
        "price": "mean"
    })
    .round(2)
)

# --- Genre distribution within each cluster ---
genre_distribution = (
    df.groupby(["cluster", "genre"])
    .size()
    .reset_index(name="count")
)

# --- Calculate proportions per cluster ---
cluster_sizes = df["cluster"].value_counts().to_dict()
genre_distribution["proportion_%"] = genre_distribution.apply(
    lambda row: round((row["count"] / cluster_sizes[row["cluster"]]) * 100, 2),
    axis=1
)

# --- Merge numeric averages with genre distribution ---
cluster_summary = (
    genre_distribution.merge(cluster_centroids, on="cluster", how="left")
    .sort_values(["cluster", "count"], ascending=[True, False])
)

# --- Display top 5 genres per cluster ---
print("üìä Cluster Composition Summary (Top 5 Genres per Cluster):\n")
display(cluster_summary.groupby("cluster").head(5))

print("\nüß≠ Interpretation Guide:")
print("- avg_rating ‚Üí Average rating within the cluster.")
print("- price ‚Üí Mean price (EUR) within the cluster.")
print("- count ‚Üí Number of books belonging to that genre in the cluster.")
print("- proportion_% ‚Üí Relative share (%) of that genre within the cluster.")









# ============================================================
# Step 6 ‚Äî Export Final Clustered Dataset & Cluster Summary
# ============================================================

from pathlib import Path
import pandas as pd

# --- Define export paths ---
data_clean_path = Path("..") / config["paths"]["data_clean"]
viz_path = Path("..") / "visualizations"
data_clean_path.mkdir(parents=True, exist_ok=True)
viz_path.mkdir(parents=True, exist_ok=True)

# --- Export final dataset ---
final_cluster_path = data_clean_path / "books_clustered_final.csv"

export_cols = [
    "title", "author", "avg_rating", "genre", "price", "currency",
    "cover_url", "link", "cluster", "pca_1", "pca_2"
]

df[export_cols].to_csv(final_cluster_path, index=False, encoding="utf-8-sig")
print(f"üíæ Final clustered dataset saved successfully ‚Üí {final_cluster_path.resolve()}")

print("\nüìò Sample of exported dataset:")
display(df[export_cols].head(10))

# ============================================================
# üìä Cluster Summary (improved: true genre distribution)
# ============================================================

# --- Numeric stats ---
cluster_centroids = (
    df.groupby("cluster")
    .agg({
        "avg_rating": "mean",
        "price": "mean"
    })
    .round(2)
    .reset_index()
)

# --- Genre distribution ---
genre_distribution = (
    df.groupby(["cluster", "genre"])
    .size()
    .reset_index(name="count")
)

# --- Add proportions per cluster ---
cluster_sizes = df["cluster"].value_counts().to_dict()
genre_distribution["proportion_%"] = genre_distribution.apply(
    lambda row: round((row["count"] / cluster_sizes[row["cluster"]]) * 100, 2),
    axis=1
)

# --- Merge numeric stats with genre composition ---
cluster_summary = genre_distribution.merge(cluster_centroids, on="cluster", how="left")

# --- Sort by cluster and descending count ---
cluster_summary = cluster_summary.sort_values(["cluster", "count"], ascending=[True, False])

print("\nüìó Cluster Summary (Top Genres per Cluster):")
display(cluster_summary.groupby("cluster").head(5))

print(f"\nüíæ Cluster summary saved ‚Üí {data_clean_path / 'cluster_summary.csv'}")




