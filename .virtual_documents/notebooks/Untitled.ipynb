


# ============================================================
# Step 1 ‚Äî Imports & Setup
# ============================================================

import sys
from pathlib import Path
import importlib

# --- Add notebooks path and import functions ---
sys.path.append("notebooks")

import functions
from functions import load_config, ensure_directories

# --- Reload in case of changes ---
importlib.reload(functions)

# --- Load config from project root ---
config_path = Path("../config.yaml")
config = load_config(config_path)

# --- Ensure directories exist (defined in config.yaml) ---
ensure_directories(config["paths"])

print("‚úÖ All functions and configuration loaded successfully!")






# ============================================================
# Step 2 ‚Äî Scrape Goodreads Pages 6‚Äì10
# ============================================================

from functions import fetch_html, parse_html, extract_books_from_soup, save_dataset
from time import sleep

print("üîπ Starting scraping for pages 6‚Äì10...")

base_url = "https://www.goodreads.com/list/show/1.Best_Books_Ever?page="
all_books_part2 = []

# --- Loop through pages 6 to 10 ---
for page in range(6, 11):
    url = base_url + str(page)
    print(f"\nüåç Fetching page {page}: {url}")

    html = fetch_html(url)
    if html is None:
        print(f"‚ö†Ô∏è Skipping page {page} due to empty response.")
        continue

    soup = parse_html(html)
    books_page = extract_books_from_soup(soup)
    all_books_part2.append(books_page)

    # --- Ethical delay ---
    sleep(random.uniform(1, 2))

print(f"\n‚úÖ Scraping completed for {len(all_books_part2)} pages.")

# --- Combine all new DataFrames ---
df_part2 = pd.concat(all_books_part2, ignore_index=True)
print(f"‚úÖ Combined dataset shape (pages 6‚Äì10): {df_part2.shape}")

# --- Save dataset correctly in the root-level data/raw ---
from pathlib import Path

raw_output_path = Path("..") / config["paths"]["data_raw"] / "goodreads_books_6_to_10.csv"
print(f"üìÅ Saving file to: {raw_output_path.resolve()}")

save_dataset(df_part2, raw_output_path)

# --- Preview ---
df_part2.head()






# ============================================================
# Step 3 ‚Äî Combine New Books (6‚Äì10) with Existing Clean Dataset
# ============================================================

import pandas as pd
from pathlib import Path
from functions import save_dataset

# --- Load existing clean dataset ---
clean_path_prev = Path("..") / config["paths"]["data_clean"] / "books_clean.csv"
df_existing = pd.read_csv(clean_path_prev)
print(f"‚úÖ Loaded previous clean dataset: {df_existing.shape}")

# --- Define expected columns from the clean dataset ---
expected_cols = df_existing.columns.tolist()

# --- Create any missing columns in df_part2 ---
for col in expected_cols:
    if col not in df_part2.columns:
        df_part2[col] = pd.NA

# --- Ensure only expected columns are kept (ignore extras safely) ---
df_part2 = df_part2[[col for col in expected_cols if col in df_part2.columns]]

# --- Combine both datasets ---
df_combined = pd.concat([df_existing, df_part2], ignore_index=True)

# --- Drop duplicates by title ---
df_combined = df_combined.drop_duplicates(subset=["title"]).reset_index(drop=True)

print(f"‚úÖ Combined dataset shape: {df_combined.shape}")
print(f"Unique authors: {df_combined['author'].nunique()}")

# --- Save the updated clean dataset ---
output_path = Path("..") / config["paths"]["data_clean"] / "books_clean_1000.csv"
print(f"üíæ Saving combined clean dataset to: {output_path.resolve()}")

save_dataset(df_combined, output_path)

# --- Preview ---
df_combined.head()






# ============================================================
# Step 4 ‚Äî Quick Data Verification
# ============================================================

import pandas as pd
from pathlib import Path

# --- Load combined dataset ---
combined_path = Path("..") / config["paths"]["data_clean"] / "books_clean_1000.csv"
df = pd.read_csv(combined_path)

print(f"‚úÖ Dataset loaded successfully: {combined_path}")
print(f"Shape: {df.shape}\n")

# --- Overview of columns ---
print("üìä Columns:")
print(df.columns.tolist())

# --- Quick info and missing values ---
print("\nüîç DataFrame Info:")
df.info()

print("\nüîç Missing values per column:")
print(df.isna().sum())

# --- Quick summary statistics (no datetime flag for older pandas) ---
print("\nüìà Descriptive Statistics:")
display(df.describe(include="all"))

# --- Check duplicates by title ---
dup_titles = df["title"].duplicated().sum()
print(f"\n‚ö†Ô∏è Duplicate titles found: {dup_titles}")

# --- Unique authors ---
unique_authors = df["author"].nunique()
print(f"üë©‚Äçüíª Unique authors: {unique_authors}")

# --- Example preview ---
print("\nüìò Preview of the combined dataset:")
display(df.head(10))






# ============================================================
# Step 5 ‚Äî Reuse Google Books API functions
# ============================================================
import requests
from tqdm import tqdm
import time

def get_book_info_from_google(title, author):
    """Query Google Books API and return metadata for a given title + author."""
    query = f"intitle:{title}+inauthor:{author}"
    url = f"https://www.googleapis.com/books/v1/volumes?q={query}"

    try:
        response = requests.get(url, timeout=10)
        if response.status_code == 200:
            data = response.json()
            if "items" in data and len(data["items"]) > 0:
                info = data["items"][0]["volumeInfo"]
                return {
                    "published_year": info.get("publishedDate", None),
                    "genre": ", ".join(info.get("categories", [])) if info.get("categories") else None,
                    "cover_url": info.get("imageLinks", {}).get("thumbnail", None)
                }
    except Exception as e:
        print(f"‚ö†Ô∏è Error fetching '{title}': {e}")
    
    return {"published_year": None, "genre": None, "cover_url": None}


def get_price_from_google(title, author):
    """Query Google Books API for price info (listPrice or retailPrice)."""
    query = f"intitle:{title}+inauthor:{author}"
    url = f"https://www.googleapis.com/books/v1/volumes?q={query}"

    try:
        response = requests.get(url, timeout=10)
        if response.status_code == 200:
            data = response.json()
            if "items" in data:
                info = data["items"][0].get("saleInfo", {})
                price_info = info.get("listPrice", {}) or info.get("retailPrice", {})
                if price_info:
                    return price_info.get("amount"), price_info.get("currencyCode")
    except Exception as e:
        print(f"‚ö†Ô∏è Error fetching '{title}': {e}")
    return None, None



# ============================================================
# Step 5.1 ‚Äî Enrich only books missing metadata
# ============================================================

# --- Load combined dataset ---
from pathlib import Path
combined_path = Path("..") / config["paths"]["data_clean"] / "books_clean_1000.csv"
df = pd.read_csv(combined_path)

# --- Filter only the books that need enrichment ---
df_to_enrich = df[df["avg_rating"].isna()].copy()
print(f"üìö Books pending enrichment: {len(df_to_enrich)}")

# --- Apply Google Books API to retrieve metadata ---
results = []
for _, row in tqdm(df_to_enrich.iterrows(), total=len(df_to_enrich)):
    meta = get_book_info_from_google(row["title"], row["author"])
    results.append(meta)
    time.sleep(1.5)  # ethical delay

api_df = pd.DataFrame(results)
df_enriched = pd.concat([df_to_enrich.reset_index(drop=True), api_df], axis=1)

print(f"‚úÖ Metadata enrichment completed for {len(df_enriched)} books.")
df_enriched.head()



df_enriched.to_csv("../data/raw/temp_books_meta_backup.csv", index=False, encoding="utf-8-sig")
print("üíæ Backup saved successfully!")



df_enriched.isna().sum()






# ============================================================
# Step 5.3 ‚Äî Price Enrichment (Optimized & Re-startable)
# ============================================================

import pandas as pd
import time
from tqdm import tqdm
from pathlib import Path

# --- Load the intermediate enriched file (from Step 5.2) ---
intermediate_path = Path("..") / config["paths"]["data_raw"] / "temp_books_meta_backup.csv"
df_enriched = pd.read_csv(intermediate_path)

print(f"‚úÖ Loaded enriched dataset for price retrieval: {df_enriched.shape[0]} books")

# --- Define checkpoint path ---
checkpoint_path = Path("..") / config["paths"]["data_raw"] / "temp_prices_checkpoint.csv"

# --- If checkpoint exists, resume from there ---
if checkpoint_path.exists():
    df_checkpoint = pd.read_csv(checkpoint_path)
    processed_titles = set(df_checkpoint["title"].unique())
    print(f"‚è© Resuming from checkpoint ({len(df_checkpoint)} books already processed).")
else:
    df_checkpoint = pd.DataFrame(columns=["title", "price", "currency"])
    processed_titles = set()
    print("üÜï Starting fresh price enrichment.")

# --- Filter only books not yet processed ---
df_to_process = df_enriched[~df_enriched["title"].isin(processed_titles)].copy()
print(f"üìö Remaining books to process: {len(df_to_process)}")

# --- Apply Google Books API to get prices ---
prices = []

for i, (_, row) in enumerate(tqdm(df_to_process.iterrows(), total=len(df_to_process))):
    price, currency = get_price_from_google(row["title"], row["author"])
    prices.append({"title": row["title"], "price": price, "currency": currency})
    
    # --- Save progress every 50 books ---
    if (i + 1) % 50 == 0 or (i + 1) == len(df_to_process):
        df_partial = pd.DataFrame(prices)
        df_checkpoint = pd.concat([df_checkpoint, df_partial], ignore_index=True)
        df_checkpoint.to_csv(checkpoint_path, index=False, encoding="utf-8-sig")
        print(f"üíæ Checkpoint saved ({len(df_checkpoint)} total so far)")
        prices = []  # reset buffer
    
    time.sleep(0.5)  # reduced ethical delay

print("\n‚úÖ Price enrichment completed!")

# --- Merge checkpoint results into main enriched dataset ---
df_prices_final = pd.read_csv(checkpoint_path)
df_final = pd.merge(df_enriched, df_prices_final, on="title", how="left")

# --- Save final enriched dataset ---
output_path = Path("..") / config["paths"]["data_clean"] / "books_clean_enriched_1000.csv"
df_final.to_csv(output_path, index=False, encoding="utf-8-sig")

print(f"\nüíæ Final enriched dataset saved successfully ‚Üí {output_path.resolve()}")
print(f"Rows: {len(df_final)}, Columns: {len(df_final.columns)}")






# ============================================================
# Step 6 ‚Äî Final Cleanup (Fix Duplicated Columns In-Place)
# ============================================================

from pathlib import Path

# --- Load dataset (already enriched, 497 rows) ---
clean_path = Path("..") / config["paths"]["data_clean"] / "books_clean_enriched_1000.csv"
df = pd.read_csv(clean_path)
print(f"‚úÖ Loaded dataset: {df.shape}")

# --- Define the column groups to fix ---
pairs = {
    "published_year": ["published_year", "published_year.1"],
    "genre": ["genre", "genre.1"],
    "price": ["price", "price_x", "price_y"],
    "currency": ["currency", "currency_x", "currency_y"],
    "cover_url": ["cover_url", "cover_url.1"]
}

# --- Keep the most complete column for each group ---
for final_col, candidates in pairs.items():
    valid_cols = [c for c in candidates if c in df.columns]
    if not valid_cols:
        continue
    # pick the one with the most non-null values
    best_col = df[valid_cols].notna().sum().idxmax()
    df[final_col] = df[best_col]

# --- Drop all duplicates (.1, _x, _y) ---
df = df.drop(columns=[c for c in df.columns if any(s in c for s in [".1", "_x", "_y"])], errors="ignore")

# --- Reorder columns ---
cols_final = [
    "title", "author", "avg_rating", "genre",
    "published_year", "price", "currency", "cover_url", "link"
]
df = df[[c for c in cols_final if c in df.columns]]

# --- Overwrite the same file (no new file creation) ---
df.to_csv(clean_path, index=False, encoding="utf-8-sig")

print(f"üíæ Cleaned and overwritten ‚Üí {clean_path.resolve()}")
print(f"‚úÖ Final shape: {df.shape}")
df.head(10)






# ============================================================
# Step 6.1 ‚Äî Recalculate `avg_rating` Column
# ============================================================

from pathlib import Path
import re

# --- Load the same dataset in place ---
clean_path = Path("..") / config["paths"]["data_clean"] / "books_clean_enriched_1000.csv"
df = pd.read_csv(clean_path)
print(f"‚úÖ Loaded dataset: {df.shape}")

# --- If rating exists, extract the numeric part ---
if "rating" in df.columns:
    df["avg_rating"] = (
        df["rating"]
        .astype(str)
        .str.extract(r"(\d+\.\d+)")
        .astype(float)
    )
    print("‚úÖ Extracted numeric avg_rating from text field.")
else:
    print("‚ö†Ô∏è Column 'rating' not found ‚Äî please confirm if it was dropped earlier.")

# --- Save updated file in place ---
df.to_csv(clean_path, index=False, encoding="utf-8-sig")

print(f"üíæ Updated avg_rating saved in ‚Üí {clean_path.resolve()}")
print(df[["title", "rating", "avg_rating"]].head(10))




