


# ============================================================
# Step 1 ‚Äî Load Configuration & Base Dataset
# ============================================================
import os
os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"
import pandas as pd
from pathlib import Path
from functions import load_config, ensure_directories

# --- Load configuration from project root ---
config_path = Path("..") / "config.yaml"
config = load_config(config_path)

# --- Ensure all folders exist ---
ensure_directories(config["paths"])

# --- Load base dataset ---
data_clean_path = Path("..") / config["paths"]["data_clean"]
input_file = data_clean_path / "books_clustered_final.csv"

df_main = pd.read_csv(input_file)

print(f"Dataset loaded successfully: {df_main.shape}")
df_main.head(3)






# ============================================================
# Step 2 ‚Äî Load Goodreads Dataset (Kaggle goodbooks-10k)
# ============================================================

import pandas as pd
from pathlib import Path

# Define path (using config paths)
data_raw_path = Path("..") / "data" / "raw"
goodreads_file = data_raw_path / "books.csv"  # rename your downloaded books.csv to this

# Load dataset
df_goodreads = pd.read_csv(goodreads_file)
print(f"Kaggle Goodreads dataset loaded: {df_goodreads.shape}")

# Display available columns
print("Columns:", df_goodreads.columns.tolist())

# Preview
df_goodreads.head(3)



print(df_goodreads.columns.tolist())






# ============================================================
# Step 3 ‚Äî Preprocess Titles & Authors for Merging
# ============================================================

# Select and rename relevant columns
cols_to_keep = [
    "title",
    "authors",
    "average_rating",
    "ratings_count",
    "original_publication_year",
    "image_url"
]

df_goodreads = df_goodreads[cols_to_keep].rename(columns={
    "authors": "author",
    "average_rating": "avg_rating_goodreads",
    "ratings_count": "ratings_count_goodreads",
    "original_publication_year": "published_year_goodreads",
    "image_url": "cover_url_goodreads"
})

# Normalize titles and authors in both datasets
df_main["title_clean"] = df_main["title"].str.lower().str.strip()
df_main["author_clean"] = df_main["author"].str.lower().str.strip()

df_goodreads["title_clean"] = df_goodreads["title"].str.lower().str.strip()
df_goodreads["author_clean"] = df_goodreads["author"].str.lower().str.strip()

print("Columns prepared for merging:")
print(df_goodreads.head(3))






# ============================================================
# Step 4 ‚Äî Merge Datasets (Left Join by Title & Author)
# ============================================================

# Perform left join
df_merged = pd.merge(
    df_main,
    df_goodreads[
        [
            "title_clean",
            "author_clean",
            "avg_rating_goodreads",
            "ratings_count_goodreads",
            "published_year_goodreads",
            "cover_url_goodreads"
        ]
    ],
    on=["title_clean", "author_clean"],
    how="left"
)

print(f"Merge completed: {df_merged.shape}")

# Display sample of enriched data
df_merged[
    ["title", "author", "avg_rating", "avg_rating_goodreads", "ratings_count_goodreads"]
].head(10)






# ============================================================
# Step 5 ‚Äî Safely Replace Imputed Ratings and Save Enriched Dataset
# ============================================================

from functions import save_dataset
from pathlib import Path

# --- Create a copy to be safe ---
df_enriched = df_merged.copy()

# Replace only imputed ratings (4.11) with Goodreads ratings when available
mask_replace = (
    df_enriched["avg_rating"].round(2) == 4.11
) & (df_enriched["avg_rating_goodreads"].notna())

df_enriched.loc[mask_replace, "avg_rating"] = df_enriched.loc[
    mask_replace, "avg_rating_goodreads"
]

# Keep Goodreads ratings_count as a new column (optional feature)
df_enriched["ratings_count"] = df_enriched["ratings_count_goodreads"]

# Remove helper columns but keep your core structure intact
df_enriched = df_enriched.drop(columns=["avg_rating_goodreads", "ratings_count_goodreads"])

# Save the enriched dataset
output_path = Path("..") / "data" / "clean" / "books_final_enriched.csv"
save_dataset(df_enriched, output_path)

# --- Summary ---
print("‚úÖ Enriched dataset saved safely ‚Üí books_final_enriched.csv")
print(f"Ratings replaced (4.11 ‚Üí Goodreads): {mask_replace.sum()}")
print(df_enriched[["title", "author", "avg_rating", "ratings_count"]].head(10))









# ============================================================
# Step 7 ‚Äî Clean Final Dataset for Re-Training (Overwrite File)
# ============================================================

import pandas as pd
from pathlib import Path

# Load enriched dataset
path_enriched = Path("..") / "data" / "clean" / "books_final_enriched.csv"
df = pd.read_csv(path_enriched)

# Drop unnecessary columns
cols_to_drop = [
    "cluster",
    "pca_1",
    "pca_2",
    "title_clean",
    "author_clean",
    "cover_url_goodreads"
]
df = df.drop(columns=[col for col in cols_to_drop if col in df.columns])

# Ensure ratings_count is numeric
df["ratings_count"] = pd.to_numeric(df["ratings_count"], errors="coerce")

# Overwrite the same file
df.to_csv(path_enriched, index=False, encoding="utf-8-sig")

print("‚úÖ Cleaned and overwritten successfully ‚Üí books_final_enriched.csv")
print(f"Final shape: {df.shape}")
print("Columns ready for re-training:")
print(df.columns.tolist())






# ============================================================
# Step 8 ‚Äî Data Health Check (Missing Values & Completeness)
# ============================================================

import pandas as pd
from pathlib import Path

# Load the cleaned enriched dataset
path_data = Path("..") / "data" / "clean" / "books_final_enriched.csv"
df = pd.read_csv(path_data)

# Select relevant columns to inspect
cols_to_check = [
    "avg_rating",
    "price",
    "ratings_count",
    "genre",
    "published_year_goodreads"
]

# Calculate missing counts and percentages
missing_counts = df[cols_to_check].isna().sum()
missing_pct = (missing_counts / len(df)) * 100

# Combine into summary DataFrame
missing_summary = pd.DataFrame({
    "Missing Values": missing_counts,
    "Missing %": missing_pct.round(2)
}).sort_values("Missing %", ascending=False)

print("Missing Value Summary:")
display(missing_summary)






# ============================================================
# Step 9 ‚Äî Feature Preparation (Price & Rating Only)
# ============================================================

from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np

# --- Select relevant numeric features ---
features = ["avg_rating", "price"]
df_features = df[features].copy()

# --- Handle missing prices ---
median_price = df_features["price"].median()
df_features["price"] = df_features["price"].fillna(median_price)
df["price"] = df["price"].fillna(median_price)
print(f"Filled missing 'price' values with median: {median_price:.2f}")

# --- Standardize numeric features ---
scaler = StandardScaler()
df_scaled = pd.DataFrame(
    scaler.fit_transform(df_features),
    columns=features
)
print("üìè Numeric columns standardized (avg_rating, price).")

# --- Check for missing values ---
missing_check = df_scaled.isna().sum().sum()
if missing_check == 0:
    print("‚úÖ No missing values remain in scaled feature matrix.")
else:
    print(f"‚ö†Ô∏è {missing_check} missing values still present ‚Äî check source data.")

# --- Create feature matrix for clustering ---
X = df_scaled.values
print(f"\n‚úÖ Feature matrix ready for clustering. Shape: {df_scaled.shape}")

# --- Quick preview ---
display(df_scaled.head(5))






# ============================================================
# Step 10 ‚Äî K-Means Clustering (Elbow & Silhouette Method)
# ============================================================

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
from pathlib import Path
import pandas as pd
import numpy as np

# --- Feature matrix (scaled numeric data) ---
X = df_scaled.copy()

# --- Initialize lists ---
inertias = []
silhouette_scores = []
K_range = range(2, 11)

print("Running K-Means for k = 2 to 10...\n")

# --- Run K-Means across different k values ---
for k in K_range:
    try:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(X)
        inertias.append(kmeans.inertia_)
        score = silhouette_score(X, kmeans.labels_)
        silhouette_scores.append(score)
        print(f"k={k} ‚Äî Inertia={kmeans.inertia_:.2f}, Silhouette={score:.4f}")
    except Exception as e:
        print(f"Error for k={k}: {e}")
        inertias.append(np.nan)
        silhouette_scores.append(np.nan)

# --- Determine best k by silhouette score ---
valid_scores = [s for s in silhouette_scores if not np.isnan(s)]
best_k = list(K_range)[silhouette_scores.index(max(valid_scores))]
print(f"\nBest k by silhouette score: {best_k}\n")

# ============================================================
# üìà Visualization ‚Äî Elbow & Silhouette
# ============================================================

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 4))

# Elbow curve
ax1.plot(K_range, inertias, marker='o', color='steelblue')
ax1.set_title("Elbow Method ‚Äî K-Means Inertia", fontsize=11)
ax1.set_xlabel("Number of Clusters (k)")
ax1.set_ylabel("Inertia")

# Silhouette curve
ax2.plot(K_range, silhouette_scores, marker='o', color='orange')
ax2.set_title("Silhouette Scores by Number of Clusters", fontsize=11)
ax2.set_xlabel("Number of Clusters (k)")
ax2.set_ylabel("Silhouette Score")

plt.tight_layout()
plt.show()

# ============================================================
# üíæ Save Results
# ============================================================

viz_path = Path("..") / "visualizations"
viz_path.mkdir(parents=True, exist_ok=True)
fig.savefig(viz_path / "kmeans_elbow_silhouette_combined.png", dpi=300, bbox_inches="tight")
print(f"Plot saved ‚Üí {viz_path / 'kmeans_elbow_silhouette_combined.png'}")

metrics_path = Path("..") / "data" / "clean"
metrics_df = pd.DataFrame({
    "k": list(K_range),
    "inertia": inertias,
    "silhouette": silhouette_scores
})
metrics_df.to_csv(metrics_path / "kmeans_metrics.csv", index=False, encoding="utf-8-sig")
print(f"Metrics saved ‚Üí {metrics_path / 'kmeans_metrics.csv'}")






# ============================================================
# Step 11 ‚Äî Train Final K-Means & Visualize Clusters (PCA 2D)
# ============================================================

from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import numpy as np
import pandas as pd

# --- Load best_k from previous step ---
k_final = best_k  # or set manually, e.g. k_final = 2
print(f"Applying final K-Means model with k = {k_final}...\n")

# --- Use scaled numeric features from Step 9 ---
X_scaled = df_scaled.copy()

# --- Train K-Means ---
kmeans_final = KMeans(n_clusters=k_final, random_state=42, n_init=10)
cluster_labels = kmeans_final.fit_predict(X_scaled)

# --- Assign clusters safely ---
df = df.copy()
df["cluster"] = cluster_labels

# --- PCA for visualization ---
pca = PCA(n_components=2, random_state=42)
pca_components = pca.fit_transform(X_scaled)
df["pca_1"] = pca_components[:, 0]
df["pca_2"] = pca_components[:, 1]

# ============================================================
# üìä PCA 2D Visualization
# ============================================================

fig_pca, ax = plt.subplots(figsize=(8, 6))
sns.scatterplot(
    data=df,
    x="pca_1", y="pca_2",
    hue="cluster",
    palette="Set2",
    s=65,
    alpha=0.85,
    edgecolor="white",
    linewidth=0.7,
    ax=ax
)
ax.set_title(f"Book Clusters ‚Äî PCA 2D Projection (k = {k_final})", fontsize=13, pad=10)
ax.set_xlabel("Principal Component 1")
ax.set_ylabel("Principal Component 2")
ax.legend(title="Cluster", loc="best", fontsize=9)
ax.grid(alpha=0.25, linestyle="--")
plt.tight_layout()
plt.show()

# ============================================================
# üíæ Save Outputs
# ============================================================

viz_path = Path("..") / "visualizations"
viz_path.mkdir(parents=True, exist_ok=True)
fig_pca.savefig(viz_path / f"pca_clusters_k{k_final}.png", dpi=300, bbox_inches="tight")

# Save updated dataset with cluster info
output_path = Path("..") / "data" / "clean" / "books_clustered_final.csv"
df.to_csv(output_path, index=False, encoding="utf-8-sig")

print(f"‚úÖ Clustering completed ‚Äî {df['cluster'].nunique()} clusters created.")
print(f"üíæ PCA plot saved ‚Üí {viz_path / f'pca_clusters_k{k_final}.png'}")
print(f"üíæ Updated dataset saved ‚Üí {output_path}\n")

# --- Preview sample ---
display(df[["title", "author", "avg_rating", "price", "cluster"]].head(10))






# ============================================================
# Step 12 ‚Äî Cluster Profiling and Genre Composition (Descriptive Only)
# ============================================================

"""
üéØ Step 12 ‚Äî Cluster Profiling and Genre Composition (Descriptive Only)
Although the clustering was trained only on numeric features (avg_rating, price),
we include genre information here to interpret and describe each group.
"""

# --- Genre distribution within clusters ---
genre_summary = (
    df.groupby(["cluster", "genre"])
    .size()
    .reset_index(name="count")
)

# --- Add proportion (%) within each cluster ---
cluster_sizes = df["cluster"].value_counts().to_dict()
genre_summary["proportion_%"] = genre_summary.apply(
    lambda row: round((row["count"] / cluster_sizes[row["cluster"]]) * 100, 2),
    axis=1
)

# --- Add numeric averages per cluster ---
cluster_means = (
    df.groupby("cluster")[["avg_rating", "price"]]
    .mean()
    .round(2)
    .reset_index()
)

# --- Merge to get complete profile ---
cluster_profile = genre_summary.merge(cluster_means, on="cluster", how="left")
cluster_profile = cluster_profile.sort_values(["cluster", "count"], ascending=[True, False])

# --- Display top genres per cluster ---
print("üìä Cluster Composition Summary (Genres used for interpretation only):\n")
display(cluster_profile.groupby("cluster").head(6))

print("\nüß≠ Interpretation Guide:")
print("- avg_rating ‚Üí Average rating in the cluster")
print("- price ‚Üí Average price in the cluster")
print("- genre ‚Üí Genre distribution (not used in clustering)")
print("- count ‚Üí Number of books per genre")
print("- proportion_% ‚Üí Share of that genre within its cluster")






# ============================================================
# Step 13 ‚Äî Export Final Clustered Dataset + Detailed Genre Summary
# ============================================================

from pathlib import Path
import pandas as pd

# --- Define export paths ---
data_clean_path = Path("..") / config["paths"]["data_clean"]
viz_path = Path("..") / "visualizations"
data_clean_path.mkdir(parents=True, exist_ok=True)
viz_path.mkdir(parents=True, exist_ok=True)

# --- Export enriched clustered dataset ---
final_cluster_path = data_clean_path / "books_clustered_final_enriched.csv"

export_cols = [
    "title", "author", "avg_rating", "genre", "price", "currency",
    "cover_url", "link", "cluster", "pca_1", "pca_2"
]

df[export_cols].to_csv(final_cluster_path, index=False, encoding="utf-8-sig")
print(f"üíæ Enriched clustered dataset saved successfully ‚Üí {final_cluster_path.resolve()}")

# ============================================================
# üìä Detailed Cluster Composition Summary (with genre proportions)
# ============================================================

# --- Count genres per cluster ---
genre_summary = (
    df.groupby(["cluster", "genre"])
    .size()
    .reset_index(name="count")
)

# --- Add proportion (%) within each cluster ---
cluster_sizes = df["cluster"].value_counts().to_dict()
genre_summary["proportion_%"] = genre_summary.apply(
    lambda row: round((row["count"] / cluster_sizes[row["cluster"]]) * 100, 2),
    axis=1
)

# --- Add numeric averages per cluster ---
cluster_means = (
    df.groupby("cluster")[["avg_rating", "price"]]
    .mean()
    .round(2)
    .reset_index()
)

# --- Merge into one summary table ---
cluster_profile = genre_summary.merge(cluster_means, on="cluster", how="left")
cluster_profile = cluster_profile.sort_values(["cluster", "count"], ascending=[True, False])

print("\nüìó Cluster Composition Summary (Top Genres per Cluster):")
display(cluster_profile.groupby("cluster").head(8))

# --- Save detailed summary ---
summary_path = data_clean_path / "cluster_summary_detailed.csv"
cluster_profile.to_csv(summary_path, index=False, encoding="utf-8-sig")

print(f"\nüíæ Detailed cluster summary saved ‚Üí {summary_path.resolve()}")



# ============================================================
# üìä Cluster Summary Table ‚Äî Detailed Genre Breakdown (k = 2)
# ============================================================

import pandas as pd
from IPython.display import display

# --- Usa la tabla real generada en el notebook (cluster_profile) ---
# Si ya la tienes en memoria, puedes usar directamente `cluster_profile`
# Si no, carga desde CSV:
# cluster_profile = pd.read_csv("../data/clean/cluster_summary_detailed.csv")

# --- Top 6 g√©neros por cluster ---
cluster_top_genres = (
    cluster_profile.groupby("cluster")
    .head(6)
    .reset_index(drop=True)
    .rename(columns={
        "cluster": "Cluster",
        "genre": "Genre",
        "count": "Count",
        "proportion_%": "Proportion (%)",
        "avg_rating": "Avg Rating",
        "price": "Avg Price (EUR)"
    })
)

# --- Estilo visual igual al de tu tabla de presentaci√≥n ---
styled_genres = (
    cluster_top_genres.style
    .set_caption("üìö Cluster Summary ‚Äî Detailed Genre Composition (k = 2)")
    .set_table_styles([
        {"selector": "caption", 
         "props": [("text-align", "left"), ("font-size", "16px"), 
                   ("font-weight", "bold"), ("color", "#00c3ff")]},
        {"selector": "table", 
         "props": [("border", "2px solid #00c3ff"), 
                   ("border-radius", "8px"),
                   ("border-collapse", "collapse")]},
        {"selector": "th", 
         "props": [("background-color", "#1c1c1c"), ("color", "white"), 
                   ("text-align", "center"), ("font-size", "14px")]},
        {"selector": "td", 
         "props": [("background-color", "#505050"), ("color", "#f2f2f2"), 
                   ("font-size", "13px"), ("text-align", "center")]}
    ])
    .hide(axis="index")
    .format({
        "Avg Rating": "{:.2f}",
        "Avg Price (EUR)": "{:.2f}",
        "Proportion (%)": "{:.2f}"
    })
)

# --- Mostrar tabla estilizada ---
display(styled_genres)






# ============================================================
# Step 14 ‚Äî Compare Cluster Summaries (Notebook 03 vs Enriched)
# ============================================================

import pandas as pd
from pathlib import Path

# --- Load both summaries ---
data_clean_path = Path("..") / "data" / "clean"
summary_old_path = data_clean_path / "cluster_summary.csv"
summary_new_path = data_clean_path / "cluster_summary_enriched.csv"

summary_old = pd.read_csv(summary_old_path)
summary_new = pd.read_csv(summary_new_path)

# --- Rename columns for clarity ---
summary_old = summary_old.rename(columns={
    "avg_rating": "avg_rating_old",
    "price": "price_old",
    "genre": "genre_old",
    "count": "count_old"
})
summary_new = summary_new.rename(columns={
    "avg_rating": "avg_rating_new",
    "price": "price_new",
    "genre": "genre_new",
    "count": "count_new"
})

# --- Merge by cluster ID ---
comparison_df = pd.merge(summary_old, summary_new, on="cluster", how="outer")

# --- Compute % changes ---
comparison_df["Œî price (%)"] = ((comparison_df["price_new"] - comparison_df["price_old"]) / comparison_df["price_old"] * 100).round(2)
comparison_df["Œî count (%)"] = ((comparison_df["count_new"] - comparison_df["count_old"]) / comparison_df["count_old"] * 100).round(2)

# --- Display final comparison ---
print("üìä Cluster Summary Comparison ‚Äî Before vs After Enrichment:\n")
display(comparison_df)

print("\nüß≠ Interpretation Guide:")
print("- avg_rating_old / new ‚Üí compare average ratings per cluster.")
print("- price change (%) ‚Üí shows shift toward premium or budget titles.")
print("- count change (%) ‚Üí indicates how cluster size changed after enrichment.")
print("- genre comparison ‚Üí highlights any change in dominant category.")







