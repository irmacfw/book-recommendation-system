


# ============================================================
# Step 1 â€” Imports & Setup
# ============================================================

import sys
from pathlib import Path
import importlib

# --- Add notebooks path and import functions ---
sys.path.append("notebooks")

import functions
from functions import load_config, ensure_directories

# --- Reload in case of changes ---
importlib.reload(functions)

# --- Load config from project root ---
config_path = Path("../config.yaml")
config = load_config(config_path)

# --- Ensure directories exist (defined in config.yaml) ---
ensure_directories(config["paths"])

print("âœ… All functions and configuration loaded successfully!")






# ============================================================
# Step 2 â€” Scrape Goodreads Pages 6â€“10
# ============================================================

from functions import fetch_html, parse_html, extract_books_from_soup, save_dataset
from time import sleep
import random
import pandas as pd

print("ðŸ”¹ Starting scraping for pages 6â€“10...")

base_url = "https://www.goodreads.com/list/show/1.Best_Books_Ever?page="
all_books_part2 = []

# --- Loop through pages 6 to 10 ---
for page in range(6, 11):
    url = base_url + str(page)
    print(f"\nðŸŒ Fetching page {page}: {url}")

    html = fetch_html(url)
    if html is None:
        print(f"âš ï¸ Skipping page {page} due to empty response.")
        continue

    soup = parse_html(html)
    books_page = extract_books_from_soup(soup)
    all_books_part2.append(books_page)

    # --- Ethical delay ---
    sleep(random.uniform(1, 2))

print(f"\nâœ… Scraping completed for {len(all_books_part2)} pages.")

# --- Combine all new DataFrames ---
df_part2 = pd.concat(all_books_part2, ignore_index=True)
print(f"âœ… Combined dataset shape (pages 6â€“10): {df_part2.shape}")

# --- Save dataset correctly in the root-level data/raw ---
from pathlib import Path

raw_output_path = Path("..") / config["paths"]["data_raw"] / "goodreads_books_6_to_10.csv"
print(f"ðŸ“ Saving file to: {raw_output_path.resolve()}")

save_dataset(df_part2, raw_output_path)

# --- Preview ---
df_part2.head()






# ============================================================
# Step 3 â€” Combine New Books (6â€“10) with Existing Clean Dataset
# ============================================================

import pandas as pd
from pathlib import Path
from functions import save_dataset

# --- Load existing clean dataset ---
clean_path_prev = Path("..") / config["paths"]["data_clean"] / "books_clean.csv"
df_existing = pd.read_csv(clean_path_prev)
print(f"âœ… Loaded previous clean dataset: {df_existing.shape}")

# --- Define expected columns from the clean dataset ---
expected_cols = df_existing.columns.tolist()

# --- Create any missing columns in df_part2 ---
for col in expected_cols:
    if col not in df_part2.columns:
        df_part2[col] = pd.NA

# --- Ensure only expected columns are kept (ignore extras safely) ---
df_part2 = df_part2[[col for col in expected_cols if col in df_part2.columns]]

# --- Combine both datasets ---
df_combined = pd.concat([df_existing, df_part2], ignore_index=True)

# --- Drop duplicates by title ---
df_combined = df_combined.drop_duplicates(subset=["title"]).reset_index(drop=True)

print(f"âœ… Combined dataset shape: {df_combined.shape}")
print(f"Unique authors: {df_combined['author'].nunique()}")

# --- Save the updated clean dataset ---
output_path = Path("..") / config["paths"]["data_clean"] / "books_clean_1000.csv"
print(f"ðŸ’¾ Saving combined clean dataset to: {output_path.resolve()}")

save_dataset(df_combined, output_path)

# --- Preview ---
df_combined.head()






# ============================================================
# Step 4 â€” Quick Data Verification
# ============================================================

import pandas as pd
from pathlib import Path

# --- Load combined dataset ---
combined_path = Path("..") / config["paths"]["data_clean"] / "books_clean_1000.csv"
df = pd.read_csv(combined_path)

print(f"âœ… Dataset loaded successfully: {combined_path}")
print(f"Shape: {df.shape}\n")

# --- Overview of columns ---
print("ðŸ“Š Columns:")
print(df.columns.tolist())

# --- Quick info and missing values ---
print("\nðŸ” DataFrame Info:")
df.info()

print("\nðŸ” Missing values per column:")
print(df.isna().sum())

# --- Quick summary statistics (no datetime flag for older pandas) ---
print("\nðŸ“ˆ Descriptive Statistics:")
display(df.describe(include="all"))

# --- Check duplicates by title ---
dup_titles = df["title"].duplicated().sum()
print(f"\nâš ï¸ Duplicate titles found: {dup_titles}")

# --- Unique authors ---
unique_authors = df["author"].nunique()
print(f"ðŸ‘©â€ðŸ’» Unique authors: {unique_authors}")

# --- Example preview ---
print("\nðŸ“˜ Preview of the combined dataset:")
display(df.head(10))






# ============================================================
# Step 5 â€” Reuse Google Books API functions
# ============================================================
import requests
from tqdm import tqdm
import time

def get_book_info_from_google(title, author):
    """Query Google Books API and return metadata for a given title + author."""
    query = f"intitle:{title}+inauthor:{author}"
    url = f"https://www.googleapis.com/books/v1/volumes?q={query}"

    try:
        response = requests.get(url, timeout=10)
        if response.status_code == 200:
            data = response.json()
            if "items" in data and len(data["items"]) > 0:
                info = data["items"][0]["volumeInfo"]
                return {
                    "published_year": info.get("publishedDate", None),
                    "genre": ", ".join(info.get("categories", [])) if info.get("categories") else None,
                    "cover_url": info.get("imageLinks", {}).get("thumbnail", None)
                }
    except Exception as e:
        print(f"âš ï¸ Error fetching '{title}': {e}")
    
    return {"published_year": None, "genre": None, "cover_url": None}


def get_price_from_google(title, author):
    """Query Google Books API for price info (listPrice or retailPrice)."""
    query = f"intitle:{title}+inauthor:{author}"
    url = f"https://www.googleapis.com/books/v1/volumes?q={query}"

    try:
        response = requests.get(url, timeout=10)
        if response.status_code == 200:
            data = response.json()
            if "items" in data:
                info = data["items"][0].get("saleInfo", {})
                price_info = info.get("listPrice", {}) or info.get("retailPrice", {})
                if price_info:
                    return price_info.get("amount"), price_info.get("currencyCode")
    except Exception as e:
        print(f"âš ï¸ Error fetching '{title}': {e}")
    return None, None



# ============================================================
# Step 5.1 â€” Enrich only books missing metadata
# ============================================================

# --- Load combined dataset ---
from pathlib import Path
combined_path = Path("..") / config["paths"]["data_clean"] / "books_clean_1000.csv"
df = pd.read_csv(combined_path)

# --- Filter only the books that need enrichment ---
df_to_enrich = df[df["avg_rating"].isna()].copy()
print(f"ðŸ“š Books pending enrichment: {len(df_to_enrich)}")

# --- Apply Google Books API to retrieve metadata ---
results = []
for _, row in tqdm(df_to_enrich.iterrows(), total=len(df_to_enrich)):
    meta = get_book_info_from_google(row["title"], row["author"])
    results.append(meta)
    time.sleep(0.5)  # ethical delay

api_df = pd.DataFrame(results)
df_enriched = pd.concat([df_to_enrich.reset_index(drop=True), api_df], axis=1)

print(f"âœ… Metadata enrichment completed for {len(df_enriched)} books.")
df_enriched.head()



df_enriched.to_csv("../data/raw/temp_books_meta_backup.csv", index=False, encoding="utf-8-sig")
print("ðŸ’¾ Backup saved successfully!")



df_enriched.isna().sum()






# ============================================================
# Step 5.2 â€” Price Enrichment (Optimized & Re-startable)
# ============================================================

import pandas as pd
import time
from tqdm import tqdm
from pathlib import Path

# --- Load the intermediate enriched file (from Step 5.2) ---
intermediate_path = Path("..") / config["paths"]["data_raw"] / "temp_books_meta_backup.csv"
df_enriched = pd.read_csv(intermediate_path)

print(f"âœ… Loaded enriched dataset for price retrieval: {df_enriched.shape[0]} books")

# --- Define checkpoint path ---
checkpoint_path = Path("..") / config["paths"]["data_raw"] / "temp_prices_checkpoint.csv"

# --- If checkpoint exists, resume from there ---
if checkpoint_path.exists():
    df_checkpoint = pd.read_csv(checkpoint_path)
    processed_titles = set(df_checkpoint["title"].unique())
    print(f"â© Resuming from checkpoint ({len(df_checkpoint)} books already processed).")
else:
    df_checkpoint = pd.DataFrame(columns=["title", "price", "currency"])
    processed_titles = set()
    print("ðŸ†• Starting fresh price enrichment.")

# --- Filter only books not yet processed ---
df_to_process = df_enriched[~df_enriched["title"].isin(processed_titles)].copy()
print(f"ðŸ“š Remaining books to process: {len(df_to_process)}")

# --- Apply Google Books API to get prices ---
prices = []

for i, (_, row) in enumerate(tqdm(df_to_process.iterrows(), total=len(df_to_process))):
    price, currency = get_price_from_google(row["title"], row["author"])
    prices.append({"title": row["title"], "price": price, "currency": currency})
    
    # --- Save progress every 50 books ---
    if (i + 1) % 50 == 0 or (i + 1) == len(df_to_process):
        df_partial = pd.DataFrame(prices)
        df_checkpoint = pd.concat([df_checkpoint, df_partial], ignore_index=True)
        df_checkpoint.to_csv(checkpoint_path, index=False, encoding="utf-8-sig")
        print(f"ðŸ’¾ Checkpoint saved ({len(df_checkpoint)} total so far)")
        prices = []  # reset buffer
    
    time.sleep(0.5)  # reduced ethical delay

print("\nâœ… Price enrichment completed!")

# --- Merge checkpoint results into main enriched dataset ---
df_prices_final = pd.read_csv(checkpoint_path)
df_final = pd.merge(df_enriched, df_prices_final, on="title", how="left")

# --- Save final enriched dataset ---
output_path = Path("..") / config["paths"]["data_clean"] / "books_clean_enriched_1000.csv"
df_final.to_csv(output_path, index=False, encoding="utf-8-sig")

print(f"\nðŸ’¾ Final enriched dataset saved successfully â†’ {output_path.resolve()}")
print(f"Rows: {len(df_final)}, Columns: {len(df_final.columns)}")






# ============================================================
# Step 6 â€” Final Cleanup (Fix Duplicated Columns In-Place)
# ============================================================

from pathlib import Path

# --- Load dataset (already enriched, 497 rows) ---
clean_path = Path("..") / config["paths"]["data_clean"] / "books_clean_enriched_1000.csv"
df = pd.read_csv(clean_path)
print(f"âœ… Loaded dataset: {df.shape}")

# --- Define the column groups to fix ---
pairs = {
    "published_year": ["published_year", "published_year.1"],
    "genre": ["genre", "genre.1"],
    "price": ["price", "price_x", "price_y"],
    "currency": ["currency", "currency_x", "currency_y"],
    "cover_url": ["cover_url", "cover_url.1"]
}

# --- Keep the most complete column for each group ---
for final_col, candidates in pairs.items():
    valid_cols = [c for c in candidates if c in df.columns]
    if not valid_cols:
        continue
    # pick the one with the most non-null values
    best_col = df[valid_cols].notna().sum().idxmax()
    df[final_col] = df[best_col]

# --- Drop all duplicates (.1, _x, _y) ---
df = df.drop(columns=[c for c in df.columns if any(s in c for s in [".1", "_x", "_y"])], errors="ignore")

# --- Reorder columns ---
cols_final = [
    "title", "author", "avg_rating", "genre",
    "published_year", "price", "currency", "cover_url", "link"
]
df = df[[c for c in cols_final if c in df.columns]]

# --- Overwrite the same file (no new file creation) ---
df.to_csv(clean_path, index=False, encoding="utf-8-sig")

print(f"ðŸ’¾ Cleaned and overwritten â†’ {clean_path.resolve()}")
print(f"âœ… Final shape: {df.shape}")
df.head(10)






# ============================================================
# Step 6.1 â€” Recalculate and Populate `avg_rating`
# ============================================================

import pandas as pd
from pathlib import Path

# --- Paths ---
base_path = Path("..") / config["paths"]["data_clean"]
path_existing = base_path / "books_clean.csv"               # Original dateset (â‰ˆ493 libros)
path_new = base_path / "books_clean_enriched_1000.csv"      # Actual dateset (â‰ˆ497 libros)

# --- Load both datasets ---
df_existing = pd.read_csv(path_existing)
df_new = pd.read_csv(path_new)

print(f"ðŸ“˜ Existing dataset (reference): {df_existing.shape}")
print(f"ðŸ“— New dataset to enrich: {df_new.shape}")

# --- Compute reference statistics ---
global_avg = round(df_existing["avg_rating"].mean(), 2)
print(f"ðŸ”¹ Global average rating: {global_avg}")

# --- Populate avg_rating where missing ---
# Match by title if exists in both datasets
df_new = df_new.merge(
    df_existing[["title", "avg_rating"]],
    on="title",
    how="left",
    suffixes=("", "_ref")
)

# Prefer the new column if exists, else fallback to reference or global average
if "avg_rating_ref" in df_new.columns:
    df_new["avg_rating"] = (
        df_new["avg_rating"].fillna(df_new["avg_rating_ref"]).fillna(global_avg)
    )
    df_new = df_new.drop(columns=["avg_rating_ref"], errors="ignore")

# --- Save result in place ---
df_new.to_csv(path_new, index=False, encoding="utf-8-sig")

print(f"âœ… avg_rating successfully populated and saved â†’ {path_new.resolve()}")
print(df_new[["title", "author", "avg_rating"]].head(10))






# ============================================================
# Step 7 â€” Merge Final Dataset (1000 Books)
# ============================================================

from pathlib import Path

# --- Paths ---
base_path = Path("..") / config["paths"]["data_clean"]
path_part1 = base_path / "books_clean.csv"                 # Dataset from pages 1â€“5
path_part2 = base_path / "books_clean_enriched_1000.csv"   # Dataset from pages 6â€“10
path_final = base_path / "books_final_1000.csv"            # Output file

# --- Load datasets ---
df_part1 = pd.read_csv(path_part1)
df_part2 = pd.read_csv(path_part2)

print(f"ðŸ“˜ First dataset: {df_part1.shape}")
print(f"ðŸ“— Second dataset: {df_part2.shape}")

# --- Standardize columns ---
common_cols = [c for c in df_part1.columns if c in df_part2.columns]
df_part1 = df_part1[common_cols]
df_part2 = df_part2[common_cols]

# --- Combine and clean ---
df_final = pd.concat([df_part1, df_part2], ignore_index=True)
df_final.drop_duplicates(subset=["title"], inplace=True)

print(f"âœ… Combined dataset shape: {df_final.shape}")
print(f"Unique authors: {df_final['author'].nunique()}")

# --- Quick sanity check ---
missing = df_final.isna().sum()
print("\nðŸ” Missing values summary:")
print(missing[missing > 0])

# --- Save final dataset ---
df_final.to_csv(path_final, index=False, encoding="utf-8-sig")

print(f"\nðŸ’¾ Final dataset saved successfully â†’ {path_final.resolve()}")
print(df_final.head(10))






# ============================================================
# Step 7.1 â€” Standardize and Format Final Dataset
# ============================================================

import pandas as pd
from pathlib import Path

# --- Load final dataset ---
final_path = Path("..") / config["paths"]["data_clean"] / "books_final_1000.csv"
df = pd.read_csv(final_path)
print(f"âœ… Loaded dataset: {df.shape}")

# --- Round numeric columns to 2 decimals ---
for col in ["avg_rating", "price"]:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors="coerce").round(2)

# --- Enforce numeric display format (2 decimals in Jupyter) ---
pd.options.display.float_format = "{:.2f}".format

# --- Clean text columns ---
for col in ["title", "author", "genre", "currency"]:
    if col in df.columns:
        df[col] = df[col].astype(str).str.strip()

# --- Standardize capitalization ---
if "genre" in df.columns:
    df["genre"] = df["genre"].str.title()
if "currency" in df.columns:
    df["currency"] = df["currency"].str.upper()

# --- Sort alphabetically by title ---
df = df.sort_values("title").reset_index(drop=True)

# --- Save standardized dataset ---
df.to_csv(final_path, index=False, encoding="utf-8-sig")
print(f"ðŸ’¾ Final standardized dataset saved â†’ {final_path.resolve()}")

# ============================================================
# âœ¨ Optional Formatting for Display (Jupyter Only)
# ============================================================

pd.set_option("display.max_colwidth", 120)

# Define alignment
left_cols = ["title", "author", "genre", "cover_url", "link"]
center_cols = [c for c in df.columns if c not in left_cols]

# Apply style formatting for notebook visualization
styled_df = (
    df.head(20)
    .style
    .set_properties(subset=left_cols, **{"text-align": "left"})
    .set_properties(subset=center_cols, **{"text-align": "center"})
    .set_table_styles(
        [{"selector": "th", "props": [("text-align", "center")]}]  # center headers
    )
)

print("\nðŸª„ Preview â€” Text aligned left, numbers centered:")
display(styled_df)







